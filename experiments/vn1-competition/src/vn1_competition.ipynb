{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VN1 Competition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The VN1 Forecasting Accuracy Challenge tasked participants with forecasting future sales using historical sales and pricing data. The goal was to develop robust predictive models capable of anticipating sales trends for various products across different clients and warehouses. Submissions were evaluated based on their accuracy and bias against actual sales figures.\n",
    "\n",
    "The competition was structured into two phases:\n",
    "\n",
    "- **Phase 1** (September 12 - October 3, 2024): Participants used the provided Phase 0 sales data to predict sales for Phase 1. This phase lasted three weeks and featured live leaderboard updates to track participant progress.\n",
    "- **Phase 2** (October 3 - October 17, 2024): Participants utilized both Phase 0 and Phase 1 data to predict sales for Phase 2. Unlike Phase 1, there were no leaderboard updates during this phase until the competition concluded.\n",
    "\n",
    "In the following notebook, we'll be showcasing how to create forecasts with ETS, ARIMA, CES and Theta models from  `statsforecast` as well as using an ensemble of this models and a hierarchical reconciliation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up with uv\n",
    "\n",
    "To set up the environment using `uv`, follow these steps:\n",
    "\n",
    "1. Install `uv` if you haven't already:\n",
    "```bash\n",
    "pip install uv\n",
    "```\n",
    "\n",
    "2. Navigate to the `vn1-competition` directory:\n",
    "```bash\n",
    "cd experiments/vn1-competition\n",
    "```\n",
    "\n",
    "3. Create a virtual environment and install dependencies using uv:\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "4. Activate the virtual environment:\n",
    "```bash\n",
    "source .venv/bin/activate  # On macOS/Linux\n",
    "# or\n",
    ".venv\\Scripts\\activate     # On Windows\n",
    "```\n",
    "\n",
    "5. Download data\n",
    "\n",
    "```bash\n",
    "make download_data\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| warning: false\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoCES, AutoTheta, Naive\n",
    "from utilsforecast.preprocessing import fill_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded data is in wide format so we transform the data in order to be used by `statsforecast`. This imply a long dataframe with columns `unique_id` denoting the time serie identifier, `ds` the date stamp and `y` the values to be forecasted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_prepare_data(file_path: str, value_name: str = \"y\") -> pd.DataFrame:\n",
    "    \"\"\"Reads data in wide format, and returns it in long format with columns `unique_id`, `ds`, `y`\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    uid_cols = [\"Client\", \"Warehouse\", \"Product\"]\n",
    "    df[\"unique_id\"] = df[uid_cols].astype(str).agg(\"-\".join, axis=1)\n",
    "    df = df.drop(uid_cols, axis=1)\n",
    "    df = df.melt(id_vars=[\"unique_id\"], var_name=\"ds\", value_name=value_name)\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "    df = df.sort_values(by=[\"unique_id\", \"ds\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = read_and_prepare_data(\"../data/phase_0_sales.csv\")\n",
    "df1 = read_and_prepare_data(\"../data/phase_1_sales.csv\")\n",
    "\n",
    "df = pd.concat([df0, df1], ignore_index=True)\n",
    "df = df.sort_values(by=[\"unique_id\", \"ds\"])\n",
    "test_df = read_and_prepare_data(\"../data/phase_2_sales.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Top 5 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare the result that we get with `statsforecast` we load the predictions for top 5 competitors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_competition_forecasts() -> pd.DataFrame|None:\n",
    "    \"\"\"Reads all competition forecasts and returns it in long format with columns `unique_id`, `ds`, `y`\"\"\"\n",
    "    fcst_df: pd.DataFrame | None = None\n",
    "    for place in [\"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\"]:\n",
    "        fcst_df_place = read_and_prepare_data(\n",
    "            f\"../data/solution_{place}_place.csv\", place\n",
    "        )\n",
    "        if fcst_df is None:\n",
    "            fcst_df = fcst_df_place\n",
    "        else:\n",
    "            fcst_df = fcst_df.merge(\n",
    "                fcst_df_place,\n",
    "                on=[\"unique_id\", \"ds\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "    return fcst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = get_competition_forecasts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the predictions from any prediction we provide the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vn1_competition_evaluation(forecasts: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Computes competition evaluation scores\"\"\"\n",
    "    actual = read_and_prepare_data(\"../data/phase_2_sales.csv\")\n",
    "    res = actual[[\"unique_id\", \"ds\", \"y\"]].merge(\n",
    "        forecasts, on=[\"unique_id\", \"ds\"], how=\"left\"\n",
    "    )\n",
    "    ids_forecasts = forecasts[\"unique_id\"].unique()\n",
    "    ids_res = res[\"unique_id\"].unique()\n",
    "    assert set(ids_forecasts) == set(ids_res), \"Some unique_ids are missing\"\n",
    "    scores = {}\n",
    "    for model in [col for col in forecasts.columns if col not in [\"unique_id\", \"ds\"]]:\n",
    "        abs_err = np.nansum(np.abs(res[model] - res[\"y\"]))\n",
    "        err = np.nansum(res[model] - res[\"y\"])\n",
    "        score = abs_err + abs(err)\n",
    "        score = score / res[\"y\"].sum()\n",
    "        scores[model] = round(score, 4)\n",
    "    score_df = pd.DataFrame(list(scores.items()), columns=[\"model\", \"score\"])\n",
    "    score_df = score_df.sort_values(by=\"score\")\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing\n",
    "### 3.1. Remove leading zeros \n",
    "\n",
    "There are some `unique_id` that have starting values in `0` meaning that the product wasn't present at the time, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"unique_id == '0-1-11000'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove the leading zeros with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| warning: false\n",
    "\n",
    "def _remove_leading_zeros(group): \n",
    "    \"\"\"\n",
    "    Removes leading zeros from series \n",
    "    \"\"\"\n",
    "    first_non_zero_index = group['y'].ne(0).idxmax()\n",
    "    return group.loc[first_non_zero_index:]\n",
    "\n",
    "df_clean = df.groupby(\"unique_id\").apply(_remove_leading_zeros).reset_index(drop=True)\n",
    "\n",
    "df_clean.shape, df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Identify obsolete series "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some products that for a long period haven't been buyed such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.query(\"unique_id == '9-82-9800'\").tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify them in order to predict 0 demand in this products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| warning: false\n",
    "\n",
    "def _is_obsolete(group, days_obsoletes):\n",
    "    \"\"\"\n",
    "    Identify obsolete series\n",
    "    \"\"\"\n",
    "    last_date = group[\"ds\"].max()\n",
    "    cutoff_date = last_date - pd.Timedelta(days=days_obsoletes)\n",
    "    recent_data = group.query(\"ds >= @cutoff_date\")\n",
    "    return (recent_data[\"y\"] == 0).all()\n",
    "\n",
    "days_obsoletes=180 # context-dependent \n",
    "obsolete_series = df_clean.groupby(\"unique_id\").apply(_is_obsolete, days_obsoletes=days_obsoletes)\n",
    "obsolete_ids = obsolete_series[obsolete_series].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to fit the `AutoARIMA`, `AutoETS`, `AutoCES` and `AutoTheta` models to all products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    AutoARIMA(season_length=52), \n",
    "    AutoETS(season_length=52), \n",
    "    AutoCES(season_length=52), \n",
    "    AutoTheta(season_length=52)\n",
    "]\n",
    "\n",
    "sf = StatsForecast(\n",
    "    models=models, \n",
    "    freq='W-MON', \n",
    "    n_jobs=-1, \n",
    "    fallback_model=Naive()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| warning: false\n",
    "fc = sf.forecast(\n",
    "    df=df_clean, \n",
    "    h=13\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to ensemble the models with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc['Ensemble'] = fc[['AutoARIMA', 'AutoETS', 'CES', 'AutoTheta']].median(axis=1)\n",
    "fc.loc[fc['Ensemble'] <= 1e-1, 'Ensemble'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For obsolete series we provide 0 prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.loc[fc[\"unique_id\"].isin(obsolete_ids), \"Ensemble\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate results\n",
    "\n",
    "Now we proceed to evaluate the results from the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = solutions.merge(fc, on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "vn1_competition_evaluation(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the median ensemble is much better than models by themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchical approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unique_id` column contains information about the product so the id is created in the following way: `Client-Warehouse-Product` we'll build models based in the Client hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client = df_clean.copy()\n",
    "df_clean[['Client', 'Warehouse', 'Product']] = df_clean['unique_id'].str.split('-', expand=True)\n",
    "df_client = df_clean.groupby(['Client', 'ds'])['y'].sum().reset_index()\n",
    "print('There are ', df_client['Client'].nunique(), 'clients in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Client level model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit the same models for client level predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| warning: false\n",
    "\n",
    "sf_client = StatsForecast(\n",
    "    models=models, \n",
    "    freq='W-MON', \n",
    "    n_jobs=-1, \n",
    "    fallback_model=Naive()\n",
    ")\n",
    "\n",
    "fc_client = sf_client.forecast(\n",
    "    df=df_client, \n",
    "    h=13, \n",
    "    id_col=\"Client\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to identify the obsolete series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| warning: false\n",
    "\n",
    "client_obsolete_series = df_client.groupby(\"Client\").apply(_is_obsolete, days_obsoletes=days_obsoletes)\n",
    "client_obsolete_ids = client_obsolete_series[client_obsolete_series].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create the ensemble model for the client level predictions and set the forecast of obsolete clients to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_client['Ensemble'] = fc_client[['AutoARIMA', 'AutoETS', 'CES', 'AutoTheta']].median(axis=1)\n",
    "\n",
    "fc_client.loc[fc_client[\"Client\"].isin(client_obsolete_ids), \"Ensemble\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Hierarchical Reconciliation with Proportions\n",
    "\n",
    "We now have two sets of independent forecasts: one at the client level (aggregated) and one at the product level (granular). These forecasts are often incoherent—meaning the sum of product-level forecasts for a client doesn't match the client-level forecast.\n",
    "\n",
    "**Proportional reconciliation** resolves this by adjusting the bottom-level (product) forecasts to sum exactly to the top-level (client) forecast, while preserving the relative proportions between products.\n",
    "\n",
    "**Example:** \n",
    "- Client-level forecast: **120 units**\n",
    "- Product A base forecast: **50 units**\n",
    "- Product B base forecast: **50 units** \n",
    "- Total product forecasts: **100 units** (incoherent with client forecast)\n",
    "\n",
    "Since each product contributes 50% (50/100) of the base forecast, proportional reconciliation scales both forecasts by the same factor (120/100 = 1.2):\n",
    "\n",
    "- Product A reconciled: **60 units** (50 × 1.2)\n",
    "- Product B reconciled: **60 units** (50 × 1.2)\n",
    "- Total: **120 units**\n",
    "\n",
    "This approach leverages the strengths of both aggregation levels: the stability of aggregate forecasts and the distributional information from granular forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hierachical reconcilation](../img/hierarchical.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating columns in order to make the join with the client level version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc[['Client', 'Warehouse', 'Product']] = fc['unique_id'].str.split('-', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to have the predictions at the same level so we need to sum the predicted values in order to have a Client level forecast and compute the proportions for each product. For this we only use the `Ensemble` forecast and merge it with the Client level forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = fc.groupby(['Client', 'ds'])['Ensemble'].sum().reset_index()\n",
    "total.rename(columns={'Ensemble': 'total_forecasted'}, inplace=True)\n",
    "total['zero_base_fc'] = np.where(\n",
    "    total['total_forecasted'] == 0, \n",
    "    True, \n",
    "    False\n",
    ")\n",
    "\n",
    "fc = fc.merge(total, on=['Client', 'ds'], how='left')\n",
    "\n",
    "fc['fc_proportions'] = fc['Ensemble']/fc['total_forecasted']\n",
    "fc['fc_proportions'] = fc['fc_proportions'].fillna(0)\n",
    "\n",
    "fc_client.rename(columns={'Ensemble': 'Ensemble_client', 'unique_id': 'Client'}, inplace=True)\n",
    "\n",
    "fc = fc.merge(fc_client[['Client', 'ds', 'Ensemble_client']], on=['Client', 'ds'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to use this proportions to reconcile with the Client level forecasted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_per_client = fc.groupby(['Client', 'ds'])['Product'].nunique().reset_index(name='products_per_client')\n",
    "\n",
    "fc = fc.merge(products_per_client, on=['Client', 'ds'], how='left')\n",
    "\n",
    "fc['Ensemble-hierar'] = np.where(\n",
    "    fc['zero_base_fc'] == False,\n",
    "    fc['fc_proportions']*fc['Ensemble_client'],\n",
    "    fc['Ensemble_client']/fc['products_per_client']\n",
    ")\n",
    "\n",
    "fc_hierar = fc[['unique_id', 'ds', 'Ensemble-hierar']]\n",
    "fc_hierar.loc[fc_hierar['Ensemble-hierar'] <= 1e-1, 'Ensemble-hierar'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to evaluate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = forecasts.merge(fc_hierar, on=[\"unique_id\", \"ds\"], how=\"left\")\n",
    "vn1_competition_evaluation(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've achivied better results with this hierarchical reconciliation! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vn1-competition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
