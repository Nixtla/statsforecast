{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VN1 Competition\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The VN1 Forecasting Accuracy Challenge tasked participants with forecasting future sales using historical sales and pricing data. The goal was to develop robust predictive models capable of anticipating sales trends for various products across different clients and warehouses. Submissions were evaluated based on their accuracy and bias against actual sales figures.\n",
    "\n",
    "The competition was structured into two phases:\n",
    "\n",
    "- **Phase 1** (September 12 - October 3, 2024): Participants used the provided Phase 0 sales data to predict sales for Phase 1. This phase lasted three weeks and featured live leaderboard updates to track participant progress.\n",
    "- **Phase 2** (October 3 - October 17, 2024): Participants utilized both Phase 0 and Phase 1 data to predict sales for Phase 2. Unlike Phase 1, there were no leaderboard updates during this phase until the competition concluded.\n",
    "\n",
    "In the following notebook, we'll be showcasing how to create forecasts with ETS, ARIMA, CES and Theta models from  `statsforecast` as well as using an ensemble of this models and a simple hierarchical reconciliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA, AutoETS, AutoCES, AutoTheta, Naive\n",
    "from utilsforecast.preprocessing import fill_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_prepare_data(file_path: str, value_name: str = \"y\") -> pd.DataFrame:\n",
    "    \"\"\"Reads data in wide format, and returns it in long format with columns `unique_id`, `ds`, `y`\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    uid_cols = [\"Client\", \"Warehouse\", \"Product\"]\n",
    "    df[\"unique_id\"] = df[uid_cols].astype(str).agg(\"-\".join, axis=1)\n",
    "    df = df.drop(uid_cols, axis=1)\n",
    "    df = df.melt(id_vars=[\"unique_id\"], var_name=\"ds\", value_name=value_name)\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "    df = df.sort_values(by=[\"unique_id\", \"ds\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = read_and_prepare_data(\"../data/phase_0_sales.csv\")\n",
    "df1 = read_and_prepare_data(\"../data/phase_1_sales.csv\")\n",
    "\n",
    "df = pd.concat([df0, df1], ignore_index=True)\n",
    "df = df.sort_values(by=[\"unique_id\", \"ds\"])\n",
    "test_df = read_and_prepare_data(\"../data/phase_2_sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsForecast.plot(df, test_df.rename(columns={\"y\": \"actual\"}), engine='matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Top 5 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_competition_forecasts() -> pd.DataFrame:\n",
    "    \"\"\"Reads all competition forecasts and returns it in long format with columns `unique_id`, `ds`, `y`\"\"\"\n",
    "    fcst_df: pd.DataFrame | None = None\n",
    "    for place in [\"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\"]:\n",
    "        fcst_df_place = read_and_prepare_data(\n",
    "            f\"../data/solution_{place}_place.csv\", place\n",
    "        )\n",
    "        if fcst_df is None:\n",
    "            fcst_df = fcst_df_place\n",
    "        else:\n",
    "            fcst_df = fcst_df.merge(\n",
    "                fcst_df_place,\n",
    "                on=[\"unique_id\", \"ds\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "    return fcst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = get_competition_forecasts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vn1_competition_evaluation(forecasts: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Computes competition evaluation scores\"\"\"\n",
    "    actual = read_and_prepare_data(\"../data/phase_2_sales.csv\")\n",
    "    res = actual[[\"unique_id\", \"ds\", \"y\"]].merge(\n",
    "        forecasts, on=[\"unique_id\", \"ds\"], how=\"left\"\n",
    "    )\n",
    "    ids_forecasts = forecasts[\"unique_id\"].unique()\n",
    "    ids_res = res[\"unique_id\"].unique()\n",
    "    assert set(ids_forecasts) == set(ids_res), \"Some unique_ids are missing\"\n",
    "    scores = {}\n",
    "    for model in [col for col in forecasts.columns if col not in [\"unique_id\", \"ds\"]]:\n",
    "        abs_err = np.nansum(np.abs(res[model] - res[\"y\"]))\n",
    "        err = np.nansum(res[model] - res[\"y\"])\n",
    "        score = abs_err + abs(err)\n",
    "        score = score / res[\"y\"].sum()\n",
    "        scores[model] = round(score, 4)\n",
    "    score_df = pd.DataFrame(list(scores.items()), columns=[\"model\", \"score\"])\n",
    "    score_df = score_df.sort_values(by=\"score\")\n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = vn1_competition_evaluation(solutions)\n",
    "scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing\n",
    "### 3.1. Remove leading zeros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove leading zeros from each series.\n",
    "def _remove_leading_zeros(group): \n",
    "    \"\"\"\n",
    "    Removes leading zeros from series \n",
    "    \"\"\"\n",
    "    first_non_zero_index = group['y'].ne(0).idxmax()\n",
    "    return group.loc[first_non_zero_index:]\n",
    "\n",
    "df_clean = df_complete.groupby(\"unique_id\").apply(_remove_leading_zeros).reset_index(drop=True)\n",
    "\n",
    "df_clean.shape, df_complete.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Identify obsolete series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Identify obsoletes series \n",
    "def _is_obsolete(group, days_obsoletes):\n",
    "    \"\"\"\n",
    "    Identify obsolete series\n",
    "    \"\"\"\n",
    "    last_date = group[\"ds\"].max()\n",
    "    cutoff_date = last_date - pd.Timedelta(days=days_obsoletes)\n",
    "    recent_data = group.query(\"ds >= @cutoff_date\")\n",
    "    return (recent_data[\"y\"] == 0).all()\n",
    "\n",
    "days_obsoletes=180 # context-dependent \n",
    "obsolete_series = df_clean.groupby(\"unique_id\").apply(_is_obsolete, days_obsoletes=days_obsoletes)\n",
    "obsolete_ids = obsolete_series[obsolete_series].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    AutoARIMA(season_length=52), \n",
    "    AutoETS(season_length=52), \n",
    "    AutoCES(season_length=52), \n",
    "    AutoTheta(season_length=52)\n",
    "]\n",
    "\n",
    "sf = StatsForecast(\n",
    "    models=models, \n",
    "    freq='W-MON', \n",
    "    n_jobs=-1, \n",
    "    fallback_model=Naive()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = sf.forecast(\n",
    "    df=df_clean, \n",
    "    h=13\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble\n",
    "fc['Ensemble'] = fc[['AutoARIMA', 'AutoETS', 'CES', 'AutoTheta']].median(axis=1)\n",
    "fc.loc[fc['Ensemble'] <= 1e-1, 'Ensemble'] = 0\n",
    "\n",
    "# Set forecasts for obsolete series to zero\n",
    "fc.loc[fc[\"unique_id\"].isin(obsolete_ids), \"Ensemble\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate results \n",
    "forecasts = solutions.merge(fc, on=[\"unique_id\", \"ds\"], how=\"inner\")\n",
    "vn1_competition_evaluation(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchical approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client = df_clean.copy()\n",
    "df_clean[['Client', 'Warehouse', 'Product']] = df_clean['unique_id'].str.split('-', expand=True)\n",
    "df_client = df_clean.groupby(['Client', 'ds'])['y'].sum().reset_index()\n",
    "print('There are ', df_client['Client'].nunique(), 'clients in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify obsolete series\n",
    "client_obsolete_series = df_client.groupby(\"Client\").apply(_is_obsolete, days_obsoletes=days_obsoletes)\n",
    "client_obsolete_ids = client_obsolete_series[client_obsolete_series].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_client = StatsForecast(\n",
    "    models=models, \n",
    "    freq='W-MON', \n",
    "    n_jobs=-1, \n",
    "    fallback_model=Naive()\n",
    ")\n",
    "\n",
    "fc_client = sf_client.forecast(\n",
    "    df=df_client, \n",
    "    h=13, \n",
    "    id_col=\"Client\"\n",
    ")\n",
    "\n",
    "# Create ensemble\n",
    "fc_client['Ensemble'] = fc_client[['AutoARIMA', 'AutoETS', 'CES', 'AutoTheta']].median(axis=1)\n",
    "\n",
    "# Set forecasts for obsolete series to zero\n",
    "fc_client.loc[fc_client[\"Client\"].isin(client_obsolete_ids), \"Ensemble\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatsForecast.plot(df_client, fc_client, id_col=\"Client\", engine='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc[['Client', 'Warehouse', 'Product']] = fc['unique_id'].str.split('-', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical reconciliation \n",
    "total = fc.groupby(['Client', 'ds'])['Ensemble'].sum().reset_index()\n",
    "total.rename(columns={'Ensemble': 'total_forecasted'}, inplace=True)\n",
    "total['zero_base_fc'] = np.where(\n",
    "    total['total_forecasted'] == 0, \n",
    "    True, \n",
    "    False\n",
    ")\n",
    "\n",
    "fc = fc.merge(total, on=['Client', 'ds'], how='left')\n",
    "\n",
    "fc['fc_proportions'] = fc['Ensemble']/fc['total_forecasted']\n",
    "fc['fc_proportions'] = fc['fc_proportions'].fillna(0)\n",
    "\n",
    "fc_client.rename(columns={'Ensemble': 'Ensemble_client', 'unique_id': 'Client'}, inplace=True)\n",
    "\n",
    "fc = fc.merge(fc_client[['Client', 'ds', 'Ensemble_client']], on=['Client', 'ds'], how='left')\n",
    "\n",
    "products_per_client = fc.groupby(['Client', 'ds'])['Product'].nunique().reset_index(name='products_per_client')\n",
    "\n",
    "fc = fc.merge(products_per_client, on=['Client', 'ds'], how='left')\n",
    "\n",
    "fc['Ensemble-hierar'] = np.where(\n",
    "    fc['zero_base_fc'] == False,\n",
    "    fc['fc_proportions']*fc['Ensemble_client'],\n",
    "    fc['Ensemble_client']/fc['products_per_client']\n",
    ")\n",
    "\n",
    "fc_hierar = fc[['unique_id', 'ds', 'Ensemble-hierar']]\n",
    "fc_hierar.loc[fc_hierar['Ensemble-hierar'] <= 1e-1, 'Ensemble-hierar'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts = forecasts.merge(fc_hierar, on=[\"unique_id\", \"ds\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn1_competition_evaluation(forecasts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
