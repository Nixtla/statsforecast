{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Forecast vs StatsForecast\n",
    "> Amazon's AutoML vs open source statistical methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will make use of the [M5 competition](https://mofc.unic.ac.cy/m5-competition/) dataset provided by Walmart. This dataset is interesting for its scale but also the fact that it features many timeseries with infrequent occurances.  Such timeseries are common in retail scenarios and are difficult for traditional timeseries forecasting techniques to address. \n",
    "\n",
    "The data are ready for download at the following URLs:\n",
    "\n",
    "- Train set: \n",
    "    `https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet`\n",
    "- Temporal exogenous variables (used by AmazonForecast): \n",
    "    `https://m5-benchmarks.s3.amazonaws.com/data/train/temporal.parquet`\n",
    "- Static exogenous variables (used by AmazonForecast): \n",
    "    `https://m5-benchmarks.s3.amazonaws.com/data/train/static.parquet`\n",
    "\n",
    "\n",
    "A more detailed description of the data can be found [here](./data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "The M5 competition is hierarchical. That is, forecasts are required for different levels of aggregation: national, state, store, etc. In this experiment, we only generate forecasts using the bottom-level data. The evaluation is performed using the bottom-up reconciliation method to obtain the forecasts for the higher hierarchies. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<<<<<<< HEAD`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Forecast\n",
    "\n",
    "Amazon Forecast is a fully automated solution for time series forecasting. The solution can take the time series to forecast and exogenous variables (temporal and static). For this experiment, we used the AutoPredict functionality of Amazon Forecast following the steps of [this tutorial](https://docs.aws.amazon.com/forecast/latest/dg/gs-console.html). A detailed description of the particular steps for this dataset can be found [here](./docs/experiments/AmazonStatsForecast).\n",
    "\n",
    "Amazon Forecast creates predictors with AutoPredictor, which involves applying the optimal combination of algorithms to each time series in your datasets.  The predictor is an Amazon Forecast model that is trained using your target time series, related time series, item metadata, and any additional datasets you include. \n",
    "\n",
    "Included algorithms range from commonly used statistical algorithms like Autoregressive Integrated Moving Average (ARIMA), to complex neural network algorithms like CNN-QR and DeepAR+.: CNN-QR, DeepAR+, Prophet, NPTS, ARIMA, and ETS.\n",
    "\n",
    "To leverage the probabilistic features of Amazon Forecast and enable confidence intervals for further analysis we forecasted the following quantiles: 0.1 | 0.5 | 0.9. \n",
    "\n",
    "The full pipeline of Amazon Forecast took 4.1 hours and the results can be found here: `s3://m5-benchmarks/forecasts/amazonforecast-m5.parquet`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`=======`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Forecast\n",
    "\n",
    "Amazon Forecast is a fully automated solution for time series forecasting. The solution can take the time series to forecast and exogenous variables (temporal and static). For this experiment, we used the AutoPredict functionality of Amazon Forecast following the steps of [this tutorial](https://docs.aws.amazon.com/forecast/latest/dg/gs-console.html). A detailed description of the particular steps for this dataset can be found [here](https://nixtlaverse.nixtla.io/statsforecast/docs/experiments/AmazonStatsForecast).\n",
    "\n",
    "Amazon Forecast creates predictors with AutoPredictor, which involves applying the optimal combination of algorithms to each time series in your datasets.  The predictor is an Amazon Forecast model that is trained using your target time series, related time series, item metadata, and any additional datasets you include. \n",
    "\n",
    "Included algorithms range from commonly used statistical algorithms like Autoregressive Integrated Moving Average (ARIMA), to complex neural network algorithms like CNN-QR and DeepAR+.: CNN-QR, DeepAR+, Prophet, NPTS, ARIMA, and ETS.\n",
    "\n",
    "To leverage the probabilistic features of Amazon Forecast and enable confidence intervals for further analysis we forecasted the following quantiles: 0.1 | 0.5 | 0.9. \n",
    "\n",
    "The full pipeline of Amazon Forecast took 4.1 hours and the results can be found here: `s3://m5-benchmarks/forecasts/amazonforecast-m5.parquet`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`>>>>>>> main`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nixtla's StatsForecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install necessary libraries\n",
    "\n",
    "We assume you have StatsForecast already installed. Check this guide for instructions on [how to install StatsForecast](../getting-started/0_Installation).\n",
    "\n",
    "Additionally, we will install `s3fs` to read from the S3 Filesystem of AWS. (If you don't want to use a cloud storage provider, you can read your files locally using pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install statsforecast s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input format\n",
    "\n",
    "We will use pandas to read the data set stored in a parquet file for efficiency. You can use ordinary pandas operations to read your data in other formats likes `.csv`. \n",
    "\n",
    "The input to StatsForecast is always a data frame in [long format](https://www.theanalysisfactor.com/wide-and-long-data/) with three columns: `unique_id`, `ds` and `y`:\n",
    "\n",
    "* The `unique_id` (string, int or category) represents an identifier for the series. \n",
    "\n",
    "* The `ds` (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.\n",
    "\n",
    "* The `y` (numeric) represents the measurement we wish to forecast. \n",
    "We will rename the \n",
    "\n",
    "So we will rename the original columns to make it compatible with StatsForecast.\n",
    "\n",
    "Depending on your internet connection, this step should take around 20 seconds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "We are reading a file from S3, so you need to install the s3fs library. To install it, run `! pip install s3fs`\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOODS_1_001_CA_1</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          unique_id          ds    y\n",
       "0  FOODS_1_001_CA_1  2011-01-29  3.0\n",
       "1  FOODS_1_001_CA_1  2011-01-30  0.0\n",
       "2  FOODS_1_001_CA_1  2011-01-31  0.0\n",
       "3  FOODS_1_001_CA_1  2011-02-01  1.0\n",
       "4  FOODS_1_001_CA_1  2011-02-02  4.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Y_df_m5 = pd.read_parquet('https://m5-benchmarks.s3.amazonaws.com/data/train/target.parquet') \n",
    "\n",
    "Y_df_m5 = Y_df_m5.rename(columns={\n",
    "    'item_id': 'unique_id', \n",
    "    'timestamp': 'ds', \n",
    "    'demand': 'y'\n",
    "}) \n",
    "\n",
    "Y_df_m5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train statistical models\n",
    "\n",
    "We fit the model by instantiating a new `StatsForecast` object with the following parameters:\n",
    "\n",
    "* `models`: a list of models. Select the models you want from [models](../../models) and import them. For this example, we will use `AutoETS` and `DynamicOptimizedTheta`. We set `season_length` to 7 because we expect seasonal effects every week. (See: [Seasonal periods](https://robjhyndman.com/hyndsight/seasonal-periods/))\n",
    "\n",
    "* `freq`: a string indicating the frequency of the data. (See [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).)\n",
    "\n",
    "* `n_jobs`: n_jobs: int, number of jobs used in the parallel processing, use -1 for all cores.\n",
    "\n",
    "* `fallback_model`: a model to be used if a model fails. \n",
    "\n",
    "Any settings are passed into the constructor. Then you call its fit method and pass in the historical data frame.\n",
    "\n",
    ":::{.callout-note}\n",
    "StatsForecast achieves its blazing speed using JIT compiling through Numba. The first time you call the statsforecast class, the fit method should take around 5 seconds. The second time -once Numba compiled your settings- it should take less than 0.2s. \n",
    ":::\n",
    "\n",
    "* `AutoETS`: Exponential Smoothing model. Automatically selects the best ETS (Error, Trend, Seasonality) model using an information criterion. Ref: `AutoETS`.\n",
    "\n",
    "* `SeasonalNaive`: Memory Efficient Seasonal Naive predictions. Ref: `SeasonalNaive`.\n",
    "\n",
    "* `DynamicOptimizedTheta`: fit two theta lines to a deseasonalized time series, using different techniques to obtain and combine the two theta lines to produce the final forecasts. Ref: `DynamicOptimizedTheta`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fede/statsforecast/statsforecast/core.py:21: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    AutoETS,\n",
    "    DynamicOptimizedTheta,\n",
    "    SeasonalNaive\n",
    ")\n",
    "\n",
    "# Create list of models\n",
    "models = [\n",
    "    AutoETS(season_length=7),\n",
    "    DynamicOptimizedTheta(season_length=7),\n",
    "]\n",
    "\n",
    "# Instantiate StatsForecast class\n",
    "sf = StatsForecast( \n",
    "    models=models,\n",
    "    freq='D', \n",
    "    n_jobs=-1,\n",
    "    fallback_model=SeasonalNaive(season_length=7)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `forecast` method takes two arguments: forecasts the next `h` (for horizon) and `level`.\n",
    "\n",
    "* `h` (int): represents the forecast h steps into the future. In this case, 12 months ahead. \n",
    "\n",
    "* `level` (list of floats): this optional parameter is used for probabilistic forecasting. Set the `level` (or confidence percentile) of your prediction interval. For example, `level=[90]` means that the model expects the real value to be inside that interval 90% of the times. \n",
    "\n",
    "The forecast object here is a new data frame that includes a column with the name of the model and the y hat values, as well as columns for the uncertainty intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "The `forecast` is inteded to be compatible with distributed clusters, so it does not store any model parameters. If you want to store parameter for everymodel you can use the `fit` and `predict` methods. However, those methods are not defined for distrubed engines like Spark, Ray or Dask.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statsforecast time M5 14.274124479293823\n"
     ]
    }
   ],
   "source": [
    "init = time()\n",
    "forecasts_df = sf.forecast(df=Y_df_m5, h=28)\n",
    "end = time()\n",
    "print(f'Statsforecast time M5 {(end - init) / 60}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_df['ThETS'] = forecasts_df[['DynamicOptimizedTheta', 'AutoETS']].clip(0).median(axis=1, numeric_only=True)\n",
    "forecasts_df.to_parquet('s3://m5-benchmarks/forecasts/statsforecast-m5.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "This section evaluates the performance of `StatsForecast` and `AmazonForecast`. To do this, we first need to install [datasetsforecast](https://github.com/Nixtla/datasetsforecast), a Python library developed by Nixtla that includes a large battery of benchmark datasets and evaluation utilities. The library will allow us to calculate the performance of the models using the original evaluation used in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasetsforecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetsforecast.m5 import M5, M5Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will allow us to evaluate a specific model included in the input dataframe. The function is useful for evaluating different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetsforecast.m5 import M5, M5Evaluation\n",
    "from statsforecast import StatsForecast\n",
    "\n",
    "### Evaluator\n",
    "def evaluate_forecasts(df, model, model_name):\n",
    "    Y_hat = df.set_index('ds', append=True)[model].unstack()\n",
    "    *_, S_df = M5.load('data')\n",
    "    Y_hat = S_df.merge(Y_hat, how='left', on=['unique_id'])\n",
    "    eval_ = M5Evaluation.evaluate(y_hat=Y_hat, directory='./data')\n",
    "    eval_ = eval_.rename(columns={'wrmsse': f'{model_name}_{model}_wrmsse'})\n",
    "    return eval_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read the forecasts generated for each solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read Forecasts\n",
    "statsforecasts_df = pd.read_parquet('s3://m5-benchmarks/forecasts/statsforecast-m5.parquet')\n",
    "amazonforecasts_df = pd.read_parquet('s3://m5-benchmarks/forecasts/amazonforecast-m5.parquet')\n",
    "\n",
    "### Amazon Forecast wrangling\n",
    "amazonforecasts_df = amazonforecasts_df.rename(columns={'item_id': 'unique_id', 'date': 'ds'})\n",
    "# amazon forecast returns the unique_id column in lower case\n",
    "# we need to transform it to upper case to ensure proper merging\n",
    "amazonforecasts_df['unique_id'] = amazonforecasts_df['unique_id'].str.upper()\n",
    "amazonforecasts_df = amazonforecasts_df.set_index('unique_id')\n",
    "# parse datestamp\n",
    "amazonforecasts_df['ds'] = pd.to_datetime(amazonforecasts_df['ds']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use our predefined function to compute the performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Level1</th>\n",
       "      <th>Level2</th>\n",
       "      <th>Level3</th>\n",
       "      <th>Level4</th>\n",
       "      <th>Level5</th>\n",
       "      <th>Level6</th>\n",
       "      <th>Level7</th>\n",
       "      <th>Level8</th>\n",
       "      <th>Level9</th>\n",
       "      <th>Level10</th>\n",
       "      <th>Level11</th>\n",
       "      <th>Level12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>StatsForecast_ThETS_wrmsse</th>\n",
       "      <td>0.669606</td>\n",
       "      <td>0.424331</td>\n",
       "      <td>0.515777</td>\n",
       "      <td>0.580670</td>\n",
       "      <td>0.474098</td>\n",
       "      <td>0.552459</td>\n",
       "      <td>0.578092</td>\n",
       "      <td>0.651079</td>\n",
       "      <td>0.642446</td>\n",
       "      <td>0.725324</td>\n",
       "      <td>1.009390</td>\n",
       "      <td>0.967537</td>\n",
       "      <td>0.914068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StatsForecast_AutoETS_wrmsse</th>\n",
       "      <td>0.672404</td>\n",
       "      <td>0.430474</td>\n",
       "      <td>0.516340</td>\n",
       "      <td>0.580736</td>\n",
       "      <td>0.482090</td>\n",
       "      <td>0.559721</td>\n",
       "      <td>0.579939</td>\n",
       "      <td>0.655362</td>\n",
       "      <td>0.643638</td>\n",
       "      <td>0.727967</td>\n",
       "      <td>1.010596</td>\n",
       "      <td>0.968168</td>\n",
       "      <td>0.913820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StatsForecast_DynamicOptimizedTheta_wrmsse</th>\n",
       "      <td>0.675333</td>\n",
       "      <td>0.429670</td>\n",
       "      <td>0.521640</td>\n",
       "      <td>0.589278</td>\n",
       "      <td>0.478730</td>\n",
       "      <td>0.557520</td>\n",
       "      <td>0.584278</td>\n",
       "      <td>0.656283</td>\n",
       "      <td>0.650613</td>\n",
       "      <td>0.731735</td>\n",
       "      <td>1.013910</td>\n",
       "      <td>0.971758</td>\n",
       "      <td>0.918576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AmazonForecast_p50_wrmsse</th>\n",
       "      <td>1.617815</td>\n",
       "      <td>1.912144</td>\n",
       "      <td>1.786991</td>\n",
       "      <td>1.736382</td>\n",
       "      <td>1.972658</td>\n",
       "      <td>2.010498</td>\n",
       "      <td>1.805926</td>\n",
       "      <td>1.819329</td>\n",
       "      <td>1.667225</td>\n",
       "      <td>1.619216</td>\n",
       "      <td>1.156432</td>\n",
       "      <td>1.012942</td>\n",
       "      <td>0.914040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Total    Level1    Level2  \\\n",
       "StatsForecast_ThETS_wrmsse                  0.669606  0.424331  0.515777   \n",
       "StatsForecast_AutoETS_wrmsse                0.672404  0.430474  0.516340   \n",
       "StatsForecast_DynamicOptimizedTheta_wrmsse  0.675333  0.429670  0.521640   \n",
       "AmazonForecast_p50_wrmsse                   1.617815  1.912144  1.786991   \n",
       "\n",
       "                                              Level3    Level4    Level5  \\\n",
       "StatsForecast_ThETS_wrmsse                  0.580670  0.474098  0.552459   \n",
       "StatsForecast_AutoETS_wrmsse                0.580736  0.482090  0.559721   \n",
       "StatsForecast_DynamicOptimizedTheta_wrmsse  0.589278  0.478730  0.557520   \n",
       "AmazonForecast_p50_wrmsse                   1.736382  1.972658  2.010498   \n",
       "\n",
       "                                              Level6    Level7    Level8  \\\n",
       "StatsForecast_ThETS_wrmsse                  0.578092  0.651079  0.642446   \n",
       "StatsForecast_AutoETS_wrmsse                0.579939  0.655362  0.643638   \n",
       "StatsForecast_DynamicOptimizedTheta_wrmsse  0.584278  0.656283  0.650613   \n",
       "AmazonForecast_p50_wrmsse                   1.805926  1.819329  1.667225   \n",
       "\n",
       "                                              Level9   Level10   Level11  \\\n",
       "StatsForecast_ThETS_wrmsse                  0.725324  1.009390  0.967537   \n",
       "StatsForecast_AutoETS_wrmsse                0.727967  1.010596  0.968168   \n",
       "StatsForecast_DynamicOptimizedTheta_wrmsse  0.731735  1.013910  0.971758   \n",
       "AmazonForecast_p50_wrmsse                   1.619216  1.156432  1.012942   \n",
       "\n",
       "                                             Level12  \n",
       "StatsForecast_ThETS_wrmsse                  0.914068  \n",
       "StatsForecast_AutoETS_wrmsse                0.913820  \n",
       "StatsForecast_DynamicOptimizedTheta_wrmsse  0.918576  \n",
       "AmazonForecast_p50_wrmsse                   0.914040  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Evaluate performances\n",
    "m5_eval_df = pd.concat([\n",
    "    evaluate_forecasts(statsforecasts_df, 'ThETS', 'StatsForecast'),\n",
    "    evaluate_forecasts(statsforecasts_df, 'AutoETS', 'StatsForecast'),\n",
    "    evaluate_forecasts(statsforecasts_df, 'DynamicOptimizedTheta', 'StatsForecast'),\n",
    "    evaluate_forecasts(amazonforecasts_df, 'p50', 'AmazonForecast'),\n",
    "], axis=1)\n",
    "m5_eval_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results (including processing time and costs) can be summarized in the following table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"637\" alt=\"image\" src=\"https://user-images.githubusercontent.com/10517170/206330119-48be0a7c-9ff6-412e-a52b-59a181c2a9d9.png\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
