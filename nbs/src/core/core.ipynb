{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# Core Methods\n",
    "> Methods for Fit, Predict, Forecast (fast), Cross Validation and plotting\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24fa50c6",
   "metadata": {},
   "source": [
    "The core methods of `StatsForecast` are:\n",
    "\n",
    "* `StatsForecast.fit` \n",
    "* `StatsForecast.predict` \n",
    "* `StatsForecast.forecast` \n",
    "* `StatsForecast.cross_validation` \n",
    "* `StatsForecast.plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce0ed1-3b26-478e-8f74-3419adbbdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import warnings\n",
    "\n",
    "from nbdev.showdoc import add_docs, show_doc\n",
    "from statsforecast.models import Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a340838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('always', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import datetime as dt\n",
    "import errno\n",
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import reprlib\n",
    "import time\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilsforecast.processing as ufp\n",
    "from fugue.execution.factory import make_execution_engine, try_get_context_execution_engine\n",
    "from threadpoolctl import ThreadpoolController\n",
    "from tqdm.auto import tqdm\n",
    "from triad import conditional_dispatcher\n",
    "from utilsforecast.compat import DataFrame, pl_DataFrame, pl_Series\n",
    "from utilsforecast.grouped_array import GroupedArray as BaseGroupedArray\n",
    "from utilsforecast.validation import ensure_time_dtype, validate_freq\n",
    "\n",
    "from statsforecast.utils import ConformalIntervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f43744-f0c9-4d6a-b897-acc9df8f59b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(name)s %(levelname)s: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    )\n",
    "logger = logging.getLogger(__name__)\n",
    "_controller = ThreadpoolController()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353aca25-18cd-401a-b6af-007a9aec8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f64258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq, test_fail, test_warns\n",
    "from statsforecast.models import _TS\n",
    "from statsforecast.utils import generate_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb71d9-3438-48cb-9247-bad463fa9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class GroupedArray(BaseGroupedArray):\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not hasattr(other, 'data') or not hasattr(other, 'indptr'):\n",
    "            return False\n",
    "        return np.allclose(self.data, other.data) and np.array_equal(self.indptr, other.indptr)\n",
    "    \n",
    "    def fit(self, models, fallback_model=None):\n",
    "        fm = np.full((self.n_groups, len(models)), np.nan, dtype=object)\n",
    "        for i, grp in enumerate(self):\n",
    "            y = grp[:, 0] if grp.ndim == 2 else grp\n",
    "            X = grp[:, 1:] if (grp.ndim == 2 and grp.shape[1] > 1) else None\n",
    "            for i_model, model in enumerate(models):\n",
    "                try:\n",
    "                    new_model = model.new()\n",
    "                    fm[i, i_model] = new_model.fit(y=y, X=X)\n",
    "                except Exception as error:\n",
    "                    if fallback_model is not None:\n",
    "                        new_fallback_model = fallback_model.new()\n",
    "                        new_fallback_model.alias = model.alias\n",
    "                        fm[i, i_model] = new_fallback_model.fit(y=y, X=X)\n",
    "                    else:\n",
    "                        raise error\n",
    "        return fm\n",
    "\n",
    "    def _get_cols(self, models, attr, h, X, level=tuple()):\n",
    "        n_models = len(models)\n",
    "        cuts = np.full(n_models + 1, fill_value=0, dtype=np.int32)\n",
    "        has_level_models = np.full(n_models, fill_value=False, dtype=bool) \n",
    "        cuts[0] = 0\n",
    "        for i_model, model in enumerate(models):\n",
    "            len_cols = 1 # mean\n",
    "            has_level = 'level' in inspect.signature(getattr(model, attr)).parameters and len(level) > 0\n",
    "            has_level_models[i_model] = has_level\n",
    "            if has_level:\n",
    "                len_cols += 2 * len(level) #levels\n",
    "            cuts[i_model + 1] = len_cols + cuts[i_model]\n",
    "        return cuts, has_level_models\n",
    "    \n",
    "    def _output_fcst(self, models, attr, h, X, level=tuple()):\n",
    "        #returns empty output according to method\n",
    "        cuts, has_level_models = self._get_cols(models=models, attr=attr, h=h, X=X, level=level)\n",
    "        out = np.full((self.n_groups * h, cuts[-1]), fill_value=np.nan, dtype=self.data.dtype)\n",
    "        return out, cuts, has_level_models\n",
    "\n",
    "    def predict(self, fm, h, X=None, level=tuple()):\n",
    "        #fm stands for fitted_models\n",
    "        #and fm should have fitted_model\n",
    "        fcsts, cuts, has_level_models = self._output_fcst(\n",
    "            models=fm[0], attr='predict', \n",
    "            h=h, X=X, level=level\n",
    "        )\n",
    "        matches = ['mean', 'lo', 'hi']\n",
    "        cols = []\n",
    "        for i_model in range(fm.shape[1]):\n",
    "            has_level = has_level_models[i_model]\n",
    "            kwargs = {}\n",
    "            if has_level:\n",
    "                kwargs['level'] = level\n",
    "            for i, _ in enumerate(self):\n",
    "                if X is not None:\n",
    "                    X_ = X[i]\n",
    "                else:\n",
    "                    X_ = None\n",
    "                res_i = fm[i, i_model].predict(h=h, X=X_, **kwargs)\n",
    "                cols_m = [key for key in res_i.keys() if any(key.startswith(m) for m in matches)]\n",
    "                fcsts_i = np.vstack([res_i[key] for key in cols_m]).T\n",
    "                model_name = repr(fm[i, i_model])\n",
    "                cols_m = [f'{model_name}' if col == 'mean' else f'{model_name}-{col}' for col in cols_m]\n",
    "                if fcsts_i.ndim == 1:\n",
    "                    fcsts_i = fcsts_i[:, None]\n",
    "                fcsts[i * h : (i + 1) * h, cuts[i_model]:cuts[i_model + 1]] = fcsts_i\n",
    "            cols += cols_m\n",
    "        return fcsts, cols\n",
    "    \n",
    "    def fit_predict(self, models, h, X=None, level=tuple()):\n",
    "        #fitted models\n",
    "        fm = self.fit(models=models)\n",
    "        #forecasts\n",
    "        fcsts, cols = self.predict(fm=fm, h=h, X=X, level=level)\n",
    "        return fm, fcsts, cols\n",
    "    \n",
    "    def forecast(\n",
    "        self,\n",
    "        models,\n",
    "        h,\n",
    "        fallback_model=None,\n",
    "        fitted=False,\n",
    "        X=None,\n",
    "        level=tuple(),\n",
    "        verbose=False,\n",
    "        target_col='y',\n",
    "    ):\n",
    "        fcsts, cuts, has_level_models = self._output_fcst(\n",
    "            models=models, attr='forecast', h=h, X=X, level=level\n",
    "        )\n",
    "        matches = ['mean', 'lo', 'hi']\n",
    "        matches_fitted = ['fitted', 'fitted-lo', 'fitted-hi']\n",
    "        if fitted:\n",
    "            #for the moment we dont return levels for fitted values in \n",
    "            #forecast mode\n",
    "            fitted_vals = np.full((self.data.shape[0], 1 + cuts[-1]), np.nan, dtype=self.data.dtype)\n",
    "            if self.data.ndim == 1:\n",
    "                fitted_vals[:, 0] = self.data\n",
    "            else:\n",
    "                fitted_vals[:, 0] = self.data[:, 0]\n",
    "        iterable = tqdm(enumerate(self), \n",
    "                        disable=(not verbose), \n",
    "                        total=len(self),\n",
    "                        desc='Forecast')\n",
    "        times = {repr(m): 0.0 for m in models}\n",
    "        for i, grp in iterable:\n",
    "            y_train = grp[:, 0] if grp.ndim == 2 else grp\n",
    "            X_train = grp[:, 1:] if (grp.ndim == 2 and grp.shape[1] > 1) else None\n",
    "            if X is not None:\n",
    "                X_f = X[i]\n",
    "            else:\n",
    "                X_f = None\n",
    "            cols = []\n",
    "            cols_fitted = []\n",
    "            for i_model, model in enumerate(models):\n",
    "                has_level = has_level_models[i_model]\n",
    "                kwargs = {}\n",
    "                if has_level:\n",
    "                    kwargs['level'] = level\n",
    "                start = time.perf_counter()\n",
    "                try:\n",
    "                    res_i = model.forecast(h=h, y=y_train, X=X_train, X_future=X_f, fitted=fitted, **kwargs)\n",
    "                except Exception as error:\n",
    "                    if fallback_model is not None:\n",
    "                        res_i = fallback_model.forecast(h=h, y=y_train, X=X_train, X_future=X_f, fitted=fitted, **kwargs)\n",
    "                    else:\n",
    "                        raise error\n",
    "                times[repr(model)] += time.perf_counter() - start\n",
    "                cols_m = [key for key in res_i.keys() if any(key.startswith(m) for m in matches)]\n",
    "                fcsts_i = np.vstack([res_i[key] for key in cols_m]).T\n",
    "                cols_m = [f'{repr(model)}' if col == 'mean' else f'{repr(model)}-{col}' for col in cols_m]\n",
    "                if fcsts_i.ndim == 1:\n",
    "                    fcsts_i = fcsts_i[:, None]\n",
    "                fcsts[i * h : (i + 1) * h, cuts[i_model]:cuts[i_model + 1]] = fcsts_i\n",
    "                cols += cols_m\n",
    "                if fitted:\n",
    "                    cols_m_fitted = [key for key in res_i.keys() if any(key.startswith(m) for m in matches_fitted)]\n",
    "                    fitted_i = np.vstack([res_i[key] for key in cols_m_fitted]).T\n",
    "                    cols_m_fitted = [f'{repr(model)}' \\\n",
    "                                     if col == 'fitted' else f\"{repr(model)}-{col.replace('fitted-', '')}\" \\\n",
    "                                     for col in cols_m_fitted]\n",
    "                    fitted_vals[self.indptr[i] : self.indptr[i + 1], (cuts[i_model] + 1):(cuts[i_model + 1] + 1)] = fitted_i\n",
    "                    cols_fitted += cols_m_fitted\n",
    "        result = {'forecasts': fcsts, 'cols': cols, 'times': times}\n",
    "        if fitted:\n",
    "            result['fitted'] = {'values': fitted_vals}\n",
    "            result['fitted']['cols'] = [target_col] + cols_fitted\n",
    "        return result\n",
    "    \n",
    "    def cross_validation(\n",
    "        self,\n",
    "        models,\n",
    "        h,\n",
    "        test_size,\n",
    "        fallback_model=None,\n",
    "        step_size=1,\n",
    "        input_size=None, \n",
    "        fitted=False,\n",
    "        level=tuple(), \n",
    "        refit=True,\n",
    "        verbose=False,\n",
    "        target_col='y',\n",
    "    ):\n",
    "        # output of size: (ts, window, h)\n",
    "        if (test_size - h) % step_size:\n",
    "            raise Exception('`test_size - h` should be module `step_size`')\n",
    "        n_windows = int((test_size - h) / step_size) + 1\n",
    "        n_models = len(models)\n",
    "        cuts, has_level_models = self._get_cols(models=models, attr='forecast', h=h, X=None, level=level)\n",
    "        # first column of out is the actual y\n",
    "        out = np.full((self.n_groups, n_windows, h, 1 + cuts[-1]), np.nan, dtype=self.data.dtype)\n",
    "        if fitted:\n",
    "            fitted_vals = np.full((self.data.shape[0], n_windows, n_models + 1), np.nan, dtype=self.data.dtype)\n",
    "            fitted_idxs = np.full((self.data.shape[0], n_windows), False, dtype=bool)\n",
    "            last_fitted_idxs = np.full_like(fitted_idxs, False, dtype=bool)\n",
    "        matches = ['mean', 'lo', 'hi']\n",
    "        steps = list(range(-test_size, -h + 1, step_size))\n",
    "        for i_ts, grp in enumerate(self):\n",
    "            iterable = tqdm(\n",
    "                enumerate(steps, start=0),\n",
    "                desc=f\"Cross Validation Time Series {i_ts + 1}\",\n",
    "                disable=(not verbose),\n",
    "                total=len(steps),\n",
    "            )\n",
    "            fitted_models = [None for _ in range(n_models)]            \n",
    "            for i_window, cutoff in iterable:\n",
    "                should_fit = i_window == 0 or (refit > 0 and i_window % refit == 0)\n",
    "                end_cutoff = cutoff + h\n",
    "                in_size_disp = cutoff if input_size is None else input_size \n",
    "                y = grp[(cutoff - in_size_disp):cutoff]\n",
    "                y_train = y[:, 0] if y.ndim == 2 else y\n",
    "                X_train = y[:, 1:] if (y.ndim == 2 and y.shape[1] > 1) else None\n",
    "                y_test = grp[cutoff:] if end_cutoff == 0 else grp[cutoff:end_cutoff]\n",
    "                X_future = y_test[:, 1:] if (y_test.ndim == 2 and y_test.shape[1] > 1) else None\n",
    "                out[i_ts, i_window, :, 0] = y_test[:, 0] if y.ndim == 2 else y_test\n",
    "                if fitted:\n",
    "                    fitted_vals[self.indptr[i_ts] : self.indptr[i_ts + 1], i_window, 0][\n",
    "                        (cutoff - in_size_disp):cutoff\n",
    "                    ] = y_train\n",
    "                    fitted_idxs[self.indptr[i_ts] : self.indptr[i_ts + 1], i_window][\n",
    "                        (cutoff - in_size_disp):cutoff\n",
    "                    ] = True\n",
    "                    last_fitted_idxs[\n",
    "                        self.indptr[i_ts] : self.indptr[i_ts + 1], i_window\n",
    "                    ][cutoff-1] = True\n",
    "                cols = [target_col]\n",
    "                for i_model, model in enumerate(models):\n",
    "                    has_level = has_level_models[i_model]\n",
    "                    kwargs = {}\n",
    "                    if has_level:\n",
    "                        kwargs['level'] = level\n",
    "                    # this is implemented like this because not all models have a forward method\n",
    "                    # so we can't do fit + forward\n",
    "                    if refit is True:\n",
    "                        forecast_kwargs = dict(\n",
    "                            h=h,\n",
    "                            y=y_train,\n",
    "                            X=X_train,\n",
    "                            X_future=X_future,\n",
    "                            fitted=fitted,\n",
    "                            **kwargs,\n",
    "                        )\n",
    "                        try:\n",
    "                            res_i = model.forecast(**forecast_kwargs)\n",
    "                        except Exception as error:\n",
    "                            if fallback_model is None:\n",
    "                                raise error\n",
    "                            res_i = fallback_model.forecast(**forecast_kwargs)\n",
    "                    else:\n",
    "                        if should_fit:\n",
    "                            try:\n",
    "                                fitted_models[i_model] = model.fit(y=y_train, X=X_train)\n",
    "                            except Exception as error:\n",
    "                                if fallback_model is None:\n",
    "                                    raise error\n",
    "                                fitted_models[i_model] = fallback_model.new().fit(y=y_train, X=X_train)\n",
    "                        res_i = fitted_models[i_model].forward(\n",
    "                            h=h,\n",
    "                            y=y_train,\n",
    "                            X=X_train, \n",
    "                            X_future=X_future,\n",
    "                            fitted=fitted,\n",
    "                            **kwargs,\n",
    "                        )\n",
    "                    cols_m = [key for key in res_i.keys() if any(key.startswith(m) for m in matches)]\n",
    "                    fcsts_i = np.vstack([res_i[key] for key in cols_m]).T\n",
    "                    cols_m = [f'{repr(model)}' if col == 'mean' else f'{repr(model)}-{col}' for col in cols_m]\n",
    "                    out[i_ts, i_window, :, (1 + cuts[i_model]):(1 + cuts[i_model + 1])] = fcsts_i\n",
    "                    if fitted:\n",
    "                        fitted_vals[self.indptr[i_ts] : self.indptr[i_ts + 1], i_window, i_model + 1][\n",
    "                            (cutoff - in_size_disp):cutoff\n",
    "                        ] = res_i['fitted']\n",
    "                    cols += cols_m\n",
    "        result = {'forecasts': out.reshape(-1, 1 + cuts[-1]), 'cols': cols}\n",
    "        if fitted:\n",
    "            result['fitted'] = {\n",
    "                'values': fitted_vals, \n",
    "                'idxs': fitted_idxs, \n",
    "                'last_idxs': last_fitted_idxs,\n",
    "                'cols': [target_col] + [repr(model) for model in models]\n",
    "            }\n",
    "        return result\n",
    "\n",
    "    def take(self, idxs):\n",
    "        data, indptr = super().take(idxs)\n",
    "        return GroupedArray(data, indptr)\n",
    "    \n",
    "    def split(self, n_chunks):\n",
    "        n_chunks = min(n_chunks, self.n_groups)\n",
    "        return [self.take(idxs) for idxs in np.array_split(range(self.n_groups), n_chunks)]\n",
    "\n",
    "    def split_fm(self, fm, n_chunks):\n",
    "        return [fm[idxs] for idxs in np.array_split(range(self.n_groups), n_chunks) if idxs.size]\n",
    "\n",
    "    @_controller.wrap(limits=1)\n",
    "    def _single_threaded_fit(self, models, fallback_model=None):\n",
    "        return self.fit(models=models, fallback_model=fallback_model)\n",
    "\n",
    "    @_controller.wrap(limits=1)\n",
    "    def _single_threaded_predict(self, fm, h, X=None, level=tuple()):\n",
    "        return self.predict(fm=fm, h=h, X=X, level=level)\n",
    "\n",
    "    @_controller.wrap(limits=1)\n",
    "    def _single_threaded_fit_predict(self, models, h, X=None, level=tuple()):\n",
    "        return self.fit_predict(models=models, h=h, X=X, level=level)\n",
    "\n",
    "    @_controller.wrap(limits=1)\n",
    "    def _single_threaded_forecast(\n",
    "        self,\n",
    "        models,\n",
    "        h,\n",
    "        fallback_model=None,\n",
    "        fitted=False,\n",
    "        X=None,\n",
    "        level=tuple(),\n",
    "        verbose=False,\n",
    "        target_col='y',\n",
    "    ):\n",
    "        return self.forecast(\n",
    "            models=models,\n",
    "            h=h,\n",
    "            fallback_model=fallback_model,\n",
    "            fitted=fitted,\n",
    "            X=X,\n",
    "            level=level,\n",
    "            verbose=verbose,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "\n",
    "    @_controller.wrap(limits=1)\n",
    "    def _single_threaded_cross_validation(\n",
    "        self,\n",
    "        models,\n",
    "        h,\n",
    "        test_size,\n",
    "        fallback_model=None,\n",
    "        step_size=1,\n",
    "        input_size=None, \n",
    "        fitted=False,\n",
    "        level=tuple(), \n",
    "        refit=True,\n",
    "        verbose=False,\n",
    "        target_col='y',\n",
    "    ):\n",
    "        return self.cross_validation(\n",
    "            models=models,\n",
    "            h=h,\n",
    "            test_size=test_size,\n",
    "            fallback_model=fallback_model,\n",
    "            step_size=step_size,\n",
    "            input_size=input_size,\n",
    "            fitted=fitted,\n",
    "            level=level,\n",
    "            refit=refit,\n",
    "            verbose=verbose,\n",
    "            target_col=target_col,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d35a24-3f5e-4b88-9b3a-5620bd14fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# sum ahead just returns the last value\n",
    "# added with h future values \n",
    "class SumAhead:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, y, X):\n",
    "        self.last_value = y[-1]\n",
    "        self.fitted_values = np.full(y.size, np.nan, dtype=y.dtype)\n",
    "        self.fitted_values[1:] = y[:1]\n",
    "        return self\n",
    "    \n",
    "    def predict(self, h, X=None, level=None):\n",
    "        mean = self.last_value + np.arange(1, h + 1)\n",
    "        res = {'mean': mean}\n",
    "        if level is not None:\n",
    "            for lv in level:\n",
    "                res[f'lo-{lv}'] = mean - 1.0\n",
    "                res[f'hi-{lv}'] = mean + 1.0\n",
    "        return res\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'SumAhead'\n",
    "    \n",
    "    def forecast(self, y, h, X=None, X_future=None, fitted=False, level=None):\n",
    "        mean = y[-1] + np.arange(1, h + 1)\n",
    "        res = {'mean': mean}\n",
    "        if fitted:\n",
    "            fitted_values = np.full(y.size, np.nan, dtype=y.dtype)\n",
    "            fitted_values[1:] = y[1:]\n",
    "            res['fitted'] = fitted_values\n",
    "        if level is not None:\n",
    "            for lv in level:\n",
    "                res[f'lo-{lv}'] = mean - 1.0\n",
    "                res[f'hi-{lv}'] = mean + 1.0\n",
    "        return res\n",
    "    \n",
    "    def forward(self, y, h, X=None, X_future=None, fitted=False, level=None):\n",
    "        # fix self.last_value for test purposes\n",
    "        mean = self.last_value + np.arange(1, h + 1)\n",
    "        res = {'mean': mean}\n",
    "        if fitted:\n",
    "            fitted_values = np.full(y.size, np.nan, dtype=mean.dtype)\n",
    "            fitted_values[1:] = y[1:]\n",
    "            res['fitted'] = fitted_values\n",
    "        if level is not None:\n",
    "            for lv in level:\n",
    "                res[f'lo-{lv}'] = mean - 1.0\n",
    "                res[f'hi-{lv}'] = mean + 1.0\n",
    "        return res\n",
    "    \n",
    "    def new(self):\n",
    "        b = type(self).__new__(type(self))\n",
    "        b.__dict__.update(self.__dict__)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822cacd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#data used for tests\n",
    "data = np.arange(12).reshape(-1, 1)\n",
    "indptr = np.array([0, 4, 8, 12])\n",
    "\n",
    "# test we can recover the \n",
    "# number of series\n",
    "ga = GroupedArray(data, indptr)\n",
    "test_eq(len(ga), 3)\n",
    "\n",
    "#test splits of data\n",
    "splits = ga.split(2)\n",
    "test_eq(splits[0], GroupedArray(data[:8], indptr[:3]))\n",
    "test_eq(splits[1], GroupedArray(data[8:], np.array([0, 4])))\n",
    "\n",
    "# fitting models for each ts\n",
    "models = [Naive(), Naive()]\n",
    "fm = ga.fit(models)\n",
    "test_eq(fm.shape, (3, 2))\n",
    "test_eq(len(ga.split_fm(fm, 2)), 2)\n",
    "\n",
    "# test forecasts\n",
    "exp_fcsts = np.vstack([2 * [data[i]] for i in indptr[1:] - 1])\n",
    "fcsts, cols = ga.predict(fm=fm, h=2)\n",
    "np.testing.assert_equal(\n",
    "    fcsts,\n",
    "    np.hstack([exp_fcsts, exp_fcsts]),\n",
    ")\n",
    "\n",
    "#test fit and predict pipelie\n",
    "fm_fp, fcsts_fp, cols_fp = ga.fit_predict(models=models, h=2) \n",
    "test_eq(fm_fp.shape, (3, 2))\n",
    "np.testing.assert_equal(fcsts_fp, fcsts)\n",
    "np.testing.assert_equal(cols_fp, cols)\n",
    "\n",
    "#test levels\n",
    "fm_lv, fcsts_lv, cols_lv = ga.fit_predict(models=models, h=2, level=(50, 90))\n",
    "test_eq(fcsts_lv.shape, (2 * len(ga), 10)) \n",
    "\n",
    "#test forecast\n",
    "fcst_f = ga.forecast(models=models, h=2, fitted=True)\n",
    "test_eq(fcst_f['forecasts'], fcsts_fp)\n",
    "test_eq(fcst_f['cols'], cols_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc375b-6fc5-44a3-8645-6cc1bf9e0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class NullModel(_TS):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forecast(self):\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"NullModel\"\n",
    "\n",
    "#test fallback model\n",
    "fcst_f = ga.forecast(models=[NullModel(), NullModel()], fallback_model=Naive(), h=2, fitted=True)\n",
    "test_eq(fcst_f['forecasts'], fcsts_fp)\n",
    "test_eq(fcst_f['cols'], ['NullModel', 'NullModel'])\n",
    "test_fail(ga.forecast, kwargs={'models': [NullModel()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895064c-7d9d-4994-826e-57efc0c9c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test levels\n",
    "lv = (50, 60)\n",
    "h = 2\n",
    "#test for forecasts\n",
    "fcsts_lv = ga.forecast(models=[SumAhead()], h=h, fitted=True, level=lv)\n",
    "test_eq(\n",
    "    fcsts_lv['forecasts'].shape,\n",
    "    (len(ga) * h, 1 + 2 * len(lv))\n",
    ")\n",
    "test_eq(\n",
    "    fcsts_lv['cols'],\n",
    "    ['SumAhead', \n",
    "     'SumAhead-lo-50', \n",
    "     'SumAhead-hi-50',\n",
    "     'SumAhead-lo-60',\n",
    "     'SumAhead-hi-60']\n",
    ")\n",
    "#fit and predict pipeline\n",
    "fm_lv_fp, fcsts_lv_fp, cols_lv_fp = ga.fit_predict(models=[SumAhead()], h=h, level=lv)\n",
    "test_eq(\n",
    "    fcsts_lv['forecasts'],\n",
    "    fcsts_lv_fp\n",
    ")\n",
    "test_eq(\n",
    "    fcsts_lv['cols'],\n",
    "    cols_lv_fp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e325a2-aaa4-406c-9008-b5987e1372f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# tests for cross valiation\n",
    "data = np.hstack([np.arange(10), np.arange(100, 200), np.arange(20, 40)])\n",
    "indptr = np.array([0, 10, 110, 130])\n",
    "ga = GroupedArray(data, indptr)\n",
    "    \n",
    "res_cv = ga.cross_validation(models=[SumAhead()], h=2, test_size=5, fitted=True)\n",
    "fcsts_cv = res_cv['forecasts']\n",
    "cols_cv = res_cv['cols']\n",
    "test_eq(\n",
    "    fcsts_cv[:, cols_cv.index('y')], \n",
    "    fcsts_cv[:, cols_cv.index('SumAhead')]\n",
    ")\n",
    "\n",
    "#levels\n",
    "res_cv_lv = ga.cross_validation(models=[SumAhead(), Naive()], h=2, test_size=5, level=(50, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d3a28-f591-4c70-96ac-9bed83a3b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "actual_step_size = np.unique(np.diff(fcsts_cv[:, cols_cv.index('SumAhead')].reshape((3, -1, 2)), axis=1))\n",
    "test_eq(actual_step_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523c6be-242e-4eab-bb32-8956d9e25d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "horizons = [1, 2, 3, 2]\n",
    "test_sizes = [3, 4, 6, 6]\n",
    "step_sizes = [2, 2, 3, 4]\n",
    "for h, test_size, step_size in zip(horizons, test_sizes, step_sizes):\n",
    "    res_cv = ga.cross_validation(\n",
    "        models=[SumAhead()], h=h, \n",
    "        test_size=test_size, \n",
    "        step_size=step_size,\n",
    "        fitted=True\n",
    "    )\n",
    "    fcsts_cv = res_cv['forecasts']\n",
    "    cols_cv = res_cv['cols']\n",
    "    test_eq(\n",
    "        fcsts_cv[:, cols_cv.index('y')], \n",
    "        fcsts_cv[:, cols_cv.index('SumAhead')]\n",
    "    )\n",
    "    fcsts_cv = fcsts_cv[:, cols_cv.index('SumAhead')].reshape((3, -1, h))\n",
    "    actual_step_size = np.unique(\n",
    "        np.diff(fcsts_cv, axis=1)\n",
    "    )\n",
    "    test_eq(actual_step_size, step_size)\n",
    "    actual_n_windows = res_cv['forecasts'].shape[1]\n",
    "    test_eq(actual_n_windows, int((test_size - h)/step_size) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2521161-1947-4b4c-9d2e-e1da646a6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "def fail_cv(h, test_size, step_size):\n",
    "    return ga.cross_validation(models=[SumAhead()], h=h, test_size=test_size, step_size=step_size)\n",
    "test_fail(fail_cv, contains='module', kwargs=dict(h=2, test_size=5, step_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90364c66-c595-4588-984f-c913d0c3c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test fallback model\n",
    "# cross validation\n",
    "fcst_cv_f = ga.cross_validation(\n",
    "    models=[NullModel(), NullModel()], \n",
    "    fallback_model=Naive(), h=2, \n",
    "    test_size=5,\n",
    "    fitted=True\n",
    ")\n",
    "fcst_cv_naive = ga.cross_validation(\n",
    "    models=[Naive(), Naive()], \n",
    "    h=2, \n",
    "    test_size=5,\n",
    "    fitted=True\n",
    ")\n",
    "test_eq(fcst_cv_f['forecasts'], fcst_cv_naive['forecasts'])\n",
    "np.testing.assert_array_equal(fcst_cv_f['fitted']['values'], fcst_cv_naive['fitted']['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test fallback model under failed fit for cross validation\n",
    "class FailedFit:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forecast(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, y, X):\n",
    "        raise Exception('Failed fit')\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"FailedFit\"\n",
    "\n",
    "fcst_cv_f = ga.cross_validation(\n",
    "    models=[FailedFit()], \n",
    "    fallback_model=Naive(), h=2, \n",
    "    test_size=5,\n",
    "    refit=False,\n",
    "    fitted=True,\n",
    ")\n",
    "fcst_cv_naive = ga.cross_validation(\n",
    "    models=[Naive()], \n",
    "    h=2, \n",
    "    test_size=5,\n",
    "    refit=False,\n",
    "    fitted=True,\n",
    ")\n",
    "test_eq(fcst_cv_f['forecasts'], fcst_cv_naive['forecasts'])\n",
    "np.testing.assert_array_equal(fcst_cv_f['fitted']['values'], fcst_cv_naive['fitted']['values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4b5d2-9caa-4d2e-9fae-6d86fd48c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test cross validation without refit\n",
    "cv_starts = np.array([0, 8, 16])\n",
    "res_cv_wo_refit = ga.cross_validation(models=[SumAhead()], h=2, test_size=5, refit=False, level=(50, 60))\n",
    "res_cv_refit = ga.cross_validation(models=[SumAhead()], h=2, test_size=5, refit=True, level=(50, 60))\n",
    "test_fail(test_eq, args=(res_cv_wo_refit['forecasts'], res_cv_refit['forecasts']))\n",
    "#test first forecasts are equal\n",
    "test_eq(\n",
    "    res_cv_wo_refit['forecasts'][cv_starts],\n",
    "    res_cv_refit['forecasts'][cv_starts]\n",
    ")\n",
    "# for refit=2 the first two windows should be the same\n",
    "res_cv_refit2 = ga.cross_validation(models=[SumAhead()], h=2, test_size=5, refit=2)\n",
    "test_eq(\n",
    "    res_cv_refit2['forecasts'][np.hstack([cv_starts + 0, cv_starts + 1]), 1],\n",
    "    res_cv_refit2['forecasts'][np.hstack([cv_starts + 2, cv_starts + 3]), 1],\n",
    ")\n",
    "# and the second two windows should be the same\n",
    "test_eq(\n",
    "    res_cv_refit2['forecasts'][np.hstack([cv_starts + 4, cv_starts + 5]), 1],\n",
    "    res_cv_refit2['forecasts'][np.hstack([cv_starts + 6, cv_starts + 7]), 1],\n",
    ")\n",
    "# but different between them\n",
    "test_fail(\n",
    "    lambda: test_eq(\n",
    "        res_cv_refit2['forecasts'][np.hstack([cv_starts + 0, cv_starts + 1]), 1],\n",
    "        res_cv_refit2['forecasts'][np.hstack([cv_starts + 4, cv_starts + 5]), 1],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ef24f-2830-4ab0-bf79-73973f83249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from statsforecast.models import AutoCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef5d40-2cd6-4b23-b581-4e70f58526d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "res_cv_wo_refit = ga.cross_validation(models=[AutoCES()], h=2, test_size=5, refit=False, level=(50, 60))\n",
    "res_cv_refit = ga.cross_validation(models=[AutoCES()], h=2, test_size=5, refit=True, level=(50, 60))\n",
    "test_fail(test_eq, args=(res_cv_wo_refit['forecasts'], res_cv_refit['forecasts']))\n",
    "#test first forecasts are equal\n",
    "test_eq(\n",
    "    res_cv_wo_refit['forecasts'][[0, 8, 16]],\n",
    "    res_cv_refit['forecasts'][[0, 8, 16]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa571eb5-f4b8-4e6b-b910-75bf8514eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _get_n_jobs(n_groups, n_jobs):\n",
    "    if n_jobs == -1 or (n_jobs is None):\n",
    "        actual_n_jobs = os.cpu_count()\n",
    "    else:\n",
    "        actual_n_jobs = n_jobs\n",
    "    return min(n_groups, actual_n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8b257-9d7b-4578-a8bf-57e72f19e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#tests for more series than resources\n",
    "test_eq(_get_n_jobs(100, -1), os.cpu_count()) \n",
    "test_eq(_get_n_jobs(100, None), os.cpu_count())\n",
    "test_eq(_get_n_jobs(100, 2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a43806-e6d7-495c-b98f-3729b8bcb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#tests for less series than resources\n",
    "test_eq(_get_n_jobs(1, -1), 1) \n",
    "test_eq(_get_n_jobs(1, None), 1)\n",
    "test_eq(_get_n_jobs(2, 10), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba1f88-c824-4ea2-8fa2-188348fe6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _warn_df_constructor():\n",
    "    warnings.warn(\n",
    "        \"The `df` argument of the StatsForecast constructor as well as reusing stored \"\n",
    "        \"dfs from other methods is deprecated and will raise an error in a future version. \"\n",
    "        \"Please provide the `df` argument to the corresponding method instead, e.g. fit/forecast.\",\n",
    "        category=FutureWarning,\n",
    "    )\n",
    "\n",
    "def _maybe_warn_sort_df(sort_df):\n",
    "    if not sort_df:  \n",
    "        warnings.warn(\n",
    "            \"The `sort_df` argument is deprecated and will be removed in a future version. \"\n",
    "            \"You can leave it to its default value (True) to supress this warning\",\n",
    "            category=FutureWarning,\n",
    "        )\n",
    "\n",
    "def _warn_id_as_idx():\n",
    "    warnings.warn(\n",
    "        \"In a future version the predictions will have the id as a column. \"\n",
    "        \"You can set the `NIXTLA_ID_AS_COL` environment variable \"\n",
    "        \"to adopt the new behavior and to suppress this warning.\",\n",
    "        category=FutureWarning,\n",
    "    )\n",
    "\n",
    "def _id_as_idx() -> bool:\n",
    "    return not bool(os.getenv('NIXTLA_ID_AS_COL', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b355d-aab4-44a2-bdbe-902b5c593633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "_param_descriptions = {\n",
    "    'freq': \"\"\"freq : str or int\n",
    "            Frequency of the data. Must be a valid pandas or polars offset alias, or an integer.\"\"\",\n",
    "    'df': \"\"\"df : pandas or polars DataFrame, optional (default=None)\n",
    "            DataFrame with ids, times, targets and exogenous.\"\"\",\n",
    "    'sort_df': \"\"\"sort_df : bool (default=True)\n",
    "            Sort `df` by ids and times.\"\"\",\n",
    "    'fallback_model': \"\"\"fallback_model : Any, optional (default=None)\n",
    "            Any, optional (default=None)\n",
    "            Model to be used if a model fails.\n",
    "            Only works with the `forecast` and `cross_validation` methods.\"\"\",\n",
    "    'id_col': \"\"\"id_col : str (default='unique_id')\n",
    "            Column that identifies each serie.\"\"\",\n",
    "    'time_col': \"\"\"time_col : str (default='ds')\n",
    "            Column that identifies each timestep, its values can be timestamps or integers.\"\"\",\n",
    "    'target_col': \"\"\"target_col : str (default='y')\n",
    "            Column that contains the target.\"\"\",\n",
    "    'h': \"\"\"h : int\n",
    "            Forecast horizon.\"\"\",\n",
    "    'X_df': \"\"\"X_df : pandas or polars DataFrame, optional (default=None)\n",
    "            DataFrame with ids, times and future exogenous.\"\"\",\n",
    "    'level': \"\"\"level : List[float], optional (default=None)\n",
    "            Confidence levels between 0 and 100 for prediction intervals.\"\"\",\n",
    "    'prediction_intervals': \"\"\"prediction_intervals : ConformalIntervals, optional (default=None)\n",
    "            Configuration to calibrate prediction intervals (Conformal Prediction).\"\"\",\n",
    "    'fitted': \"\"\"fitted : bool (default=False)\n",
    "            Store in-sample predictions.\"\"\",\n",
    "    'n_jobs': \"\"\"n_jobs : int (default=1)\n",
    "            Number of jobs used in the parallel processing, use -1 for all cores.\"\"\",\n",
    "    'verbose': \"\"\"verbose : bool (default=True)\n",
    "            Prints TQDM progress bar when `n_jobs=1`.\"\"\",\n",
    "    'models': \"\"\"models : List[Any]\n",
    "            List of instantiated objects models.StatsForecast.\"\"\",\n",
    "    'n_windows': \"\"\"n_windows : int (default=1)\n",
    "            Number of windows used for cross validation.\"\"\",\n",
    "    'step_size': \"\"\"step_size : int (default=1)\n",
    "            Step size between each window.\"\"\",\n",
    "    'test_size': \"\"\"test_size : int, optional (default=None)\n",
    "            Length of test size. If passed, set `n_windows=None`.\"\"\",\n",
    "    'input_size': \"\"\"input_size : int, optional (default=None)\n",
    "            Input size for each window, if not none rolled windows.\"\"\",\n",
    "    'refit': \"\"\"refit : bool or int (default=True)\n",
    "            Wether or not refit the model for each window.\n",
    "            If int, train the models every `refit` windows.\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c9795-05d1-47b7-9f1a-f5057c5908eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class _StatsForecast:\n",
    "    \"\"\"The `StatsForecast` class allows you to efficiently fit multiple `StatsForecast` models \n",
    "    for large sets of time series. It operates on a DataFrame `df` with at least three columns\n",
    "    ids, times and targets.\n",
    "\n",
    "    The class has memory-efficient `StatsForecast.forecast` method that avoids storing partial \n",
    "    model outputs. While the `StatsForecast.fit` and `StatsForecast.predict` methods with \n",
    "    Scikit-learn interface store the fitted models.\n",
    "\n",
    "    The `StatsForecast` class offers parallelization utilities with Dask, Spark and Ray back-ends.\n",
    "    See distributed computing example [here](https://github.com/Nixtla/statsforecast/tree/main/experiments/ray).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        models: List[Any],\n",
    "        freq: Union[str, int],\n",
    "        n_jobs: int = 1,\n",
    "        df: Optional[DataFrame] = None,\n",
    "        sort_df: bool = True,\n",
    "        fallback_model: Optional[Any] = None,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        \"\"\"Train statistical models.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        {models}\n",
    "        {freq}\n",
    "        {n_jobs}\n",
    "        {df}\n",
    "        {sort_df}\n",
    "        {fallback_model}\n",
    "        {verbose}\n",
    "        \"\"\"\n",
    "        # TODO @fede: needed for residuals, think about it later\n",
    "        self.models = models\n",
    "        self._validate_model_names()\n",
    "        self.freq = freq\n",
    "        self.n_jobs = n_jobs\n",
    "        self.fallback_model = fallback_model\n",
    "        self.verbose = verbose\n",
    "        if df is not None:\n",
    "            _warn_df_constructor()\n",
    "            self._prepare_fit(df=df, sort_df=sort_df)\n",
    "        else:\n",
    "            _maybe_warn_sort_df(sort_df)\n",
    "\n",
    "    __init__.__doc__ = __init__.__doc__.format(**_param_descriptions)  # type: ignore[union-attr]\n",
    "    \n",
    "    def _validate_model_names(self):\n",
    "        # Some test models don't have alias\n",
    "        names = [getattr(model, 'alias', lambda: None) for model in self.models]\n",
    "        names = [x for x in names if x is not None]\n",
    "        if len(names) != len(set(names)):\n",
    "            raise ValueError('Model names must be unique. You can use `alias` to set a unique name for each model.')\n",
    "\n",
    "    def _prepare_fit(\n",
    "        self,\n",
    "        df: Optional[DataFrame],\n",
    "        sort_df: bool = True,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',        \n",
    "    ) -> None:\n",
    "        if df is None:\n",
    "            if not hasattr(self, 'ga'):\n",
    "                raise ValueError('You must provide the `df` argument.')\n",
    "            _warn_df_constructor()\n",
    "            return\n",
    "        df = ensure_time_dtype(df, time_col)\n",
    "        validate_freq(df[time_col], self.freq)\n",
    "        if isinstance(df, pd.DataFrame) and df.index.name == id_col:\n",
    "            warnings.warn(\n",
    "                \"Passing unique_id as the index is deprecated. \"\n",
    "                \"Please provide it as a column instead.\",\n",
    "                category=FutureWarning\n",
    "            )\n",
    "            df = df.reset_index()\n",
    "        _maybe_warn_sort_df(sort_df)\n",
    "        self.uids, last_times, data, indptr, sort_idxs = ufp.process_df(\n",
    "            df, id_col, time_col, target_col\n",
    "        )\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            self.last_dates = pd.Index(last_times, name=time_col)\n",
    "        else:\n",
    "            self.last_dates = pl_Series(last_times)\n",
    "        self.ga = GroupedArray(data, indptr)\n",
    "        self.og_dates = df[time_col].to_numpy()\n",
    "        if sort_idxs is not None:\n",
    "            self.og_dates = self.og_dates[sort_idxs]\n",
    "        self.n_jobs = _get_n_jobs(len(self.ga), self.n_jobs)\n",
    "        self.df_constructor = type(df)\n",
    "        self.id_col = id_col\n",
    "        self.time_col = time_col\n",
    "        self.target_col = target_col\n",
    "        self._exog = [c for c in df.columns if c not in (id_col, time_col, target_col)]        \n",
    "\n",
    "    def _validate_sizes_for_prediction_intervals(\n",
    "        self,\n",
    "        prediction_intervals: Optional[ConformalIntervals],\n",
    "        offset: int = 0,\n",
    "    ) -> None:\n",
    "        if prediction_intervals is None:\n",
    "            return\n",
    "        sizes = np.diff(self.ga.indptr) - offset\n",
    "        # the absolute minimum requires two windows\n",
    "        min_samples = 2 * prediction_intervals.h + 1\n",
    "        if np.any(sizes < min_samples):\n",
    "            raise ValueError(\n",
    "                f'Minimum samples for computing prediction intervals are {min_samples + offset:,}, '\n",
    "                'some series have less. Please remove them or adjust the horizon.'\n",
    "            )\n",
    "        # required samples for current configuration\n",
    "        required_samples = prediction_intervals.n_windows * prediction_intervals.h + 1\n",
    "        if np.any(sizes < required_samples):\n",
    "            warnings.warn(\n",
    "                f'Prediction intervals settings require at least {required_samples + offset:,} samples, '\n",
    "                'some series have less and will use less windows.'\n",
    "            )\n",
    "\n",
    "    def _set_prediction_intervals(\n",
    "        self, prediction_intervals: Optional[ConformalIntervals]\n",
    "    ) -> None:\n",
    "        for model in self.models:\n",
    "            interval = getattr(model, \"prediction_intervals\", None)\n",
    "            if interval is None:\n",
    "                setattr(model, \"prediction_intervals\", prediction_intervals)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        df: Optional[DataFrame] = None, \n",
    "        sort_df: bool = True,\n",
    "        prediction_intervals: Optional[ConformalIntervals] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "    ):\n",
    "        \"\"\"Fit statistical models.\n",
    "\n",
    "        Fit `models` to a large set of time series from DataFrame `df`\n",
    "        and store fitted models for later inspection.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        {df}\n",
    "            If None, the `StatsForecast` class should have been instantiated using `df`.\n",
    "        {sort_df}\n",
    "        {prediction_intervals}\n",
    "        {id_col}\n",
    "        {time_col}\n",
    "        {target_col}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : StatsForecast\n",
    "            Returns with stored `StatsForecast` fitted `models`.\n",
    "        \"\"\"\n",
    "        self._prepare_fit(\n",
    "            df=df, sort_df=sort_df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "        )\n",
    "        self._validate_sizes_for_prediction_intervals(prediction_intervals)\n",
    "        self._set_prediction_intervals(prediction_intervals=prediction_intervals)\n",
    "        if self.n_jobs == 1:\n",
    "            self.fitted_ = self.ga.fit(models=self.models, fallback_model=self.fallback_model)\n",
    "        else:\n",
    "            self.fitted_ = self._fit_parallel()\n",
    "        return self\n",
    "\n",
    "    fit.__doc__ = fit.__doc__.format(**_param_descriptions)  # type: ignore[union-attr]\n",
    "    \n",
    "    def _make_future_df(self, h: int):\n",
    "        start_dates = ufp.offset_times(self.last_dates, freq=self.freq, n=1)\n",
    "        dates = ufp.time_ranges(start_dates, freq=self.freq, periods=h)\n",
    "        uids = ufp.repeat(self.uids, n=h)\n",
    "        df = self.df_constructor({self.id_col: uids, self.time_col: dates})\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            if _id_as_idx():\n",
    "                _warn_id_as_idx()\n",
    "                df = df.set_index(self.id_col)\n",
    "            else:\n",
    "                df = df.reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def _parse_X_level(self, h: int, X: Optional[DataFrame], level: Optional[List[int]]):\n",
    "        if level is None:\n",
    "            level = []\n",
    "        if X is None:\n",
    "            return X, level\n",
    "        expected_shape = (h * len(self.ga), self.ga.data.shape[1] + 1)\n",
    "        if X.shape != expected_shape:\n",
    "            raise ValueError(f'Expected X to have shape {expected_shape}, but got {X.shape}')\n",
    "        _, _, data, indptr, _ = ufp.process_df(X, self.id_col, self.time_col, None)\n",
    "        return GroupedArray(data, indptr), level\n",
    "\n",
    "    def _validate_exog(self, X_df: Optional[DataFrame] = None) -> None:\n",
    "        if not any(m.uses_exog for m in self.models) or not self._exog:\n",
    "            return\n",
    "        err_msg = (\n",
    "            f'Models require the following exogenous features {self._exog} '\n",
    "            'for the forecasting step. Please provide them through `X_df`.'\n",
    "        )\n",
    "        if X_df is None:\n",
    "            raise ValueError(err_msg)\n",
    "        missing_exog = [c for c in self._exog if c not in X_df.columns]\n",
    "        if missing_exog:\n",
    "            raise ValueError(err_msg)\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        h: int,\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[int]] = None,\n",
    "    ):\n",
    "        \"\"\"Predict statistical models.\n",
    "\n",
    "        Use stored fitted `models` to predict large set of time series from DataFrame `df`.        \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        {h}\n",
    "        {X_df}\n",
    "        {level}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas or polars DataFrame\n",
    "            DataFrame with `models` columns for point predictions and probabilistic\n",
    "            predictions for all fitted `models`.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'fitted_'):\n",
    "            raise ValueError('You must call the fit method before calling predict.')\n",
    "        if any(getattr(m, 'prediction_intervals', None) is not None for m in self.models) and level is None:\n",
    "            warnings.warn(\n",
    "                \"Prediction intervals are set but `level` was not provided. \"\n",
    "                \"Predictions won't have intervals.\"\n",
    "            )\n",
    "        self._validate_exog(X_df)\n",
    "        X, level = self._parse_X_level(h=h, X=X_df, level=level)\n",
    "        if self.n_jobs == 1:\n",
    "            fcsts, cols = self.ga.predict(fm=self.fitted_, h=h, X=X, level=level)\n",
    "        else:\n",
    "            fcsts, cols = self._predict_parallel(h=h, X=X, level=level)\n",
    "        fcsts_df = self._make_future_df(h=h)\n",
    "        fcsts_df[cols] = fcsts\n",
    "        return fcsts_df\n",
    "\n",
    "    predict.__doc__ = predict.__doc__.format(**_param_descriptions)  # type: ignore[union-attr]\n",
    "    \n",
    "    def fit_predict(\n",
    "        self,\n",
    "        h: int,\n",
    "        df: Optional[DataFrame] = None,\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[int]] = None,\n",
    "        sort_df: bool = True,\n",
    "        prediction_intervals: Optional[ConformalIntervals] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',        \n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Fit and Predict with statistical models.\n",
    "\n",
    "        This method avoids memory burden due from object storage.\n",
    "        It is analogous to Scikit-Learn `fit_predict` without storing information.\n",
    "        It requires the forecast horizon `h` in advance. \n",
    "        \n",
    "        In contrast to `StatsForecast.forecast` this method stores partial models outputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        {h}\n",
    "        {df}\n",
    "            If None, the `StatsForecast` class should have been instantiated using `df`.\n",
    "        {X_df}\n",
    "        {level}\n",
    "        {sort_df}\n",
    "        {prediction_intervals}\n",
    "        {id_col}\n",
    "        {time_col}\n",
    "        {target_col}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas or polars DataFrame\n",
    "            DataFrame with `models` columns for point predictions and probabilistic\n",
    "            predictions for all fitted `models`.\n",
    "        \"\"\"\n",
    "        self._prepare_fit(\n",
    "            df=df, sort_df=sort_df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "        )\n",
    "        self._validate_exog(X_df)\n",
    "        if prediction_intervals is not None and level is None:\n",
    "            raise ValueError('You must specify `level` when using `prediction_intervals`')\n",
    "        self._validate_sizes_for_prediction_intervals(prediction_intervals)            \n",
    "        self._set_prediction_intervals(prediction_intervals=prediction_intervals)\n",
    "        X, level = self._parse_X_level(h=h, X=X_df, level=level)\n",
    "        if self.n_jobs == 1:\n",
    "            self.fitted_, fcsts, cols = self.ga.fit_predict(models=self.models, h=h, X=X, level=level)\n",
    "        else:\n",
    "            self.fitted_, fcsts, cols = self._fit_predict_parallel(h=h, X=X, level=level)\n",
    "        fcsts_df = self._make_future_df(h=h)\n",
    "        fcsts_df[cols] = fcsts\n",
    "        return fcsts_df\n",
    "\n",
    "    fit_predict.__doc__ = fit_predict.__doc__.format(**_param_descriptions)  # type: ignore[union-attr]\n",
    "    \n",
    "    def forecast(\n",
    "        self,\n",
    "        h: int,\n",
    "        df: Optional[DataFrame] = None,\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[int]] = None,\n",
    "        fitted: bool = False,\n",
    "        sort_df: bool = True,\n",
    "        prediction_intervals: Optional[ConformalIntervals] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Memory Efficient predictions.\n",
    "\n",
    "        This method avoids memory burden due from object storage.\n",
    "        It is analogous to Scikit-Learn `fit_predict` without storing information.\n",
    "        It requires the forecast horizon `h` in advance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        {h}\n",
    "        {df}\n",
    "        {X_df}\n",
    "        {level}\n",
    "        {fitted}\n",
    "        {sort_df}\n",
    "        {prediction_intervals}\n",
    "        {id_col}\n",
    "        {time_col}\n",
    "        {target_col}           \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas or polars DataFrame\n",
    "            DataFrame with `models` columns for point predictions and probabilistic\n",
    "            predictions for all fitted `models`.\n",
    "        \"\"\"\n",
    "        self.__dict__.pop('fcst_fitted_values_', None)\n",
    "        self._prepare_fit(\n",
    "            df=df, sort_df=sort_df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "        )\n",
    "        self._validate_exog(X_df)\n",
    "        self._validate_sizes_for_prediction_intervals(prediction_intervals)        \n",
    "        self._set_prediction_intervals(prediction_intervals=prediction_intervals)\n",
    "        X, level = self._parse_X_level(h=h, X=X_df, level=level)\n",
    "        if self.n_jobs == 1:\n",
    "            res_fcsts = self.ga.forecast(\n",
    "                models=self.models, \n",
    "                h=h,\n",
    "                fallback_model=self.fallback_model, \n",
    "                fitted=fitted,\n",
    "                X=X,\n",
    "                level=level, \n",
    "                verbose=self.verbose,\n",
    "                target_col=target_col,\n",
    "            )\n",
    "        else:\n",
    "            res_fcsts = self._forecast_parallel(\n",
    "                h=h,\n",
    "                fitted=fitted,\n",
    "                X=X,\n",
    "                level=level,\n",
    "                target_col=target_col,\n",
    "            )\n",
    "        if fitted:\n",
    "            self.fcst_fitted_values_ = res_fcsts['fitted']\n",
    "        fcsts = res_fcsts['forecasts']\n",
    "        cols = res_fcsts['cols']\n",
    "        fcsts_df = self._make_future_df(h=h)\n",
    "        fcsts_df[cols] = fcsts\n",
    "        self.forecast_times_ = res_fcsts['times']\n",
    "        return fcsts_df\n",
    "\n",
    "    forecast.__doc__ = forecast.__doc__.format(**_param_descriptions)  # type: ignore[union-attr]\n",
    "    \n",
    "    def forecast_fitted_values(self):\n",
    "        \"\"\"Access insample predictions.\n",
    "\n",
    "        After executing `StatsForecast.forecast`, you can access the insample \n",
    "        prediction values for each model. To get them, you need to pass `fitted=True` \n",
    "        to the `StatsForecast.forecast` method and then use the \n",
    "        `StatsForecast.forecast_fitted_values` method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas.DataFrame | polars.DataFrame\n",
    "            DataFrame with insample `models` columns for point predictions and probabilistic\n",
    "            predictions for all fitted `models`.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"fcst_fitted_values_\"):\n",
    "            raise Exception(\"Please run `forecast` method using `fitted=True`\")\n",
    "        cols = self.fcst_fitted_values_[\"cols\"]\n",
    "        df = self.df_constructor({\n",
    "            self.id_col: ufp.repeat(self.uids, np.diff(self.ga.indptr)),\n",
    "            self.time_col: self.og_dates\n",
    "        })\n",
    "        df[cols] = self.fcst_fitted_values_['values']\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            if _id_as_idx():\n",
    "                _warn_id_as_idx()\n",
    "                df = df.set_index(self.id_col)\n",
    "            else:\n",
    "                df = df.reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        h: int,\n",
    "        df: Optional[DataFrame] = None,\n",
    "        n_windows: int = 1,\n",
    "        step_size: int = 1,\n",
    "        test_size: Optional[int] = None,\n",
    "        input_size: Optional[int] = None,\n",
    "        level: Optional[List[int]] = None,\n",
    "        fitted: bool = False,\n",
    "        refit: Union[bool, int] = True,\n",
    "        sort_df: bool = True,\n",
    "        prediction_intervals: Optional[ConformalIntervals] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',        \n",
    "    ) -> DataFrame:\n",
    "        \"\"\"Temporal Cross-Validation.\n",
    "\n",
    "        Efficiently fits a list of `StatsForecast` \n",
    "        models through multiple training windows, in either chained or rolled manner.\n",
    "        \n",
    "        `StatsForecast.models`' speed allows to overcome this evaluation technique \n",
    "        high computational costs. Temporal cross-validation provides better model's \n",
    "        generalization measurements by increasing the test's length and diversity.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        {h}\n",
    "        {df}\n",
    "            If None, the `StatsForecast` class should have been instantiated using `df`.\n",
    "        {n_windows}\n",
    "        {step_size}\n",
    "        {test_size}\n",
    "        {input_size}            \n",
    "        {level}\n",
    "        {fitted}\n",
    "        {refit}\n",
    "        {sort_df}\n",
    "        {prediction_intervals}\n",
    "        {id_col}\n",
    "        {time_col}\n",
    "        {target_col}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas or polars DataFrame\n",
    "            DataFrame with insample `models` columns for point predictions and probabilistic\n",
    "            predictions for all fitted `models`.\n",
    "        \"\"\"\n",
    "        if n_windows is None and test_size is None:\n",
    "            raise ValueError('you must define `n_windows` or `test_size`')\n",
    "        if test_size is None:\n",
    "            test_size = h + step_size * (n_windows - 1)\n",
    "        if prediction_intervals is not None and level is None:\n",
    "            raise ValueError('You must specify `level` when using `prediction_intervals`')\n",
    "        if refit != True:\n",
    "            no_forward = [m for m in self.models if not hasattr(m, 'forward')]\n",
    "            if no_forward:\n",
    "                raise ValueError(\n",
    "                    'Can only use integer refit or refit=False with models that implement the forward method. '\n",
    "                    f'The following models do not implement the forward method: {no_forward}.'\n",
    "                )\n",
    "            if self.fallback_model is not None and not hasattr(self.fallback_model, 'forward'):\n",
    "                raise ValueError(\n",
    "                    'Can only use integer refit or refit=False with a fallback model that implements the forward method.'\n",
    "                )\n",
    "        self.__dict__.pop('cv_fitted_values_', None)\n",
    "        self._prepare_fit(\n",
    "            df=df, sort_df=sort_df, id_col=id_col, time_col=time_col, target_col=target_col\n",
    "        )\n",
    "        series_sizes = np.diff(self.ga.indptr)\n",
    "        short_series = series_sizes <= test_size\n",
    "        if short_series.any():\n",
    "            short_ids = self.uids[short_series].to_numpy().tolist()\n",
    "            raise ValueError(\n",
    "                f\"The following series are too short for the cross validation settings: {reprlib.repr(short_ids)}\\n\"\n",
    "                \"Please remove these series or change the settings, e.g. reducing the horizon or the number of windows.\"\n",
    "            )        \n",
    "        self._validate_sizes_for_prediction_intervals(\n",
    "            prediction_intervals=prediction_intervals, offset=test_size\n",
    "        )\n",
    "        self._set_prediction_intervals(prediction_intervals=prediction_intervals)\n",
    "        _, level = self._parse_X_level(h=h, X=None, level=level)\n",
    "        if self.n_jobs == 1:\n",
    "            res_fcsts = self.ga.cross_validation(\n",
    "                models=self.models, h=h, test_size=test_size, \n",
    "                fallback_model=self.fallback_model, \n",
    "                step_size=step_size, \n",
    "                input_size=input_size, \n",
    "                fitted=fitted,\n",
    "                level=level,\n",
    "                verbose=self.verbose,\n",
    "                refit=refit,\n",
    "                target_col=target_col,\n",
    "            )\n",
    "        else:\n",
    "            res_fcsts = self._cross_validation_parallel(\n",
    "                h=h, \n",
    "                test_size=test_size,\n",
    "                step_size=step_size,\n",
    "                input_size=input_size,\n",
    "                fitted=fitted,\n",
    "                level=level,\n",
    "                refit=refit,\n",
    "                target_col=target_col,\n",
    "            )\n",
    "        if fitted:\n",
    "            self.cv_fitted_values_ = res_fcsts['fitted']\n",
    "            self.n_cv_ = n_windows\n",
    "        fcsts_df = ufp.cv_times(\n",
    "            times=self.og_dates,\n",
    "            uids=self.uids,\n",
    "            indptr=self.ga.indptr,\n",
    "            h=h,\n",
    "            test_size=test_size,\n",
    "            step_size=step_size,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "        )\n",
    "        # the cv_times is sorted by window and then id\n",
    "        fcsts_df = ufp.sort(fcsts_df, [id_col, \"cutoff\", time_col])\n",
    "        fcsts_df = ufp.assign_columns(fcsts_df, res_fcsts[\"cols\"], res_fcsts[\"forecasts\"])\n",
    "        if isinstance(fcsts_df, pd.DataFrame) and _id_as_idx():\n",
    "            _warn_id_as_idx()\n",
    "            fcsts_df = fcsts_df.set_index(id_col)\n",
    "        return fcsts_df\n",
    "\n",
    "    cross_validation.__doc__ = cross_validation.__doc__.format(**_param_descriptions)  # type: ignore[union-attr]\n",
    "\n",
    "    def cross_validation_fitted_values(self) -> DataFrame:\n",
    "        \"\"\"Access insample cross validated predictions.\n",
    "\n",
    "        After executing `StatsForecast.cross_validation`, you can access the insample \n",
    "        prediction values for each model and window. To get them, you need to pass `fitted=True` \n",
    "        to the `StatsForecast.cross_validation` method and then use the \n",
    "        `StatsForecast.cross_validation_fitted_values` method.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fcsts_df : pandas or polars DataFrame\n",
    "            DataFrame with insample `models` columns for point predictions \n",
    "            and probabilistic predictions for all fitted `models`.\n",
    "        \"\"\"        \n",
    "        if not hasattr(self, 'cv_fitted_values_'):\n",
    "            raise Exception('Please run `cross_validation` method using `fitted=True`')\n",
    "        idxs = self.cv_fitted_values_['idxs'].flatten(order='F')            \n",
    "        train_uids = ufp.repeat(self.uids, np.diff(self.ga.indptr))\n",
    "        cv_uids = ufp.vertical_concat([train_uids for _ in range(self.n_cv_)])\n",
    "        used_uids = ufp.take_rows(cv_uids, idxs)\n",
    "        dates = np.tile(self.og_dates, self.n_cv_)[idxs]\n",
    "        cutoffs_mask = self.cv_fitted_values_['last_idxs'].flatten(order='F')[idxs]\n",
    "        cutoffs_sizes = np.diff(np.append(0, np.where(cutoffs_mask)[0] + 1))\n",
    "        cutoffs = np.repeat(dates[cutoffs_mask], cutoffs_sizes)        \n",
    "        df = self.df_constructor({\n",
    "            self.id_col: used_uids,\n",
    "            self.time_col: dates,\n",
    "            'cutoff': cutoffs,\n",
    "        })\n",
    "        fitted_vals = np.reshape(\n",
    "            self.cv_fitted_values_['values'],\n",
    "            (-1, len(self.models) + 1),\n",
    "            order='F',\n",
    "        )\n",
    "        df = ufp.assign_columns(df, self.cv_fitted_values_['cols'], fitted_vals[idxs])\n",
    "        df = ufp.drop_index_if_pandas(df)\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            if _id_as_idx():\n",
    "                _warn_id_as_idx()\n",
    "                df = df.set_index(self.id_col)\n",
    "            else:\n",
    "                df = df.reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def _get_pool(self):\n",
    "        from multiprocessing import Pool\n",
    "\n",
    "        pool_kwargs = dict()\n",
    "        return Pool, pool_kwargs\n",
    "    \n",
    "    def _fit_parallel(self):\n",
    "        gas = self.ga.split(self.n_jobs)\n",
    "        Pool, pool_kwargs = self._get_pool()\n",
    "        with Pool(self.n_jobs, **pool_kwargs) as executor:\n",
    "            futures = []\n",
    "            for ga in gas:\n",
    "                future = executor.apply_async(\n",
    "                    ga._single_threaded_fit,\n",
    "                    (self.models, self.fallback_model)\n",
    "                )\n",
    "                futures.append(future)\n",
    "            fm = np.vstack([f.get() for f in futures])\n",
    "        return fm    \n",
    "    \n",
    "    def _get_gas_Xs(self, X, tasks_per_job=1):\n",
    "        n_chunks = min(tasks_per_job * self.n_jobs, self.ga.n_groups)\n",
    "        gas = self.ga.split(n_chunks)\n",
    "        if X is not None:\n",
    "            Xs = X.split(n_chunks)\n",
    "        else:\n",
    "            from itertools import repeat\n",
    "\n",
    "            Xs = repeat(None)\n",
    "        return gas, Xs\n",
    "    \n",
    "    def _predict_parallel(self, h, X, level):\n",
    "        #create elements for each core\n",
    "        gas, Xs = self._get_gas_Xs(X=X)\n",
    "        fms = self.ga.split_fm(self.fitted_, self.n_jobs)\n",
    "        Pool, pool_kwargs = self._get_pool()\n",
    "        #compute parallel forecasts\n",
    "        with Pool(self.n_jobs, **pool_kwargs) as executor:\n",
    "            futures = []\n",
    "            for ga, fm, X_ in zip(gas, fms, Xs):\n",
    "                future = executor.apply_async(\n",
    "                    ga._single_threaded_predict,\n",
    "                    (fm, h, X_, level),\n",
    "                )\n",
    "                futures.append(future)\n",
    "            out = [f.get() for f in futures]\n",
    "            fcsts, cols = list(zip(*out))\n",
    "            fcsts = np.vstack(fcsts)\n",
    "            cols = cols[0]\n",
    "        return fcsts, cols\n",
    "    \n",
    "    def _fit_predict_parallel(self, h, X, level):\n",
    "        #create elements for each core\n",
    "        gas, Xs = self._get_gas_Xs(X=X)\n",
    "        Pool, pool_kwargs = self._get_pool()\n",
    "        #compute parallel forecasts\n",
    "        with Pool(self.n_jobs, **pool_kwargs) as executor:\n",
    "            futures = []\n",
    "            for ga, X_ in zip(gas, Xs):\n",
    "                future = executor.apply_async(\n",
    "                    ga._single_threaded_fit_predict,\n",
    "                    (self.models, h, X_, level),\n",
    "                )\n",
    "                futures.append(future)\n",
    "            out = [f.get() for f in futures]\n",
    "            fm, fcsts, cols = list(zip(*out))\n",
    "            fm = np.vstack(fm)\n",
    "            fcsts = np.vstack(fcsts)\n",
    "            cols = cols[0]\n",
    "        return fm, fcsts, cols\n",
    "\n",
    "    def _forecast_parallel(self, h, fitted, X, level, target_col):\n",
    "        gas, Xs = self._get_gas_Xs(X=X, tasks_per_job=100)\n",
    "        results = [None] * len(gas)\n",
    "        with ProcessPoolExecutor(self.n_jobs) as executor:\n",
    "            future2pos = {\n",
    "                executor.submit(\n",
    "                    ga._single_threaded_forecast,\n",
    "                    h=h,\n",
    "                    models=self.models,\n",
    "                    fallback_model=self.fallback_model,\n",
    "                    fitted=fitted,\n",
    "                    X=X,\n",
    "                    level=level,\n",
    "                    verbose=False,\n",
    "                    target_col=target_col,\n",
    "                ): i\n",
    "                for i, (ga, X) in enumerate(zip(gas, Xs))\n",
    "            }\n",
    "            iterable = tqdm(\n",
    "                as_completed(future2pos),\n",
    "                disable=not self.verbose,\n",
    "                total=len(future2pos),\n",
    "                desc=\"Forecast\",\n",
    "                bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [Elapsed: {elapsed}{postfix}]\",\n",
    "            )\n",
    "            for future in iterable:\n",
    "                i = future2pos[future]\n",
    "                results[i] = future.result()\n",
    "        result = {\n",
    "            'cols': results[0]['cols'],\n",
    "            'forecasts': np.vstack([r['forecasts'] for r in results]),\n",
    "            'times': {\n",
    "                m: sum(r['times'][m] for r in results)\n",
    "                for m in [repr(m) for m in self.models]\n",
    "            },\n",
    "        }\n",
    "        if fitted:\n",
    "            result['fitted'] = {\n",
    "                'cols': results[0]['fitted']['cols'],\n",
    "                'values': np.vstack([r['fitted']['values'] for r in results]),\n",
    "            }\n",
    "        return result\n",
    "\n",
    "    def _cross_validation_parallel(self, h, test_size, step_size, input_size, fitted, level, refit, target_col):\n",
    "        #create elements for each core\n",
    "        gas = self.ga.split(self.n_jobs)\n",
    "        Pool, pool_kwargs = self._get_pool()\n",
    "        #compute parallel forecasts\n",
    "        result = {}\n",
    "        with Pool(self.n_jobs, **pool_kwargs) as executor:\n",
    "            futures = []\n",
    "            for ga in gas:\n",
    "                future = executor.apply_async(\n",
    "                    ga._single_threaded_cross_validation,\n",
    "                    tuple(),\n",
    "                    dict(\n",
    "                        models=self.models,\n",
    "                        h=h,\n",
    "                        test_size=test_size,\n",
    "                        fallback_model=self.fallback_model,\n",
    "                        step_size=step_size,\n",
    "                        input_size=input_size,\n",
    "                        fitted=fitted,\n",
    "                        level=level,\n",
    "                        refit=refit,\n",
    "                        verbose=self.verbose,\n",
    "                        target_col=target_col,\n",
    "                    ),\n",
    "                )\n",
    "                futures.append(future)\n",
    "            out = [f.get() for f in futures]\n",
    "            fcsts = [d['forecasts'] for d in out]\n",
    "            fcsts = np.vstack(fcsts)\n",
    "            cols = out[0]['cols']\n",
    "            result['forecasts'] = fcsts\n",
    "            result['cols'] = cols\n",
    "            if fitted:\n",
    "                result['fitted'] = {}\n",
    "                result['fitted']['values'] = np.concatenate([d['fitted']['values'] for d in out])\n",
    "                for key in ['last_idxs', 'idxs']:\n",
    "                    result['fitted'][key] = np.concatenate([d['fitted'][key] for d in out])\n",
    "                result['fitted']['cols'] = out[0]['fitted']['cols']\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(\n",
    "        df: DataFrame,\n",
    "        forecasts_df: Optional[DataFrame] = None,\n",
    "        unique_ids: Union[Optional[List[str]], np.ndarray] = None,\n",
    "        plot_random: bool = True, \n",
    "        models: Optional[List[str]] = None, \n",
    "        level: Optional[List[float]] = None,\n",
    "        max_insample_length: Optional[int] = None,\n",
    "        plot_anomalies: bool = False,\n",
    "        engine: str = 'matplotlib',\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        resampler_kwargs: Optional[Dict] = None\n",
    "    ):\n",
    "        \"\"\"Plot forecasts and insample values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        {df}\n",
    "        forecasts_df : pandas or polars DataFrame, optional (default=None)\n",
    "            DataFrame ids, times and models.\n",
    "        unique_ids : list of str, optional (default=None)\n",
    "            ids to plot. If None, they're selected randomly.\n",
    "        plot_random : bool (default=True)\n",
    "            Select time series to plot randomly.\n",
    "        models : List[str], optional (default=None)\n",
    "            List of models to plot.\n",
    "        level : List[float], optional (default=None)\n",
    "            List of prediction intervals to plot if paseed.\n",
    "        max_insample_length : int, optional (default=None)\n",
    "            Max number of train/insample observations to be plotted.\n",
    "        plot_anomalies : bool (default=False)\n",
    "            Plot anomalies for each prediction interval.\n",
    "        engine : str (default='matplotlib')\n",
    "            Library used to plot. 'plotly', 'plotly-resampler' or 'matplotlib'.\n",
    "        {id_col}\n",
    "        {time_col}\n",
    "        {target_col}            \n",
    "        resampler_kwargs : dict\n",
    "            Kwargs to be passed to plotly-resampler constructor. \n",
    "            For further custumization (\"show_dash\") call the method,\n",
    "            store the plotting object and add the extra arguments to\n",
    "            its `show_dash` method.\n",
    "        \"\"\"\n",
    "        from utilsforecast.plotting import plot_series\n",
    "\n",
    "        df = ensure_time_dtype(df, time_col)        \n",
    "        if isinstance(df, pd.DataFrame) and df.index.name == id_col:\n",
    "            warnings.warn(\n",
    "                \"Passing the ids as the index is deprecated. \"\n",
    "                \"Please provide them as a column instead.\",\n",
    "                category=FutureWarning\n",
    "            )\n",
    "            df = df.reset_index()\n",
    "        if forecasts_df is not None:\n",
    "            forecasts_df = ensure_time_dtype(forecasts_df, time_col)            \n",
    "        if isinstance(forecasts_df, pd.DataFrame) and forecasts_df.index.name == id_col:\n",
    "            warnings.warn(\n",
    "                \"Passing the ids as the index is deprecated. \"\n",
    "                \"Please provide them as a column instead.\",\n",
    "                category=FutureWarning\n",
    "            )\n",
    "            forecasts_df = forecasts_df.reset_index()          \n",
    "        return plot_series(\n",
    "            df=df,\n",
    "            forecasts_df=forecasts_df,\n",
    "            ids=unique_ids,\n",
    "            plot_random=plot_random,\n",
    "            models=models,\n",
    "            level=level,\n",
    "            max_insample_length=max_insample_length,\n",
    "            plot_anomalies=plot_anomalies,\n",
    "            engine=engine,\n",
    "            resampler_kwargs=resampler_kwargs,\n",
    "            palette='tab20b',\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "    \n",
    "    def save(\n",
    "        self, \n",
    "        path: Optional[Union[Path, str]] = None,\n",
    "        max_size: Optional[str] = None,\n",
    "        trim: bool = False,\n",
    "    ):\n",
    "        \"\"\"Function that will save StatsForecast class with certain settings to make it \n",
    "        reproducible.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str or pathlib.Path, optional (default=None)\n",
    "            Path of the file to be saved. If `None` will create one in the current \n",
    "            directory using the current UTC timestamp.\n",
    "        max_size : str, optional (default = None)\n",
    "            StatsForecast object should not exceed this size.\n",
    "            Available byte naming: ['B', 'KB', 'MB', 'GB']\n",
    "        trim : bool (default = False)\n",
    "            Delete any attributes not needed for inference.\n",
    "        \"\"\"\n",
    "        # Will be used to find the size of the fitted models\n",
    "        # Never expecting anything higher than GB (even that's a lot')\n",
    "        bytes_hmap = {\n",
    "            \"B\": 1,\n",
    "            \"KB\": 2**10,\n",
    "            \"MB\": 2**20,\n",
    "            \"GB\": 2**30,\n",
    "        }\n",
    "\n",
    "        # Removing unnecessary attributes\n",
    "        # @jmoralez decide future implementation\n",
    "        trim_attr:list = [\"fcst_fitted_values_\", \"cv_fitted_values_\"]\n",
    "        if trim:\n",
    "            for attr in trim_attr:\n",
    "                # remove unnecessary attributes here\n",
    "                self.__dict__.pop(attr, None)\n",
    "\n",
    "        sf_size = len(pickle.dumps(self))\n",
    "\n",
    "        if max_size is not None:\n",
    "            cap_size = self._get_cap_size(max_size, bytes_hmap)\n",
    "            if sf_size >= cap_size:\n",
    "                err_messg = \"StatsForecast is larger than the specified max_size\"\n",
    "                raise OSError(errno.EFBIG, err_messg) \n",
    "\n",
    "        converted_size, sf_byte = None, None\n",
    "        for key in reversed(list(bytes_hmap.keys())):\n",
    "            x_byte = bytes_hmap[key]\n",
    "            if sf_size >= x_byte:\n",
    "                converted_size = sf_size / x_byte\n",
    "                sf_byte = key\n",
    "                break\n",
    "    \n",
    "        if converted_size is None or sf_byte is None:\n",
    "            err_messg = \"Internal Error, this shouldn't happen, please open an issue\"\n",
    "            raise RuntimeError(err_messg)\n",
    "    \n",
    "        print(f\"Saving StatsForecast object of size {converted_size:.2f}{sf_byte}.\")\n",
    "    \n",
    "        if path is None:\n",
    "            datetime_record = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            path = f\"StatsForecast_{datetime_record}.pkl\"\n",
    "    \n",
    "        with open(path, \"wb\") as m_file:\n",
    "            pickle.dump(self, m_file)\n",
    "        print(\"StatsForecast object saved\")\n",
    "\n",
    "    def _get_cap_size(self, max_size, bytes_hmap):\n",
    "        max_size = max_size.upper().replace(\" \", \"\")\n",
    "        match = re.match(r'(\\d+\\.\\d+|\\d+)(\\w+)', max_size)\n",
    "        if match is None or len(match.groups()) < 2 or match[2] not in bytes_hmap.keys():\n",
    "            parsing_error = \"Couldn't parse `max_size`, it should be `None`\", \\\n",
    "            \" or a number followed by one of the following units: ['B', 'KB', 'MB', 'GB']\"\n",
    "            raise ValueError(parsing_error)\n",
    "        else:\n",
    "            m_size = float(match[1])\n",
    "            key_ = match[2]\n",
    "            cap_size = m_size * bytes_hmap[key_]\n",
    "        return cap_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path:Union[Path, str]):\n",
    "        \"\"\"\n",
    "        Automatically loads the model into ready StatsForecast.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : str or pathlib.Path\n",
    "            Path to saved StatsForecast file.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        sf: StatsForecast\n",
    "            Previously saved StatsForecast\n",
    "        \"\"\"\n",
    "        if not Path(path).exists():\n",
    "            raise ValueError(\"Specified path does not exist, check again and retry.\")\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"StatsForecast(models=[{','.join(map(repr, self.models))}])\"\n",
    "\n",
    "_StatsForecast.plot.__doc__ = _StatsForecast.plot.__doc__.format(**_param_descriptions)  # type: ignore[union-attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59672b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class ParallelBackend:\n",
    "    def forecast(\n",
    "        self,\n",
    "        *,\n",
    "        models,\n",
    "        fallback_model,        \n",
    "        freq,\n",
    "        h,        \n",
    "        df,\n",
    "        X_df,\n",
    "        level,\n",
    "        fitted,\n",
    "        prediction_intervals,\n",
    "        id_col,\n",
    "        time_col,\n",
    "        target_col,\n",
    "    ) -> Any:\n",
    "        model = _StatsForecast(\n",
    "            models=models,\n",
    "            freq=freq,\n",
    "            fallback_model=fallback_model,\n",
    "        )\n",
    "        return model.forecast(\n",
    "            df=df,\n",
    "            h=h,\n",
    "            X_df=X_df,\n",
    "            level=level,\n",
    "            fitted=fitted,\n",
    "            prediction_intervals=prediction_intervals,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,            \n",
    "        )\n",
    "\n",
    "    def cross_validation(\n",
    "        self,\n",
    "        *,\n",
    "        df,\n",
    "        models,\n",
    "        freq,\n",
    "        fallback_model,\n",
    "        h,\n",
    "        n_windows,\n",
    "        step_size,\n",
    "        test_size,\n",
    "        input_size,\n",
    "        level,\n",
    "        refit,\n",
    "        fitted,\n",
    "        prediction_intervals,\n",
    "        id_col,\n",
    "        time_col,\n",
    "        target_col,        \n",
    "    ) -> Any:\n",
    "        model = _StatsForecast(\n",
    "            models=models,\n",
    "            freq=freq,\n",
    "            fallback_model=fallback_model,\n",
    "        )\n",
    "        return model.cross_validation(\n",
    "            df=df,\n",
    "            h=h,\n",
    "            n_windows=n_windows,\n",
    "            step_size=step_size,\n",
    "            test_size=test_size,\n",
    "            input_size=input_size,\n",
    "            level=level,\n",
    "            refit=refit,\n",
    "            fitted=fitted,\n",
    "            prediction_intervals=prediction_intervals,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,             \n",
    "        )\n",
    "\n",
    "@conditional_dispatcher\n",
    "def make_backend(obj:Any, *args:Any, **kwargs:Any) -> ParallelBackend:\n",
    "    return ParallelBackend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0770f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class StatsForecast(_StatsForecast):\n",
    "    def forecast(\n",
    "        self,\n",
    "        h: int,\n",
    "        df: Any = None,\n",
    "        X_df: Optional[DataFrame] = None,\n",
    "        level: Optional[List[int]] = None,\n",
    "        fitted: bool = False,\n",
    "        sort_df: bool = True,\n",
    "        prediction_intervals: Optional[ConformalIntervals] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "    ):\n",
    "        if prediction_intervals is not None and level is None:\n",
    "            raise ValueError('You must specify `level` when using `prediction_intervals`')\n",
    "        if self._is_native(df=df):\n",
    "            return super().forecast(\n",
    "                df=df,                \n",
    "                h=h,\n",
    "                X_df=X_df,\n",
    "                level=level,\n",
    "                fitted=fitted,\n",
    "                sort_df=sort_df,\n",
    "                prediction_intervals=prediction_intervals,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,                \n",
    "            )\n",
    "        assert df is not None\n",
    "        engine = make_execution_engine(infer_by=[df])\n",
    "        self._backend = make_backend(engine)\n",
    "        return self._backend.forecast(\n",
    "            models=self.models,\n",
    "            fallback_model=self.fallback_model,            \n",
    "            freq=self.freq,            \n",
    "            df=df,\n",
    "            h=h,\n",
    "            X_df=X_df,\n",
    "            level=level,\n",
    "            fitted=fitted,\n",
    "            prediction_intervals=prediction_intervals,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "        )\n",
    "\n",
    "    def forecast_fitted_values(self):\n",
    "        if hasattr(self, '_backend'):\n",
    "            res = self._backend.forecast_fitted_values()\n",
    "        else:\n",
    "            res = super().forecast_fitted_values()\n",
    "        return res\n",
    "    \n",
    "    def cross_validation(\n",
    "        self,\n",
    "        h: int,\n",
    "        df: Any = None,\n",
    "        n_windows: int = 1,\n",
    "        step_size: int = 1,\n",
    "        test_size: Optional[int] = None,\n",
    "        input_size: Optional[int] = None,\n",
    "        level: Optional[List[int]] = None,\n",
    "        fitted: bool = False,\n",
    "        refit: Union[bool, int] = True,\n",
    "        sort_df: bool = True,\n",
    "        prediction_intervals: Optional[ConformalIntervals] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',        \n",
    "    ):\n",
    "        if self._is_native(df=df):\n",
    "            return super().cross_validation(\n",
    "                h=h,\n",
    "                df=df,\n",
    "                n_windows=n_windows,\n",
    "                step_size=step_size,\n",
    "                test_size=test_size,\n",
    "                input_size=input_size,\n",
    "                level=level,\n",
    "                fitted=fitted,\n",
    "                refit=refit,\n",
    "                sort_df=sort_df,\n",
    "                prediction_intervals=prediction_intervals,\n",
    "                id_col=id_col,\n",
    "                time_col=time_col,\n",
    "                target_col=target_col,                \n",
    "            )\n",
    "        assert df is not None\n",
    "        engine = make_execution_engine(infer_by=[df])\n",
    "        backend = make_backend(engine)\n",
    "        return backend.cross_validation(\n",
    "            df=df,\n",
    "            models=self.models,\n",
    "            freq=self.freq,\n",
    "            fallback_model=self.fallback_model,\n",
    "            h=h,\n",
    "            n_windows=n_windows,\n",
    "            step_size=step_size,\n",
    "            test_size=test_size,\n",
    "            input_size=input_size,\n",
    "            level=level,\n",
    "            refit=refit,\n",
    "            fitted=fitted,\n",
    "            prediction_intervals=prediction_intervals,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,            \n",
    "        )\n",
    "\n",
    "    def _is_native(self, df) -> bool:\n",
    "        engine = try_get_context_execution_engine()\n",
    "        return engine is None and (df is None or isinstance(df, pd.DataFrame) or isinstance(df, pl_DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ad5ce-010c-44fb-9e92-3e364ca292b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(StatsForecast, title_level=2, name='StatsForecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916780f5-67eb-47ef-a6c5-55bde8232220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatsForecast's class usage example\n",
    "\n",
    "#from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import ( \n",
    "    ADIDA,\n",
    "    AutoARIMA,\n",
    "    CrostonClassic,\n",
    "    CrostonOptimized,\n",
    "    CrostonSBA,\n",
    "    HistoricAverage,\n",
    "    IMAPA,\n",
    "    Naive,\n",
    "    RandomWalkWithDrift,\n",
    "    SeasonalExponentialSmoothing,\n",
    "    SeasonalNaive,\n",
    "    SeasonalWindowAverage,\n",
    "    SimpleExponentialSmoothing,\n",
    "    TSB,\n",
    "    WindowAverage,\n",
    "    DynamicOptimizedTheta,\n",
    "    AutoETS,\n",
    "    AutoCES\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic panel DataFrame for example\n",
    "panel_df = generate_series(n_series=9, equal_ends=False, engine='pandas')\n",
    "panel_df.groupby('unique_id').tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c3890-9006-4071-9f96-aea42ef62db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'NIXTLA_ID_AS_COL' in os.environ:\n",
    "    del os.environ['NIXTLA_ID_AS_COL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb857aba-763a-4f1f-9029-e360161b0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# id as index warnings\n",
    "fcst = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst.fit(df=panel_df)\n",
    "with warnings.catch_warnings(record=True) as issued_warnings:\n",
    "    warnings.simplefilter('always', category=FutureWarning)\n",
    "    std_preds = fcst.predict(h=1)\n",
    "    std_preds2 = fcst.fit_predict(df=panel_df, h=1)\n",
    "    std_fcst = fcst.forecast(df=panel_df, h=1, fitted=True)\n",
    "    std_fitted = fcst.forecast_fitted_values()\n",
    "    std_cv = fcst.cross_validation(df=panel_df, h=1, fitted=True)\n",
    "    std_fitted_cv = fcst.cross_validation_fitted_values()\n",
    "assert len(issued_warnings) == 6\n",
    "assert all('the predictions will have the id as a column' in str(w.message) for w in issued_warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e3746-68d2-4038-8926-240ab1e08b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NIXTLA_ID_AS_COL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7069ba1-f9c6-40bf-9270-dcc0af4a7863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# if we train with a model that uses exog we must provide them through X_df\n",
    "# otherwise an error will be raised indicating this\n",
    "panel_with_exog = panel_df[panel_df['unique_id'] == 0].copy()\n",
    "panel_with_exog['month'] = panel_df['ds'].dt.month\n",
    "sf = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=12)],\n",
    "    freq='M',\n",
    ")\n",
    "sf.fit(panel_with_exog)\n",
    "expected_msg = \"['month'] for the forecasting step. Please provide them through `X_df`\"\n",
    "test_fail(\n",
    "    lambda: sf.predict(h=12),\n",
    "    contains=expected_msg,\n",
    ")\n",
    "test_fail(\n",
    "    lambda: sf.forecast(df=panel_with_exog, h=12),\n",
    "    contains=expected_msg,\n",
    ")\n",
    "test_fail(\n",
    "    lambda: sf.fit_predict(df=panel_with_exog, h=12),\n",
    "    contains=expected_msg,\n",
    ")\n",
    "# if the models don't use exog then it continues\n",
    "sf = StatsForecast(\n",
    "    models=[SeasonalNaive(season_length=10), Naive()],\n",
    "    freq='M',\n",
    ")\n",
    "sf.fit(panel_with_exog)\n",
    "_ = sf.predict(h=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e1ef7c-dc3f-48ae-a40e-2bde2e068648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# checks for sizes with prediction intervals\n",
    "fcst = StatsForecast(models=[Naive()], freq='D')\n",
    "intervals = ConformalIntervals(n_windows=4, h=10)\n",
    "# intervals require 41 samples, with 30 we can use 2 windows, should warn\n",
    "with warnings.catch_warnings(record=True) as issued_warnings:\n",
    "    fcst.fit(df=panel_df.head(30), prediction_intervals=intervals)\n",
    "assert 'will use less windows' in str(issued_warnings[0].message)\n",
    "assert fcst.fitted_[0, 0]._cs.shape[0] == 2\n",
    "# if we have less than 21 samples (2 windows, h = 10 + 1 for training) it should fail\n",
    "test_fail(\n",
    "    lambda: fcst.fit(df=panel_df.head(20), prediction_intervals=intervals),\n",
    "    contains='Please remove them or adjust the horizon',\n",
    ")\n",
    "# for CV it should consider the test size (20 for CV, 21 for intervals)\n",
    "test_fail(\n",
    "    lambda: fcst.cross_validation(\n",
    "        df=panel_df.head(40),\n",
    "        n_windows=2,\n",
    "        step_size=10,\n",
    "        h=10,\n",
    "        prediction_intervals=intervals,\n",
    "        level=[80],\n",
    "    ),\n",
    "    contains='Minimum samples for computing prediction intervals are 41',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd4821-c96a-4b36-9b45-1a182d1d1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# integer refit or refit=False raises errors for unsupported models\n",
    "fcst = StatsForecast(models=[Naive(), SeasonalNaive(season_length=7)], freq='D')\n",
    "test_fail(\n",
    "    lambda: fcst.cross_validation(df=panel_df, h=8, n_windows=4, refit=2),\n",
    "    contains='implement the forward method: [SeasonalNaive]'\n",
    ")\n",
    "test_fail(\n",
    "    lambda: fcst.cross_validation(df=panel_df, h=8, n_windows=4, refit=False),\n",
    "    contains='implement the forward method: [SeasonalNaive]'\n",
    ")\n",
    "fcst = StatsForecast(models=[Naive()], freq='D', fallback_model=SeasonalNaive(season_length=7))\n",
    "test_fail(\n",
    "    lambda: fcst.cross_validation(df=panel_df, h=8, n_windows=4, refit=2),\n",
    "    contains='a fallback model that implements the forward method.'\n",
    ")\n",
    "test_fail(\n",
    "    lambda: fcst.cross_validation(df=panel_df, h=8, n_windows=4, refit=False),\n",
    "    contains='a fallback model that implements the forward method.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b993ae-24eb-4f56-a860-5108746ec991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# non standard colnames\n",
    "renamer = {'unique_id': 'uid', 'ds': 'time', 'y': 'target'}\n",
    "kwargs = dict(id_col='uid', time_col='time', target_col='target')\n",
    "inverse_renamer = {v: k for k, v in renamer.items()}\n",
    "non_std_df = panel_df.rename(columns=renamer)\n",
    "\n",
    "def assert_equal_results(df1, df2):\n",
    "    pd.testing.assert_frame_equal(\n",
    "        df1.reset_index(),\n",
    "        df2.rename(columns=inverse_renamer),\n",
    "    )\n",
    "\n",
    "fcst = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst.fit(df=non_std_df, **kwargs)\n",
    "non_std_preds = fcst.predict(h=1)\n",
    "non_std_preds2 = fcst.fit_predict(df=non_std_df, h=1, **kwargs)\n",
    "non_std_fcst = fcst.forecast(df=non_std_df, h=1, fitted=True, **kwargs)\n",
    "non_std_fitted = fcst.forecast_fitted_values()\n",
    "non_std_cv = fcst.cross_validation(df=non_std_df, h=1, fitted=True, **kwargs)\n",
    "non_std_fitted_cv = fcst.cross_validation_fitted_values()\n",
    "\n",
    "assert_equal_results(std_preds, non_std_preds)\n",
    "assert_equal_results(std_preds2, non_std_preds2)\n",
    "assert_equal_results(std_fcst, non_std_fcst)\n",
    "assert_equal_results(std_fitted, non_std_fitted)\n",
    "assert_equal_results(std_cv, non_std_cv)\n",
    "assert_equal_results(std_fitted_cv, non_std_fitted_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare list of instantiated StatsForecast estimators to be fitted\n",
    "# You can try other estimator's hyperparameters\n",
    "# You can try other methods from the `models.StatsForecast` collection\n",
    "# Check them here: https://nixtla.github.io/statsforecast/models.html\n",
    "models=[AutoARIMA(), Naive(), \n",
    "        AutoETS(), AutoARIMA(allowmean=True, alias='MeanAutoARIMA')] \n",
    "\n",
    "# Instantiate StatsForecast class\n",
    "fcst = StatsForecast(models=models,\n",
    "                     freq='D',\n",
    "                     n_jobs=1,\n",
    "                     verbose=True)\n",
    "\n",
    "# Efficiently predict\n",
    "fcsts_df = fcst.forecast(df=panel_df, h=4, fitted=True)\n",
    "fcsts_df.groupby('unique_id').tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b00e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Testing save and load \n",
    "import tempfile\n",
    "from polars.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9eaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    f_path = Path(td).joinpath(\"sf_test.pickle\")\n",
    "    \n",
    "    test_df = generate_series(n_series=9, equal_ends=False, engine='polars')\n",
    "    test_frcs = StatsForecast(\n",
    "        models=models,\n",
    "        freq='1d', \n",
    "        n_jobs=1, \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    origin_df = test_frcs.forecast(df=test_df, h=4, fitted=True)\n",
    "\n",
    "    test_frcs.save(f_path)\n",
    "\n",
    "    sf_test = StatsForecast.load(f_path)\n",
    "    load_df = sf_test.forecast(df=test_df, h=4, fitted=True)\n",
    "    \n",
    "    assert_frame_equal(origin_df, load_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b763a-43bc-488c-a9ab-c3c46f5123de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test custom names\n",
    "test_eq(\n",
    "    fcsts_df.columns[-1],\n",
    "    'MeanAutoARIMA'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc392e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test no duplicate names\n",
    "test_fail(lambda: StatsForecast(models=[Naive(), Naive()], freq=\"D\"))\n",
    "StatsForecast(models=[Naive(), Naive(alias=\"Naive2\")], freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6995824-162d-489a-a983-a46bdbcaace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig = StatsForecast.plot(panel_df, max_insample_length=10)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7256d-0c93-4109-8bda-dd44377c0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_fail(\n",
    "    StatsForecast.plot, \n",
    "    contains='Please use a list',\n",
    "    kwargs={'df': panel_df, 'level': 90}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557cc43-dc8e-4e55-a20d-7b0f23689532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df, fcsts_df, engine='matplotlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8632b014-02fd-4afb-bb58-80fdea8ba3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test plot with ds as object\n",
    "panel_df['ds'] = panel_df['ds'].astype(str)\n",
    "fcst.plot(panel_df, fcsts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a36d0-bb8d-4a12-bc4e-9e7f35429ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcsts_df = fcst.forecast(df=panel_df, h=4, fitted=True, level=[90, 80, 30])\n",
    "fcsts_df.groupby('unique_id').tail(4)\n",
    "fcst.plot(panel_df, fcsts_df, models=['AutoARIMA', 'AutoETS'], level=[90, 80], max_insample_length=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f936cc7-cbe6-4537-a4a0-c7d3d14b993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(fcst.forecast_fitted_values(),\n",
    "          forecasts_df=fcsts_df,\n",
    "          models=['AutoARIMA', 'AutoETS'], level=[80], \n",
    "          max_insample_length=20,\n",
    "          plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c379b1d-2814-41ac-9411-5cb14672fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df, fcsts_df, models=['AutoARIMA', 'Naive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9feaa92-1cfe-442f-8739-e7d0525f6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df, fcsts_df, models=['AutoARIMA', 'Naive'], max_insample_length=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a90245-b58d-4eac-8de2-b9f17e853144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df.query('unique_id in [0, 1]'), fcsts_df, models=['AutoARIMA', 'Naive'], level=[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68458156-deb2-42e9-b345-91ca35d57c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df, fcsts_df, unique_ids=[0, 1], models=['AutoARIMA', 'Naive'], level=[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc58040-082e-4d91-bb22-7cc237eef122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df.query('unique_id == 0'), fcsts_df, models=['AutoARIMA', 'Naive'], level=[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b9d20-8404-4afa-ac6e-f988367c58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df, fcsts_df.query('unique_id == 0'), models=['AutoARIMA', 'Naive'], level=[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a504079-8997-4b3f-bc72-143e31b5328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df, fcsts_df, unique_ids=[0], models=['AutoARIMA', 'Naive'], level=[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd7687e-d7ec-405e-8f26-7f6c31945b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df.query('unique_id in [0, 1, 3]'), fcsts_df, models=['AutoARIMA', 'Naive'], level=[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88986dbc-e855-4d4d-ba96-f0519649bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.plot(panel_df, fcsts_df, unique_ids=[0, 1, 2], level=[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336dc6d6-7988-4ea0-9490-a6d9703d98e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig = fcst.plot(panel_df, fcsts_df, unique_ids=[0, 1, 2, 3, 4], models=['AutoARIMA', 'Naive'], level=[90])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861aa6e1-e1e4-4dde-bb83-cfe9d3e225ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fig = fcst.plot(\n",
    "    panel_df, fcsts_df, unique_ids=[0, 1, 2, 3, 4], \n",
    "    models=['AutoARIMA', 'Naive'], \n",
    "    level=[90],\n",
    "    engine='matplotlib'\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test model prediction_interval overrides\n",
    "models=[SimpleExponentialSmoothing(alpha=0.1, prediction_intervals=ConformalIntervals(h=24, n_windows=2))]\n",
    "fcst = StatsForecast(models=models, freq='D', n_jobs=1)\n",
    "fcst._set_prediction_intervals(None)\n",
    "assert models[0].prediction_intervals is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeecb2b-8f99-40fb-9504-944ced7cb677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst = StatsForecast(models=[AutoARIMA(season_length=7)],\n",
    "                     freq='D', \n",
    "                     n_jobs=1, \n",
    "                     verbose=True)\n",
    "fcsts_df = fcst.forecast(df=panel_df, h=4, fitted=True, level=[90])\n",
    "fcsts_df.groupby('unique_id').tail(4)\n",
    "fitted_vals = fcst.forecast_fitted_values()\n",
    "fcst.plot(panel_df, fitted_vals.drop(columns='y'), level=[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02f1ef5-6fd0-4f37-82b1-6ddbb98d08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_StatsForecast.fit, \n",
    "         title_level=2,\n",
    "         name='StatsForecast.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c1811-abbc-4be0-8eff-df3b35a07398",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_StatsForecast.predict, \n",
    "         title_level=2,\n",
    "         name='SatstForecast.predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df99d1-604c-4ca8-a27a-f23e230073e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_StatsForecast.fit_predict, \n",
    "         title_level=2,\n",
    "         name='StatsForecast.fit_predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33759814-8ae5-4973-a329-9db88ae542bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_StatsForecast.forecast, title_level=2, name='StatsForecast.forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d838629-e055-47c3-807f-0c86a8e5143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatsForecast.forecast method usage example\n",
    "\n",
    "#from statsforecast.core import StatsForecast\n",
    "from statsforecast.utils import AirPassengersDF as panel_df\n",
    "from statsforecast.models import AutoARIMA, Naive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0fe3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StatsForecast class\n",
    "fcst = StatsForecast(models=[AutoARIMA(), Naive()],\n",
    "                     freq='D', n_jobs=1)\n",
    "\n",
    "# Efficiently predict without storing memory\n",
    "fcsts_df = fcst.forecast(df=panel_df, h=4, fitted=True)\n",
    "fcsts_df.groupby('unique_id').tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdff618-884a-43fd-a4f9-1bb88be8cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "series = generate_series(100, n_static_features=2, equal_ends=False)\n",
    "\n",
    "models = [\n",
    "    ADIDA(), CrostonClassic(), CrostonOptimized(),\n",
    "    CrostonSBA(), HistoricAverage(), \n",
    "    IMAPA(), Naive(), \n",
    "    RandomWalkWithDrift(), \n",
    "    SeasonalExponentialSmoothing(season_length=7, alpha=0.1),\n",
    "    SeasonalNaive(season_length=7),\n",
    "    SeasonalWindowAverage(season_length=7, window_size=4),\n",
    "    SimpleExponentialSmoothing(alpha=0.1),\n",
    "    TSB(alpha_d=0.1, alpha_p=0.3),\n",
    "    WindowAverage(window_size=4)\n",
    "]\n",
    "\n",
    "fcst = StatsForecast(\n",
    "    models=models,\n",
    "    freq='D',\n",
    "    n_jobs=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "res = fcst.forecast(df=series, h=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150499d-929f-42f1-866e-54793123835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test series without ds as datetime\n",
    "series_wo_dt = series.copy()\n",
    "series_wo_dt['ds'] = series_wo_dt['ds'].astype(str) \n",
    "fcst = StatsForecast(models=models, freq='D')\n",
    "fcsts_wo_dt = fcst.forecast(df=series_wo_dt, h=14)\n",
    "test_eq(res, fcsts_wo_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(res['unique_id'].unique(), fcst.uids.values)\n",
    "last_dates = series.groupby('unique_id')['ds'].max()\n",
    "test_eq(res.groupby('unique_id')['ds'].min().values, last_dates + pd.offsets.Day())\n",
    "test_eq(res.groupby('unique_id')['ds'].max().values, last_dates + 14 * pd.offsets.Day())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d35ac3-8e2b-44cf-a84c-4093d91731d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#tests for monthly data\n",
    "monthly_series = generate_series(10_000, freq='M', min_length=10, max_length=20, equal_ends=True)\n",
    "monthly_series\n",
    "\n",
    "fcst = StatsForecast(\n",
    "    models=[Naive()],\n",
    "    freq='M'\n",
    ")\n",
    "monthly_res = fcst.forecast(df=monthly_series, h=4)\n",
    "monthly_res\n",
    "\n",
    "last_dates = monthly_series.groupby('unique_id')['ds'].max()\n",
    "test_eq(monthly_res.groupby('unique_id')['ds'].min().values, pd.Series(fcst.last_dates) + pd.offsets.MonthEnd())\n",
    "test_eq(monthly_res.groupby('unique_id')['ds'].max().values, pd.Series(fcst.last_dates) + 4 * pd.offsets.MonthEnd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e988f-89cb-47fa-a170-7e1062acdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_StatsForecast.forecast_fitted_values, \n",
    "         title_level=2, \n",
    "         name='StatsForecast.forecast_fitted_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70087b0a-eede-42b7-8af1-0a929cb2c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatsForecast.forecast_fitted_values method usage example\n",
    "\n",
    "#from statsforecast.core import StatsForecast\n",
    "from statsforecast.utils import AirPassengersDF as panel_df\n",
    "from statsforecast.models import Naive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f42031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StatsForecast class\n",
    "fcst = StatsForecast(models=[AutoARIMA()], freq='D', n_jobs=1)\n",
    "\n",
    "# Access insample predictions\n",
    "fcsts_df = fcst.forecast(df=panel_df, h=12, fitted=True, level=(90, 10))\n",
    "insample_fcsts_df = fcst.forecast_fitted_values()\n",
    "insample_fcsts_df.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4be34-72c4-4ff7-b8c5-ccaa848fcde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#tests for fitted values\n",
    "def test_fcst_fitted(series, n_jobs=1, str_ds=False):\n",
    "    if str_ds:\n",
    "        series = series.copy()\n",
    "        series['ds'] = series['ds'].astype(str)\n",
    "    fitted_fcst = StatsForecast(\n",
    "        models=[Naive()],\n",
    "        freq='D',\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "    fitted_res = fitted_fcst.forecast(df=series, h=14, fitted=True)\n",
    "    fitted = fitted_fcst.forecast_fitted_values()\n",
    "    if str_ds:\n",
    "        test_eq(pd.to_datetime(series['ds']), fitted['ds'])\n",
    "    else:\n",
    "        test_eq(series['ds'], fitted['ds'])\n",
    "    test_eq(series['y'], fitted['y'])\n",
    "test_fcst_fitted(series)\n",
    "test_fcst_fitted(series, str_ds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0204f2d-3055-4489-a0b3-bf62267d064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#tests for fallback model\n",
    "def test_fcst_fallback_model(n_jobs=1):\n",
    "    fitted_fcst = StatsForecast(\n",
    "        models=[NullModel()],\n",
    "        freq='D',\n",
    "        n_jobs=n_jobs,\n",
    "        fallback_model=Naive()\n",
    "    )\n",
    "    fitted_res = fitted_fcst.forecast(df=series, h=14, fitted=True)\n",
    "    fitted = fitted_fcst.forecast_fitted_values()\n",
    "    test_eq(series['ds'], fitted['ds'])\n",
    "    test_eq(series['y'], fitted['y'])\n",
    "    # test NullModel actualy fails\n",
    "    fitted_fcst = StatsForecast(\n",
    "        models=[NullModel()],\n",
    "        freq='D',\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "    test_fail(lambda: fitted_fcst.forecast(df=series, h=14))\n",
    "test_fcst_fallback_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89108e-7585-4008-ac14-dee7a1d73c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_StatsForecast.cross_validation, \n",
    "         title_level=2, \n",
    "         name='StatsForecast.cross_validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aba573-eda2-402c-8c3b-efbdae53cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatsForecast.crossvalidation method usage example\n",
    "\n",
    "#from statsforecast.core import StatsForecast\n",
    "from statsforecast.utils import AirPassengersDF as panel_df\n",
    "from statsforecast.models import Naive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957554d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StatsForecast class\n",
    "fcst = StatsForecast(models=[Naive()],\n",
    "                     freq='D', n_jobs=1, verbose=True)\n",
    "\n",
    "# Access insample predictions\n",
    "rolled_fcsts_df = fcst.cross_validation(df=panel_df, h=14, n_windows=2)\n",
    "rolled_fcsts_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22bc745-b4e7-454e-9044-f5b9dfd6f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test for cross_validation\n",
    "series_cv = pd.DataFrame({\n",
    "    'unique_id': np.array(10 * ['id_0'] + 100 * ['id_1'] + 20 * ['id_2']),    \n",
    "    'ds': np.hstack([\n",
    "        pd.date_range(end='2021-01-01', freq='D', periods=10),\n",
    "        pd.date_range(end='2022-01-01', freq='D', periods=100),\n",
    "        pd.date_range(end='2020-01-01', freq='D', periods=20)\n",
    "    ]),\n",
    "    'y': np.hstack([np.arange(10.), np.arange(100, 200), np.arange(20, 40)]),\n",
    "})\n",
    "\n",
    "fcst = StatsForecast(\n",
    "    models=[SumAhead(), Naive()],\n",
    "    freq='D',\n",
    "    verbose=True,\n",
    ")\n",
    "res_cv = fcst.cross_validation(df=series_cv, h=2, test_size=5, n_windows=None, level=(50, 60))\n",
    "test_eq(0., np.mean(res_cv['y'] - res_cv['SumAhead']))\n",
    "\n",
    "n_windows = fcst.cross_validation(df=series_cv, h=2, n_windows=2).groupby('unique_id').size().unique()\n",
    "test_eq(n_windows, 2 * 2)\n",
    "test_eq(0., np.mean(res_cv['y'] - res_cv['SumAhead']))\n",
    "\n",
    "n_windows = fcst.cross_validation(df=series_cv, h=3, n_windows=3, step_size=3, fitted=True).groupby('unique_id').size().unique()\n",
    "test_eq(n_windows, 3 * 3)\n",
    "test_eq(0., np.mean(res_cv['y'] - res_cv['SumAhead']))\n",
    "\n",
    "test_fail(lambda: fcst.cross_validation(df=series_cv, h=10), contains=\"The following series are too short for the cross validation settings: ['id_0']\")\n",
    "test_fail(lambda: fcst.cross_validation(df=series_cv, h=20), contains=\"The following series are too short for the cross validation settings: ['id_0', 'id_2']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22064439-bf92-465d-98c2-02b80b256464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test cross validation refit=False\n",
    "fcst = StatsForecast(\n",
    "    models=[SumAhead()],\n",
    "    freq='D',\n",
    "    verbose=True\n",
    ")\n",
    "res_cv_wo_refit = fcst.cross_validation(\n",
    "    df=series_cv,\n",
    "    h=2,\n",
    "    test_size=5,\n",
    "    n_windows=None,\n",
    "    level=(50, 60),\n",
    "    refit=False,\n",
    ")\n",
    "test_fail(test_eq, args=(res_cv_wo_refit, res_cv))\n",
    "cols_wo_refit = res_cv_wo_refit.columns\n",
    "test_eq(res_cv_wo_refit.groupby('unique_id').head(1), res_cv[cols_wo_refit].groupby('unique_id').head(1))\n",
    "\n",
    "n_windows = fcst.cross_validation(\n",
    "    df=series_cv,\n",
    "    h=2,\n",
    "    n_windows=2,\n",
    "    refit=False,\n",
    ").groupby('unique_id').size().unique()\n",
    "test_eq(n_windows, 2 * 2)\n",
    "\n",
    "n_windows = fcst.cross_validation(\n",
    "    df=series_cv,\n",
    "    h=3,\n",
    "    n_windows=3,\n",
    "    step_size=3,\n",
    "    fitted=True,\n",
    "    refit=False,\n",
    ").groupby('unique_id').size().unique()\n",
    "test_eq(n_windows, 3 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3a183-30bc-4b4b-99dc-dfb1cd4c5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test cross validation refit=False\n",
    "fcst = StatsForecast(\n",
    "    models=[DynamicOptimizedTheta(), AutoCES(), \n",
    "            DynamicOptimizedTheta(season_length=7, alias='test')],\n",
    "    freq='D',\n",
    "    verbose=True\n",
    ")\n",
    "res_cv_wo_refit = fcst.cross_validation(\n",
    "    df=series_cv,\n",
    "    h=2,\n",
    "    test_size=5,\n",
    "    n_windows=None,\n",
    "    level=(50, 60),\n",
    "    refit=False,\n",
    ")\n",
    "res_cv_w_refit = fcst.cross_validation(\n",
    "    df=series_cv,\n",
    "    h=2,\n",
    "    test_size=5,\n",
    "    n_windows=None,\n",
    "    level=(50, 60),\n",
    "    refit=True,\n",
    ")\n",
    "test_fail(test_eq, args=(res_cv_wo_refit, res_cv_w_refit))\n",
    "test_eq(\n",
    "    res_cv_wo_refit.groupby('unique_id').head(1), \n",
    "    res_cv_w_refit.groupby('unique_id').head(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0668b-8218-4691-ad24-961cb8b2cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test series without ds as datetime\n",
    "series_cv_wo_dt = series_cv.copy()\n",
    "series_cv_wo_dt['ds'] = series_cv_wo_dt['ds'].astype(str) \n",
    "fcst = StatsForecast(\n",
    "    models=[SumAhead(), Naive()],\n",
    "    freq='D',\n",
    "    verbose=False\n",
    ")\n",
    "res_cv_wo_dt = fcst.cross_validation(\n",
    "    df=series_cv_wo_dt,\n",
    "    h=2,\n",
    "    test_size=5,\n",
    "    n_windows=None,\n",
    "    level=(50, 60),\n",
    ")\n",
    "test_eq(res_cv, res_cv_wo_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71f530-c7cf-4381-b831-9c85b5ab6111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#test for equal ends cross_validation\n",
    "series_cv = pd.DataFrame({\n",
    "    'unique_id': np.hstack([np.zeros(10), np.zeros(100) + 1, np.zeros(20) + 2]).astype('int64'),\n",
    "    'ds': np.hstack([\n",
    "        pd.date_range(end='2022-01-01', freq='D', periods=10),\n",
    "        pd.date_range(end='2022-01-01', freq='D', periods=100),\n",
    "        pd.date_range(end='2022-01-01', freq='D', periods=20)\n",
    "    ]),\n",
    "    'y': np.hstack([np.arange(10), np.arange(100, 200), np.arange(20, 40)]),\n",
    "})\n",
    "fcst = StatsForecast(\n",
    "    models=[SumAhead()],\n",
    "    freq='D',\n",
    ")\n",
    "res_cv = fcst.cross_validation(\n",
    "    df=series_cv,\n",
    "    h=2,\n",
    "    test_size=5,\n",
    "    n_windows=None,\n",
    "    level=(50,60),\n",
    "    fitted=True,\n",
    ")\n",
    "test_eq(0., np.mean(res_cv['y'] - res_cv['SumAhead']))\n",
    "\n",
    "n_windows = fcst.cross_validation(\n",
    "    df=series_cv,\n",
    "    h=2,\n",
    "    n_windows=2,\n",
    ").groupby('unique_id').size().unique()\n",
    "test_eq(n_windows, 2 * 2)\n",
    "test_eq(0., np.mean(res_cv['y'] - res_cv['SumAhead']))\n",
    "\n",
    "n_windows = fcst.cross_validation(\n",
    "    df=series_cv,\n",
    "    h=3,\n",
    "    n_windows=3, \n",
    "    step_size=3,\n",
    ").groupby('unique_id').size().unique()\n",
    "test_eq(n_windows, 3 * 3)\n",
    "test_eq(0., np.mean(res_cv['y'] - res_cv['SumAhead']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551bd4d-d71a-4455-9c48-3df44bbf4fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_StatsForecast.cross_validation_fitted_values, \n",
    "         title_level=2, \n",
    "         name='StatsForecast.cross_validation_fitted_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e837869-3229-438b-8507-31e5242f1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatsForecast.cross_validation_fitted_values method usage example\n",
    "\n",
    "#from statsforecast.core import StatsForecast\n",
    "from statsforecast.utils import AirPassengersDF as panel_df\n",
    "from statsforecast.models import Naive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de58b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StatsForecast class\n",
    "fcst = StatsForecast(models=[Naive()],\n",
    "                     freq='D', n_jobs=1)\n",
    "\n",
    "# Access insample predictions\n",
    "rolled_fcsts_df = fcst.cross_validation(df=panel_df, h=12, n_windows=2, fitted=True)\n",
    "insample_rolled_fcsts_df = fcst.cross_validation_fitted_values()\n",
    "insample_rolled_fcsts_df.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba27b00-70f0-4d89-a027-f1a8a5f550ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#tests for fitted values cross_validation\n",
    "def test_cv_fitted(series_cv, n_jobs=1, str_ds=False):\n",
    "    if str_ds:\n",
    "        series_cv = series_cv.copy()\n",
    "        series_cv['ds'] = series_cv['ds'].astype(str)\n",
    "    resids_fcst = StatsForecast(\n",
    "        models=[SumAhead(), Naive()],\n",
    "        freq='D',\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    resids_res_cv = resids_fcst.cross_validation(df=series_cv, h=2, n_windows=4, fitted=True)\n",
    "    resids_cv = resids_fcst.cross_validation_fitted_values()\n",
    "    np.testing.assert_array_equal(\n",
    "        resids_cv['cutoff'].unique(),\n",
    "        resids_res_cv['cutoff'].unique()\n",
    "    )\n",
    "    if str_ds:\n",
    "        series_cv['ds'] = pd.to_datetime(series_cv['ds'])\n",
    "    for uid in resids_cv['unique_id'].unique():\n",
    "        resids_uid = resids_cv[resids_cv['unique_id'].eq(uid)]\n",
    "        for cutoff in resids_uid['cutoff'].unique():\n",
    "            pd.testing.assert_frame_equal(\n",
    "                resids_uid.query('cutoff == @cutoff')[['unique_id', 'ds', 'y']].reset_index(drop=True),\n",
    "                series_cv.query('ds <= @cutoff & unique_id == @uid')[['unique_id', 'ds', 'y']].reset_index(drop=True),\n",
    "                check_dtype=False\n",
    "            )\n",
    "test_cv_fitted(series_cv)\n",
    "test_cv_fitted(series_cv, str_ds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2b54e-b4dd-4481-a98f-303683122f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#tests for fallback model\n",
    "def test_cv_fallback_model(n_jobs=1):\n",
    "    fitted_fcst = StatsForecast(\n",
    "        models=[NullModel()],\n",
    "        freq='D',\n",
    "        n_jobs=n_jobs,\n",
    "        fallback_model=Naive()\n",
    "    )\n",
    "    fitted_res = fitted_fcst.cross_validation(df=series, h=2, n_windows=4, fitted=True)\n",
    "    fitted = fitted_fcst.cross_validation_fitted_values()\n",
    "    # test NullModel actualy fails\n",
    "    fitted_fcst = StatsForecast(\n",
    "        models=[NullModel()],\n",
    "        freq='D',\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "    test_fail(lambda: fitted_fcst.cross_validation(df=series, h=12, n_windows=4), \n",
    "              contains='got an unexpected keyword argument')\n",
    "test_cv_fallback_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9fad4-3b5a-45f0-9179-a4c671f41a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_StatsForecast.plot, \n",
    "         title_level=2, \n",
    "         name='StatsForecast.plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(StatsForecast.save, title_level=2, name='StatsForecast.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4134ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(StatsForecast.load, title_level=2, name='StatsForecast.load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4773d32a-d231-4c8d-9525-28d95055ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fcst.fit(df=series)\n",
    "test_eq(\n",
    "    fcst.predict(h=12),\n",
    "    fcst.forecast(df=series, h=12)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fe4d4-3e6b-4472-b29a-14f24c35024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(\n",
    "    fcst.fit_predict(df=series, h=12),\n",
    "    fcst.forecast(df=series, h=12)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f1935-4305-4e0a-a0b9-0bb4afea900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test for conformal prediction\n",
    "uids = series.index.unique()[:10]\n",
    "series_subset = series.query('unique_id in @uids')[['unique_id', 'ds', 'y']]\n",
    "sf = StatsForecast(\n",
    "    models=[SeasonalNaive(season_length=7)],\n",
    "    freq='D', \n",
    "    n_jobs=1,\n",
    ")\n",
    "sf = sf.fit(df=series_subset, prediction_intervals=ConformalIntervals(h=12))\n",
    "test_eq(\n",
    "    sf.predict(h=12, level=[80, 90]),\n",
    "    sf.fit_predict(df=series_subset, h=12, level=[80, 90], prediction_intervals=ConformalIntervals(h=12)),\n",
    ")\n",
    "test_eq(\n",
    "    sf.predict(h=12, level=[80, 90]),\n",
    "    sf.forecast(df=series_subset, h=12, level=[80, 90], prediction_intervals=ConformalIntervals(h=12)),\n",
    ")\n",
    "\n",
    "# test errors/warnings are raised when level is not specified\n",
    "intervals = ConformalIntervals(h=12)\n",
    "sf2 = StatsForecast(\n",
    "    models=[ADIDA()],\n",
    "    freq='D', \n",
    "    n_jobs=1,\n",
    ")\n",
    "sf2.fit(df=series_subset, prediction_intervals=intervals)\n",
    "test_warns(lambda: sf2.predict(h=12))\n",
    "test_fail(lambda: sf2.forecast(df=series_subset, h=12, prediction_intervals=intervals))\n",
    "test_fail(lambda: sf2.fit_predict(df=series_subset, h=12, prediction_intervals=intervals))\n",
    "test_fail(lambda: sf2.cross_validation(df=series_subset, h=12, prediction_intervals=intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19fbc4-7e77-496c-b386-4f9bee8a6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test conformal cross validation\n",
    "cv_conformal = sf.cross_validation(\n",
    "    df=series_subset, \n",
    "    h=12, \n",
    "    n_windows=2,\n",
    "    level=[80, 90], \n",
    "    prediction_intervals=ConformalIntervals(h=12),\n",
    ")\n",
    "cv_no_conformal = sf.cross_validation(\n",
    "    df=series_subset, \n",
    "    h=12, \n",
    "    n_windows=2,\n",
    "    level=[80, 90],  \n",
    ")\n",
    "test_eq(\n",
    "    cv_conformal.columns,\n",
    "    cv_no_conformal.columns,    \n",
    ")\n",
    "test_eq(\n",
    "    cv_conformal.filter(regex='ds|cutoff|y|AutoARIMA$'),\n",
    "    cv_no_conformal.filter(regex='ds|cutoff|y|AutoARIMA$')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a039c-1d54-420c-819d-814c0d97c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = StatsForecast(\n",
    "    models=[ADIDA(), SimpleExponentialSmoothing(0.1), \n",
    "            HistoricAverage(), CrostonClassic()],\n",
    "    freq='D',\n",
    "    n_jobs=1\n",
    ")\n",
    "res = fcst.forecast(df=series, h=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73536e57-fa38-4b7c-967d-a5ddd54733d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "#tests for parallel processing\n",
    "fcst = StatsForecast(\n",
    "    models=[ADIDA(), SimpleExponentialSmoothing(0.1), \n",
    "            HistoricAverage(), CrostonClassic()],\n",
    "    freq='D',\n",
    "    n_jobs=-1\n",
    ")\n",
    "res = fcst.forecast(df=series, h=14)\n",
    "res_cv = fcst.cross_validation(df=series, h=3, test_size=10, n_windows=None)\n",
    "fcst = StatsForecast(\n",
    "    models=[SumAhead()],\n",
    "    freq='D',\n",
    ")\n",
    "res_cv = fcst.cross_validation(df=series_cv, h=2, test_size=5, n_windows=None)\n",
    "test_eq(0., np.mean(res_cv['y'] - res_cv['SumAhead']))\n",
    "\n",
    "test_fcst_fitted(series, n_jobs=-1)\n",
    "test_cv_fitted(series_cv, n_jobs=-1)\n",
    "test_fcst_fitted(series, n_jobs=-1, str_ds=True)\n",
    "test_cv_fitted(series_cv, n_jobs=-1, str_ds=True)\n",
    "# check n_windows argument\n",
    "n_windows = fcst.cross_validation(df=series_cv, h=2, n_windows=2).groupby('unique_id').size().unique()\n",
    "test_eq(n_windows, 2 * 2)\n",
    "test_eq(0., np.mean(res_cv['y'] - res_cv['SumAhead']))\n",
    "# check step_size argument\n",
    "n_windows = fcst.cross_validation(df=series_cv, h=3, n_windows=3, step_size=3).groupby('unique_id').size().unique()\n",
    "test_eq(n_windows, 3 * 3)\n",
    "test_eq(0., np.mean(res_cv['y'] - res_cv['SumAhead']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a8ac02e",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1ddc742-cae0-43f8-89eb-f5ae2effeb15",
   "metadata": {},
   "source": [
    "## Integer datestamp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1985e76-32d6-472a-b38e-fc35a933dac5",
   "metadata": {},
   "source": [
    "The `StatsForecast` class can also receive integers as datestamp, the following example shows how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc45251-a56b-4dad-9d84-9a843de0e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsforecast.core import StatsForecast\n",
    "from statsforecast.utils import AirPassengers as ap\n",
    "from statsforecast.models import HistoricAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e072d-5f62-4c1a-8b0c-1592a7f992b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_ds_df = pd.DataFrame({'ds': np.arange(1, len(ap) + 1), 'y': ap})\n",
    "int_ds_df.insert(0, 'unique_id', 'AirPassengers')\n",
    "int_ds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1575c5-ad2b-419d-aff1-f757ce39f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_ds_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e5517",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfbe2a7-3205-4976-9e78-3225b7808444",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst = StatsForecast(models=[HistoricAverage()], freq=1)\n",
    "horizon = 7\n",
    "forecast = fcst.forecast(df=int_ds_df, h=horizon)\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df545ed2-7b28-44a6-875d-58a3ea7a934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = int_ds_df['ds'].max()\n",
    "test_eq(forecast['ds'].values, np.arange(last_date + 1, last_date + 1 + horizon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589bdae-0303-4878-b387-07935afa855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_ds_cv = fcst.cross_validation(df=int_ds_df, h=7, test_size=8, n_windows=None)\n",
    "int_ds_cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23eb42c4-8275-430f-b074-d93f71c0ca22",
   "metadata": {},
   "source": [
    "## External regressors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3320eba6-603d-431e-9f5c-0ba2df7876d8",
   "metadata": {},
   "source": [
    "Every column after **y** is considered an external regressor and will be passed to the models that allow them. If you use them you must supply the future values to the `StatsForecast.forecast` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99fb36-c574-47d7-bff6-d95a1172c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(_TS):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, y, X):\n",
    "        self.coefs_, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, h, X):\n",
    "        mean = X @ coefs\n",
    "        return mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'LinearRegression()'\n",
    "    \n",
    "    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n",
    "        coefs, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "        return {'mean': X_future @ coefs}\n",
    "    \n",
    "    def new(self):\n",
    "        b = type(self).__new__(type(self))\n",
    "        b.__dict__.update(self.__dict__)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_xreg = series = generate_series(10_000, equal_ends=True)\n",
    "series_xreg['intercept'] = 1\n",
    "series_xreg['dayofweek'] = series_xreg['ds'].dt.dayofweek\n",
    "series_xreg = pd.get_dummies(series_xreg, columns=['dayofweek'], drop_first=True)\n",
    "series_xreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bc4c7-9f0f-4cdc-a639-965cc2ea0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(series_xreg['ds'].unique())\n",
    "valid_start = dates[-14]\n",
    "train_mask = series_xreg['ds'] < valid_start\n",
    "series_train = series_xreg[train_mask]\n",
    "series_valid = series_xreg[~train_mask]\n",
    "X_valid = series_valid.drop(columns=['y'])\n",
    "fcst = StatsForecast(\n",
    "    models=[LinearRegression()],\n",
    "    freq='D',\n",
    ")\n",
    "xreg_res = fcst.forecast(df=series_train, h=14, X_df=X_valid)\n",
    "xreg_res['y'] = series_valid['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b339f52-af88-4dc8-93af-cbbc751e231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xreg_res.drop(columns='unique_id').groupby('ds').mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af9dd2-eeb8-4ef6-9eef-13608ace264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xreg_res_cv = fcst.cross_validation(df=series_train, h=3, test_size=5, n_windows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d885f-9ef9-4237-aece-a87777c46caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# the following cells contain tests for external regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8c27e-1cb1-4e02-8243-6d3ccfc76860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "class ReturnX(_TS):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, y, X):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, h, X):\n",
    "        mean = X\n",
    "        return X\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ReturnX'\n",
    "    \n",
    "    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n",
    "        return {'mean': X_future.flatten()}\n",
    "    \n",
    "    def new(self):\n",
    "        b = type(self).__new__(type(self))\n",
    "        b.__dict__.update(self.__dict__)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da58f77-7194-4108-a32d-d3f01b39b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'unique_id': [0] * 10 + [1] * 10,\n",
    "        'ds': np.hstack([np.arange(10), np.arange(10)]),\n",
    "        'y': np.random.rand(20),\n",
    "        'x': np.arange(20, dtype=np.float64),\n",
    "    }\n",
    ")\n",
    "train_mask = df['ds'] < 6\n",
    "train_df = df[train_mask]\n",
    "test_df = df[~train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34f367-211b-40c2-a98a-8413abbe8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_x_vars(n_jobs=1):\n",
    "    fcst = StatsForecast(\n",
    "        models=[ReturnX()],\n",
    "        freq=1,\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "    xreg = test_df.drop(columns='y')\n",
    "    res = fcst.forecast(df=train_df, h=4, X_df=xreg)\n",
    "    expected_res = xreg.rename(columns={'x': 'ReturnX'})\n",
    "    pd.testing.assert_frame_equal(\n",
    "        res,\n",
    "        expected_res.reset_index(drop=True),\n",
    "        check_dtype=False,\n",
    "    )\n",
    "test_x_vars(n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6095cae-4788-49f5-9a15-9a61ea49c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "test_x_vars(n_jobs=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3df775a7-fa4a-42eb-b556-bea2c851f23f",
   "metadata": {},
   "source": [
    "## Prediction intervals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d34eca7-8f71-4668-8521-20e1637f6140",
   "metadata": {},
   "source": [
    "You can pass the argument `level` to the `StatsForecast.forecast` method to calculate prediction intervals. Not all models can calculate them at the moment, so we will only obtain the intervals of those models that have it implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f5a41-d19e-444e-b023-b7969ad773ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df = pd.DataFrame({'ds': np.arange(ap.size), 'y': ap})\n",
    "ap_df['unique_id'] = 0\n",
    "sf = StatsForecast(\n",
    "    models=[\n",
    "        SeasonalNaive(season_length=12), \n",
    "        AutoARIMA(season_length=12)\n",
    "    ],\n",
    "    freq=1,\n",
    "    n_jobs=1\n",
    ")\n",
    "ap_ci = sf.forecast(df=ap_df, h=12, level=(80, 95))\n",
    "fcst.plot(ap_df, ap_ci, level=[80], engine=\"matplotlib\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "034d106b-9ce9-4822-b119-ae5c3bd453c5",
   "metadata": {},
   "source": [
    "## Conformal Prediction intervals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faf7dbeb-8a32-43b5-a9e2-bfd3b52783a6",
   "metadata": {},
   "source": [
    "You can also add conformal intervals using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e76391-687e-44a3-8b20-349212e17180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast.utils import ConformalIntervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b89ac3-0fe7-4d5a-8384-3257f977faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = StatsForecast(\n",
    "    models=[\n",
    "        AutoARIMA(season_length=12),\n",
    "        AutoARIMA(\n",
    "            season_length=12, \n",
    "            prediction_intervals=ConformalIntervals(n_windows=2, h=12),\n",
    "            alias='ConformalAutoARIMA'\n",
    "        ),\n",
    "    ],\n",
    "    freq=1,\n",
    "    n_jobs=1\n",
    ")\n",
    "ap_ci = sf.forecast(df=ap_df, h=12, level=(80, 95))\n",
    "fcst.plot(ap_df, ap_ci, level=[80], engine=\"plotly\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19650d45-5079-44b8-81d6-bb1fadb471fa",
   "metadata": {},
   "source": [
    "You can also compute conformal intervals for all the models that support them, using the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88622d64-b394-47d5-b361-8596d81fd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = StatsForecast(\n",
    "    models=[\n",
    "        AutoARIMA(season_length=12),\n",
    "    ],\n",
    "    freq=1,\n",
    "    n_jobs=1\n",
    ")\n",
    "ap_ci = sf.forecast(\n",
    "    df=ap_df, \n",
    "    h=12, \n",
    "    level=(50, 80, 95), \n",
    "    prediction_intervals=ConformalIntervals(h=12),\n",
    ")\n",
    "fcst.plot(ap_df, ap_ci, level=[80], engine=\"matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9fd95-52c4-4c42-a350-8e667fba2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def test_conf_intervals(n_jobs=1):\n",
    "    ap_df = pd.DataFrame(\n",
    "        {\n",
    "            'unique_id': [0] * ap.size,\n",
    "            'ds': np.arange(ap.size),\n",
    "             'y': ap\n",
    "        }\n",
    "    )\n",
    "    fcst = StatsForecast(\n",
    "        models=[\n",
    "            SeasonalNaive(season_length=12), \n",
    "            AutoARIMA(season_length=12)\n",
    "        ],\n",
    "        freq=1,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    ap_ci = fcst.forecast(df=ap_df, h=12, level=(80, 95))\n",
    "    ap_ci.drop(columns='unique_id').set_index('ds').plot(marker='.', figsize=(10, 6))\n",
    "test_conf_intervals(n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e2b7a-f3b2-4200-b902-b4c5d369e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "#test number of jobs greater than the available cores\n",
    "test_conf_intervals(n_jobs=101)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
