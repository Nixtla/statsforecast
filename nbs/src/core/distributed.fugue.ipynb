{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "524620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp distributed.fugue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15392f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa25a4",
   "metadata": {},
   "source": [
    "# FugueBackend\n",
    "\n",
    "> The computational efficiency of `StatsForecast` can be tracked to its two core components:<br>1. Its `models` written in NumBa that optimizes Python code to reach C speeds.<br>2. Its `core.StatsForecast` class that enables distributed computing.<br><br>Here we use [Fugue](https://github.com/fugue-project/fugue) which is a unified interface for `Dask` and `Spark`.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06732b96-bd80-4a4d-b9a2-4f95c7a82331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from nbdev.showdoc import add_docs, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc16100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinkho/Work/statsforecast/statsforecast/core.py:24: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import inspect\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fugue import transform, DataFrame, FugueWorkflow, ExecutionEngine\n",
    "from fugue.collections.yielded import Yielded\n",
    "from fugue.constants import FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT\n",
    "from statsforecast.core import _StatsForecast, ParallelBackend, make_backend\n",
    "from triad import Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711a14ea-b3e7-466c-bd38-ab94ebbd279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _cotransform(\n",
    "    df1: Any,\n",
    "    df2: Any,\n",
    "    using: Any,\n",
    "    schema: Any = None,\n",
    "    params: Any = None,\n",
    "    partition: Any = None,\n",
    "    engine: Any = None,\n",
    "    engine_conf: Any = None,\n",
    "    force_output_fugue_dataframe: bool = False,\n",
    "    as_local: bool = False,\n",
    ") -> Any:\n",
    "    dag = FugueWorkflow(compile_conf={FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT: 0})\n",
    "    \n",
    "    src = dag.create_data(df1).zip(dag.create_data(df2), partition=partition)\n",
    "    tdf = src.transform(\n",
    "        using=using,\n",
    "        schema=schema,\n",
    "        params=params,\n",
    "        pre_partition=partition,\n",
    "    )\n",
    "    tdf.yield_dataframe_as(\"result\", as_local=as_local)\n",
    "    dag.run(engine, conf=engine_conf)\n",
    "    result = dag.yields[\"result\"].result  # type:ignore\n",
    "    if force_output_fugue_dataframe or isinstance(df1, (DataFrame, Yielded)):\n",
    "        return result\n",
    "    return result.as_pandas() if result.is_local else result.native  # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d8d5b82-2be9-41f5-8cd0-3903d0761e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FugueBackend(ParallelBackend):\n",
    "    \"\"\"FugueBackend for Distributed Computation.\n",
    "    [Source code](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py).\n",
    "\n",
    "    This class uses [Fugue](https://github.com/fugue-project/fugue) backend capable of distributing \n",
    "    computation on Spark, Dask and Ray without any rewrites.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `engine`: fugue.ExecutionEngine, a selection between Spark, Dask, and Ray.<br>\n",
    "    `conf`: fugue.Config, engine configuration.<br>\n",
    "    `**transform_kwargs`: additional kwargs for Fugue's transform method.<br>\n",
    "\n",
    "    **Notes:**<br>\n",
    "    A short introduction to Fugue, with examples on how to scale pandas code to Spark, Dask or Ray\n",
    "     is available [here](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            engine: Any = None,\n",
    "            conf: Any = None,\n",
    "            **transform_kwargs: Any\n",
    "        ):        \n",
    "        self._engine = engine\n",
    "        self._conf = conf\n",
    "        self._transform_kwargs = dict(transform_kwargs)\n",
    "\n",
    "    def __getstate__(self) -> Dict[str, Any]:\n",
    "        return {}\n",
    "\n",
    "    def forecast(\n",
    "            self, \n",
    "            df,\n",
    "            models,\n",
    "            freq,\n",
    "            fallback_model = None,\n",
    "            X_df = None,\n",
    "            **kwargs: Any,\n",
    "        ) -> Any:\n",
    "        \"\"\"Memory Efficient core.StatsForecast predictions with FugueBackend.\n",
    "\n",
    "        This method uses Fugue's transform function, in combination with \n",
    "        `core.StatsForecast`'s forecast to efficiently fit a list of StatsForecast models.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `freq`: str, frequency of the data, [pandas available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "        `models`: List[typing.Any], list of instantiated objects `StatsForecast.models`.<br>\n",
    "        `fallback_model`: Any, Model to be used if a model fails.<br>\n",
    "        `X_df`: pandas.DataFrame, with [unique_id, ds] columns and df’s future exogenous.\n",
    "        `**kwargs`: Additional `core.StatsForecast` parameters. Example forecast horizon `h`.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with `models` columns for point predictions and probabilistic\n",
    "        predictions for all fitted `models`.<br>\n",
    "        \n",
    "        **References:**<br>\n",
    "        For more information check the \n",
    "        [Fugue's transform](https://fugue-tutorials.readthedocs.io/tutorials/beginner/transform.html)\n",
    "        tutorial.<br>\n",
    "        The [core.StatsForecast's forecast](https://nixtla.github.io/statsforecast/core.html#statsforecast.forecast)\n",
    "        method documentation.<br>\n",
    "        Or the list of available [StatsForecast's models](https://nixtla.github.io/statsforecast/src/core/models.html).\n",
    "        \"\"\"\n",
    "        level = kwargs.get(\"level\", [])\n",
    "        schema = \"*-y+\" + str(self._get_output_schema(models, level))\n",
    "        if X_df is None:\n",
    "            return transform(\n",
    "                df,\n",
    "                self._forecast_series,\n",
    "                params=dict(models=models, freq=freq, \n",
    "                            kwargs=kwargs, fallback_model=fallback_model),\n",
    "                schema=schema,\n",
    "                partition={\"by\": \"unique_id\"},\n",
    "                engine=self._engine,\n",
    "                engine_conf=self._conf,\n",
    "                **self._transform_kwargs,\n",
    "            )\n",
    "        else:\n",
    "            schema = \"unique_id:str,ds:str,\" + str(self._get_output_schema(models, level))\n",
    "            return _cotransform(\n",
    "                df,\n",
    "                X_df,\n",
    "                self._forecast_series_X,\n",
    "                params=dict(models=models, freq=freq, \n",
    "                            kwargs=kwargs, fallback_model=fallback_model),\n",
    "                schema=schema,\n",
    "                partition={\"by\": \"unique_id\"},\n",
    "                engine=self._engine,\n",
    "                engine_conf=self._conf,\n",
    "                **self._transform_kwargs,\n",
    "            )\n",
    "            \n",
    "\n",
    "    def cross_validation(\n",
    "            self, \n",
    "            df,\n",
    "            models,\n",
    "            freq,\n",
    "            fallback_model=None,\n",
    "            **kwargs: Any, \n",
    "        ) -> Any:\n",
    "        \"\"\"Temporal Cross-Validation with core.StatsForecast and FugueBackend.\n",
    "\n",
    "        This method uses Fugue's transform function, in combination with \n",
    "        `core.StatsForecast`'s cross-validation to efficiently fit a list of StatsForecast \n",
    "        models through multiple training windows, in either chained or rolled manner.\n",
    "\n",
    "        `StatsForecast.models`' speed along with Fugue's distributed computation allow to \n",
    "        overcome this evaluation technique high computational costs. Temporal cross-validation \n",
    "        provides better model's generalization measurements by increasing the test's length \n",
    "        and diversity.\n",
    "\n",
    "        **Parameters:**<br>\n",
    "        `df`: pandas.DataFrame, with columns [`unique_id`, `ds`, `y`] and exogenous.<br>\n",
    "        `freq`: str, frequency of the data, [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n",
    "        `models`: List[typing.Any], list of instantiated objects `StatsForecast.models`.<br>\n",
    "        `fallback_model`: Any, Model to be used if a model fails.<br>\n",
    "\n",
    "        **Returns:**<br>\n",
    "        `fcsts_df`: pandas.DataFrame, with `models` columns for point predictions and probabilistic\n",
    "        predictions for all fitted `models`.<br>\n",
    "        \n",
    "        **References:**<br>\n",
    "        The [core.StatsForecast's cross validation](https://nixtla.github.io/statsforecast/core.html#statsforecast.cross_validation)\n",
    "        method documentation.<br>\n",
    "        [Rob J. Hyndman and George Athanasopoulos (2018). \"Forecasting principles and practice, Temporal Cross-Validation\"](https://otexts.com/fpp3/tscv.html).\n",
    "        \"\"\"\n",
    "        level = kwargs.get(\"level\", [])\n",
    "        schema = \"*-y+\" + str(self._get_output_schema(models, level, mode=\"cv\"))\n",
    "        return transform(\n",
    "            df,\n",
    "            self._cv,\n",
    "            params=dict(models=models, freq=freq, \n",
    "                        kwargs=kwargs, \n",
    "                        fallback_model=fallback_model),\n",
    "            schema=schema,\n",
    "            partition={\"by\": \"unique_id\"},\n",
    "            engine=self._engine,\n",
    "            engine_conf=self._conf,\n",
    "            **self._transform_kwargs,\n",
    "        )\n",
    "\n",
    "    def _forecast_series(self, df: pd.DataFrame, models, freq, fallback_model, kwargs) -> pd.DataFrame:\n",
    "        model = _StatsForecast(df=df, models=models, freq=freq, \n",
    "                               fallback_model=fallback_model, n_jobs=1)\n",
    "        return model.forecast(**kwargs).reset_index()\n",
    "    \n",
    "    # schema: unique_id:str, ds:str, *\n",
    "    def _forecast_series_X(self, df: pd.DataFrame, X_df: pd.DataFrame, models, freq, fallback_model, kwargs) -> pd.DataFrame:\n",
    "        model = _StatsForecast(df=df, models=models, freq=freq, \n",
    "                               fallback_model=fallback_model, n_jobs=1)\n",
    "        if len(X_df) != kwargs['h']:\n",
    "            raise Exception(\n",
    "                'Please be sure that your exogenous variables `X_df` '\n",
    "                'have the same length than your forecast horizon `h`'\n",
    "            )\n",
    "        return model.forecast(X_df=X_df, **kwargs).reset_index()\n",
    "\n",
    "    def _cv(self, df: pd.DataFrame, models, freq, fallback_model, kwargs) -> pd.DataFrame:\n",
    "        model = _StatsForecast(df=df, models=models, freq=freq, \n",
    "                               fallback_model=fallback_model, n_jobs=1)\n",
    "        return model.cross_validation(**kwargs).reset_index()\n",
    "\n",
    "    def _get_output_schema(self, models, level=None, mode=\"forecast\") -> Schema:\n",
    "        cols: List[Any] = []\n",
    "        if level is None:\n",
    "            level = []\n",
    "        for model in models:\n",
    "            has_levels = (\n",
    "                \"level\" in inspect.signature(getattr(model, \"forecast\")).parameters\n",
    "                and len(level) > 0\n",
    "            )\n",
    "            cols.append((repr(model), np.float32))\n",
    "            if has_levels:\n",
    "                cols.extend([(f\"{repr(model)}-lo-{l}\", np.float32) for l in reversed(level)])\n",
    "                cols.extend([(f\"{repr(model)}-hi-{l}\", np.float32) for l in level])\n",
    "        if mode == \"cv\":\n",
    "            cols = [(\"cutoff\", \"datetime\"), (\"y\", np.float32)] + cols\n",
    "        return Schema(cols)\n",
    "    \n",
    "\n",
    "@make_backend.candidate(lambda obj, *args, **kwargs: isinstance(obj, ExecutionEngine))\n",
    "def _make_fugue_backend(obj:ExecutionEngine, *args, **kwargs) -> ParallelBackend:\n",
    "    return FugueBackend(obj, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5369129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>cutoff</th>\n",
       "      <th>y</th>\n",
       "      <th>AutoETS</th>\n",
       "      <th>AutoETS-lo-90</th>\n",
       "      <th>AutoETS-hi-90</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-10</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>2.472186</td>\n",
       "      <td>2.264802</td>\n",
       "      <td>2.029021</td>\n",
       "      <td>2.500583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-11</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>3.369775</td>\n",
       "      <td>3.207784</td>\n",
       "      <td>2.972003</td>\n",
       "      <td>3.443565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-12</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>4.245229</td>\n",
       "      <td>4.248131</td>\n",
       "      <td>4.012350</td>\n",
       "      <td>4.483912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-13</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>5.113708</td>\n",
       "      <td>5.267366</td>\n",
       "      <td>5.031586</td>\n",
       "      <td>5.503148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-07-14</td>\n",
       "      <td>2000-07-09</td>\n",
       "      <td>6.127178</td>\n",
       "      <td>6.203136</td>\n",
       "      <td>5.967356</td>\n",
       "      <td>6.438918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ds     cutoff         y   AutoETS  AutoETS-lo-90  \\\n",
       "unique_id                                                            \n",
       "0         2000-07-10 2000-07-09  2.472186  2.264802       2.029021   \n",
       "0         2000-07-11 2000-07-09  3.369775  3.207784       2.972003   \n",
       "0         2000-07-12 2000-07-09  4.245229  4.248131       4.012350   \n",
       "0         2000-07-13 2000-07-09  5.113708  5.267366       5.031586   \n",
       "0         2000-07-14 2000-07-09  6.127178  6.203136       5.967356   \n",
       "\n",
       "           AutoETS-hi-90  \n",
       "unique_id                 \n",
       "0               2.500583  \n",
       "0               3.443565  \n",
       "0               4.483912  \n",
       "0               5.503148  \n",
       "0               6.438918  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import ( \n",
    "    AutoARIMA,\n",
    "    AutoETS,\n",
    ")\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "n_series = 4\n",
    "horizon = 7\n",
    "\n",
    "series = generate_series(n_series)\n",
    "\n",
    "sf = StatsForecast(\n",
    "    models=[AutoETS(season_length=7)],\n",
    "    freq='D',\n",
    ")\n",
    "\n",
    "sf.cross_validation(df=series, h=horizon, step_size = 24,\n",
    "    n_windows = 2, level=[90]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d84def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/08 17:13:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+-----------+----------+-------------+-------------+\n",
      "|unique_id|                 ds|             cutoff|          y|   AutoETS|AutoETS-lo-90|AutoETS-hi-90|\n",
      "+---------+-------------------+-------------------+-----------+----------+-------------+-------------+\n",
      "|        1|2000-03-07 00:00:00|2000-03-06 00:00:00|  1.1842923| 1.1227854|   0.88283557|    1.3627354|\n",
      "|        1|2000-03-08 00:00:00|2000-03-06 00:00:00|  2.0684502| 2.3335178|     2.093568|    2.5734677|\n",
      "|        1|2000-03-09 00:00:00|2000-03-06 00:00:00|   3.411059|  3.249278|    3.0093281|    3.4892278|\n",
      "|        1|2000-03-10 00:00:00|2000-03-06 00:00:00|   4.094924| 4.3513813|    4.1114316|    4.5913315|\n",
      "|        1|2000-03-11 00:00:00|2000-03-06 00:00:00|  5.2556596| 5.2070827|     4.967133|     5.447033|\n",
      "|        1|2000-03-12 00:00:00|2000-03-06 00:00:00|  6.1121583|   6.28834|      6.04839|      6.52829|\n",
      "|        1|2000-03-13 00:00:00|2000-03-06 00:00:00| 0.04892224|0.25212684|  0.012176938|   0.49207672|\n",
      "|        1|2000-03-31 00:00:00|2000-03-30 00:00:00|   4.336024| 4.3107433|    4.0474195|    4.5740666|\n",
      "|        1|2000-04-01 00:00:00|2000-03-30 00:00:00|  5.1226835| 5.2358456|    4.9725223|     5.499169|\n",
      "|        1|2000-04-02 00:00:00|2000-03-30 00:00:00|    6.21027| 6.2425833|      5.97926|    6.5059066|\n",
      "|        1|2000-04-03 00:00:00|2000-03-30 00:00:00|  0.2786844|0.22976251|  -0.03356086|    0.4930859|\n",
      "|        1|2000-04-04 00:00:00|2000-03-30 00:00:00|  1.4302756| 1.1543093|    0.8909859|    1.4176327|\n",
      "|        1|2000-04-05 00:00:00|2000-03-30 00:00:00|   2.363522| 2.2840557|    2.0207324|     2.547379|\n",
      "|        1|2000-04-06 00:00:00|2000-03-30 00:00:00|   3.135164| 3.2521222|    2.9887989|    3.5154455|\n",
      "|        3|2000-07-30 00:00:00|2000-07-29 00:00:00|  4.4213886| 4.1870832|     3.950173|     4.423993|\n",
      "|        3|2000-07-31 00:00:00|2000-07-29 00:00:00|   5.186608| 5.2659216|    5.0290112|    5.5028315|\n",
      "|        3|2000-08-01 00:00:00|2000-07-29 00:00:00|   6.111432|  6.255861|    6.0189505|    6.4927707|\n",
      "|        3|2000-08-02 00:00:00|2000-07-29 00:00:00|0.040266003| 0.2792256|  0.042315517|    0.5161357|\n",
      "|        3|2000-08-03 00:00:00|2000-07-29 00:00:00|  1.0426555| 1.2719641|     1.035054|    1.5088742|\n",
      "|        3|2000-08-04 00:00:00|2000-07-29 00:00:00|  2.1106982|  2.245433|     2.008523|    2.4823432|\n",
      "+---------+-------------------+-------------------+-----------+----------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Make unique_id a column\n",
    "series = series.reset_index()\n",
    "series['unique_id'] = series['unique_id'].astype(str)\n",
    "\n",
    "# Convert to Spark\n",
    "sdf = spark.createDataFrame(series)\n",
    "\n",
    "# Returns a Spark DataFrame\n",
    "sf.cross_validation(df=sdf, h=horizon, step_size = 24,\n",
    "    n_windows = 2, level=[90]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53913de1-81b9-401c-93a2-83e42047e471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py#L48){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FugueBackend\n",
       "\n",
       ">      FugueBackend (engine:Any=None, conf:Any=None, **transform_kwargs:Any)\n",
       "\n",
       "FugueBackend for Distributed Computation.\n",
       "[Source code](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py).\n",
       "\n",
       "This class uses [Fugue](https://github.com/fugue-project/fugue) backend capable of distributing \n",
       "computation on Spark, Dask and Ray without any rewrites.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`engine`: fugue.ExecutionEngine, a selection between Spark, Dask, and Ray.<br>\n",
       "`conf`: fugue.Config, engine configuration.<br>\n",
       "`**transform_kwargs`: additional kwargs for Fugue's transform method.<br>\n",
       "\n",
       "**Notes:**<br>\n",
       "A short introduction to Fugue, with examples on how to scale pandas code to Spark, Dask or Ray\n",
       " is available [here](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html)."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py#L48){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FugueBackend\n",
       "\n",
       ">      FugueBackend (engine:Any=None, conf:Any=None, **transform_kwargs:Any)\n",
       "\n",
       "FugueBackend for Distributed Computation.\n",
       "[Source code](https://github.com/Nixtla/statsforecast/blob/main/statsforecast/distributed/fugue.py).\n",
       "\n",
       "This class uses [Fugue](https://github.com/fugue-project/fugue) backend capable of distributing \n",
       "computation on Spark, Dask and Ray without any rewrites.\n",
       "\n",
       "**Parameters:**<br>\n",
       "`engine`: fugue.ExecutionEngine, a selection between Spark, Dask, and Ray.<br>\n",
       "`conf`: fugue.Config, engine configuration.<br>\n",
       "`**transform_kwargs`: additional kwargs for Fugue's transform method.<br>\n",
       "\n",
       "**Notes:**<br>\n",
       "A short introduction to Fugue, with examples on how to scale pandas code to Spark, Dask or Ray\n",
       " is available [here](https://fugue-tutorials.readthedocs.io/tutorials/quick_look/ten_minutes.html)."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FugueBackend, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037f72d-4ace-44e8-b4d5-b8399d5e294d",
   "metadata": {},
   "source": [
    "## Dask Distributed Predictions\n",
    "\n",
    "Here we provide an example for the distribution of the `StatsForecast` predictions using `Fugue` to execute the code in a Dask cluster.\n",
    "\n",
    "To do it we instantiate the `FugueBackend` class with a `DaskExecutionEngine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2df29ce-c1ac-44d9-829e-47096adf2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from fugue_dask import DaskExecutionEngine\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "# Generate Synthetic Panel Data\n",
    "df = generate_series(10).reset_index()\n",
    "df['unique_id'] = df['unique_id'].astype(str)\n",
    "df = dd.from_pandas(df, npartitions=10)\n",
    "\n",
    "# Instantiate FugueBackend with DaskExecutionEngine\n",
    "dask_client = Client()\n",
    "engine = DaskExecutionEngine(dask_client=dask_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832937cc-2b7b-4ddc-84d0-e68a650bdc12",
   "metadata": {},
   "source": [
    "We have simply create the class to the usual `StatsForecast` instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a08c1f2-3b0a-460f-a1a6-1d25d982104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "sf = StatsForecast(models=[Naive()], freq='D')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a44ec-787f-4ef7-8129-71409d2dd32a",
   "metadata": {},
   "source": [
    "### Distributed Forecast\n",
    "\n",
    "For extremely fast distributed predictions we use FugueBackend as backend that operates like the original [StatsForecast.forecast](https://nixtla.github.io/statsforecast/core.html#statsforecast.forecast) method.\n",
    "\n",
    "It receives as input a pandas.DataFrame with columns [`unique_id`,`ds`,`y`] and exogenous, where the `ds` (datestamp) column should be of a format expected by Pandas. The `y` column must be numeric, and represents the measurement we wish to forecast. And the `unique_id` uniquely identifies the series in the panel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf2454a-7683-40d1-8828-877dba0345fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>Naive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-08-30</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-08-31</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-09-01</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-09-02</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2000-09-03</td>\n",
       "      <td>6.182456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-07</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-08</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-09</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-10</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>2000-03-11</td>\n",
       "      <td>0.162962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id         ds     Naive\n",
       "0          3 2000-08-30  6.182456\n",
       "1          3 2000-08-31  6.182456\n",
       "2          3 2000-09-01  6.182456\n",
       "3          3 2000-09-02  6.182456\n",
       "4          3 2000-09-03  6.182456\n",
       "..       ...        ...       ...\n",
       "7          8 2000-03-07  0.162962\n",
       "8          8 2000-03-08  0.162962\n",
       "9          8 2000-03-09  0.162962\n",
       "10         8 2000-03-10  0.162962\n",
       "11         8 2000-03-11  0.162962\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Distributed predictions with FugueBackend.\n",
    "sf.forecast(df=df, h=12).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a8e7b42-8129-472a-99fd-0725ae4cb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# fallback model\n",
    "class FailNaive:\n",
    "    def forecast(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return 'Naive'\n",
    "sf = StatsForecast(models=[Naive()], freq='D', fallback_model=Naive())\n",
    "dask_fcst = sf.forecast(df=df, h=12).compute()\n",
    "fcst_stats = sf.forecast(df=df.compute(), h=12)\n",
    "test_eq(dask_fcst.sort_values(by=['unique_id', 'ds']).reset_index(drop=True).astype({\"unique_id\": str}), \n",
    "        fcst_stats.reset_index().astype({\"unique_id\": str}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "964caa9d-d580-44a7-9115-2522e9b64ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Distributed exogenous regressors\n",
    "class ReturnX:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, y, X):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, h, X):\n",
    "        mean = X\n",
    "        return X\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ReturnX'\n",
    "    \n",
    "    def forecast(self, y, h, X=None, X_future=None, fitted=False):\n",
    "        return {'mean': X_future.flatten()}\n",
    "    \n",
    "    def new(self):\n",
    "        b = type(self).__new__(type(self))\n",
    "        b.__dict__.update(self.__dict__)\n",
    "        return b\n",
    "    \n",
    "df_w_ex = pd.DataFrame(\n",
    "    {\n",
    "        'ds': np.hstack([np.arange(10), np.arange(10)]),\n",
    "        'y': np.random.rand(20),\n",
    "        'x': np.arange(20, dtype=np.float32),\n",
    "    },\n",
    "    index=pd.Index([0] * 10 + [1] * 10, name='unique_id'),\n",
    ").reset_index()\n",
    "train_mask = df_w_ex['ds'] < 6\n",
    "train_df = dd.from_pandas(df_w_ex[train_mask], npartitions=10)\n",
    "test_df = df_w_ex[~train_mask]\n",
    "xreg = dd.from_pandas(test_df.drop(columns='y').reset_index(drop=True), npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67e92bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.757988</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.579568</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.306531</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.125165</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.492395</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id  ds         y    x\n",
       "0          0   0  0.757988  0.0\n",
       "1          0   1  0.579568  1.0\n",
       "2          0   2  0.306531  2.0\n",
       "3          0   3  0.125165  3.0\n",
       "4          0   4  0.492395  4.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w_ex = pd.DataFrame(\n",
    "    {\n",
    "        'ds': np.hstack([np.arange(10), np.arange(10)]),\n",
    "        'y': np.random.rand(20),\n",
    "        'x': np.arange(20, dtype=np.float32),\n",
    "    },\n",
    "    index=pd.Index([0] * 10 + [1] * 10, name='unique_id'),\n",
    ").reset_index()\n",
    "train_mask = df_w_ex['ds'] < 6\n",
    "train_df = df_w_ex[train_mask]\n",
    "test_df = df_w_ex[~train_mask]\n",
    "xreg = test_df.drop(columns='y').reset_index(drop=True)\n",
    "df_w_ex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45fe4905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id  ds     x\n",
       "0          0   6   6.0\n",
       "1          0   7   7.0\n",
       "2          0   8   8.0\n",
       "3          0   9   9.0\n",
       "4          1   6  16.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xreg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d84a7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.757988    0.        ]\n",
      " [ 0.57956802  1.        ]\n",
      " [ 0.30653073  2.        ]\n",
      " [ 0.12516507  3.        ]\n",
      " [ 0.49239535  4.        ]\n",
      " [ 0.26161233  5.        ]\n",
      " [ 0.11359065 10.        ]\n",
      " [ 0.48704471 11.        ]\n",
      " [ 0.85532895 12.        ]\n",
      " [ 0.08479259 13.        ]\n",
      " [ 0.70416622 14.        ]\n",
      " [ 0.31714871 15.        ]]\n",
      "   unique_id  ds     x\n",
      "0          0   6   6.0\n",
      "1          0   7   7.0\n",
      "2          0   8   8.0\n",
      "3          0   9   9.0\n",
      "4          1   6  16.0\n",
      "5          1   7  17.0\n",
      "6          1   8  18.0\n",
      "7          1   9  19.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected X to have shape (8, 2), but got (8, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fcst_x \u001b[38;5;241m=\u001b[39m StatsForecast(models\u001b[38;5;241m=\u001b[39m[AutoARIMA()], freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfcst_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_w_ex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mX_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Work/statsforecast/statsforecast/core.py:1831\u001b[0m, in \u001b[0;36mStatsForecast.forecast\u001b[0;34m(self, h, df, X_df, level, fitted, sort_df, prediction_intervals)\u001b[0m\n\u001b[1;32m   1820\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforecast\u001b[39m(\n\u001b[1;32m   1821\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1822\u001b[0m     h: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1828\u001b[0m     prediction_intervals: Optional[ConformalIntervals] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1829\u001b[0m ):\n\u001b[1;32m   1830\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_native(df\u001b[39m=\u001b[39mdf):\n\u001b[0;32m-> 1831\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforecast(\n\u001b[1;32m   1832\u001b[0m             h\u001b[39m=\u001b[39;49mh,\n\u001b[1;32m   1833\u001b[0m             df\u001b[39m=\u001b[39;49mdf,\n\u001b[1;32m   1834\u001b[0m             X_df\u001b[39m=\u001b[39;49mX_df,\n\u001b[1;32m   1835\u001b[0m             level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   1836\u001b[0m             fitted\u001b[39m=\u001b[39;49mfitted,\n\u001b[1;32m   1837\u001b[0m             sort_df\u001b[39m=\u001b[39;49msort_df,\n\u001b[1;32m   1838\u001b[0m             prediction_intervals\u001b[39m=\u001b[39;49mprediction_intervals,\n\u001b[1;32m   1839\u001b[0m         )\n\u001b[1;32m   1840\u001b[0m     \u001b[39massert\u001b[39;00m df \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1841\u001b[0m     engine \u001b[39m=\u001b[39m make_execution_engine(infer_by\u001b[39m=\u001b[39m[df])\n",
      "File \u001b[0;32m~/Work/statsforecast/statsforecast/core.py:1053\u001b[0m, in \u001b[0;36m_StatsForecast.forecast\u001b[0;34m(self, h, df, X_df, level, fitted, sort_df, prediction_intervals)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_prediction_intervals(prediction_intervals\u001b[39m=\u001b[39mprediction_intervals)\n\u001b[1;32m   1052\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_fit(df, sort_df)\n\u001b[0;32m-> 1053\u001b[0m X, level \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_X_level(h\u001b[39m=\u001b[39;49mh, X\u001b[39m=\u001b[39;49mX_df, level\u001b[39m=\u001b[39;49mlevel)\n\u001b[1;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1055\u001b[0m     res_fcsts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mga\u001b[39m.\u001b[39mforecast(\n\u001b[1;32m   1056\u001b[0m         models\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels,\n\u001b[1;32m   1057\u001b[0m         h\u001b[39m=\u001b[39mh,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1062\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m   1063\u001b[0m     )\n",
      "File \u001b[0;32m~/Work/statsforecast/statsforecast/core.py:911\u001b[0m, in \u001b[0;36m_StatsForecast._parse_X_level\u001b[0;34m(self, h, X, level)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[39mprint\u001b[39m(X)\n\u001b[1;32m    910\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m expected_shape:\n\u001b[0;32m--> 911\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    912\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected X to have shape \u001b[39m\u001b[39m{\u001b[39;00mexpected_shape\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     X \u001b[39m=\u001b[39m DataFrameProcessing(\n\u001b[1;32m    915\u001b[0m         X, sort_dataframe\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_df, validate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    916\u001b[0m     )\u001b[39m.\u001b[39mgrouped_array()\n\u001b[1;32m    917\u001b[0m \u001b[39mif\u001b[39;00m level \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected X to have shape (8, 2), but got (8, 3)"
     ]
    }
   ],
   "source": [
    "fcst_x = StatsForecast(models=[AutoARIMA()], freq='D')\n",
    "res = fcst_x.forecast(df=df_w_ex[train_mask], \n",
    "                      X_df=xreg, \n",
    "                      h=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "936e390c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected X to have shape (8, 2), but got (8, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# xreg = test_df.drop(columns='y').reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m fcst_x \u001b[38;5;241m=\u001b[39m StatsForecast(models\u001b[38;5;241m=\u001b[39m[AutoARIMA()], freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfcst_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_w_ex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mX_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m res\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Work/statsforecast/statsforecast/core.py:1830\u001b[0m, in \u001b[0;36mStatsForecast.forecast\u001b[0;34m(self, h, df, X_df, level, fitted, sort_df, prediction_intervals)\u001b[0m\n\u001b[1;32m   1819\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforecast\u001b[39m(\n\u001b[1;32m   1820\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1821\u001b[0m     h: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1827\u001b[0m     prediction_intervals: Optional[ConformalIntervals] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1828\u001b[0m ):\n\u001b[1;32m   1829\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_native(df\u001b[39m=\u001b[39mdf):\n\u001b[0;32m-> 1830\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforecast(\n\u001b[1;32m   1831\u001b[0m             h\u001b[39m=\u001b[39;49mh,\n\u001b[1;32m   1832\u001b[0m             df\u001b[39m=\u001b[39;49mdf,\n\u001b[1;32m   1833\u001b[0m             X_df\u001b[39m=\u001b[39;49mX_df,\n\u001b[1;32m   1834\u001b[0m             level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   1835\u001b[0m             fitted\u001b[39m=\u001b[39;49mfitted,\n\u001b[1;32m   1836\u001b[0m             sort_df\u001b[39m=\u001b[39;49msort_df,\n\u001b[1;32m   1837\u001b[0m             prediction_intervals\u001b[39m=\u001b[39;49mprediction_intervals,\n\u001b[1;32m   1838\u001b[0m         )\n\u001b[1;32m   1839\u001b[0m     \u001b[39massert\u001b[39;00m df \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1840\u001b[0m     engine \u001b[39m=\u001b[39m make_execution_engine(infer_by\u001b[39m=\u001b[39m[df])\n",
      "File \u001b[0;32m~/Work/statsforecast/statsforecast/core.py:1052\u001b[0m, in \u001b[0;36m_StatsForecast.forecast\u001b[0;34m(self, h, df, X_df, level, fitted, sort_df, prediction_intervals)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_prediction_intervals(prediction_intervals\u001b[39m=\u001b[39mprediction_intervals)\n\u001b[1;32m   1051\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_fit(df, sort_df)\n\u001b[0;32m-> 1052\u001b[0m X, level \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_X_level(h\u001b[39m=\u001b[39;49mh, X\u001b[39m=\u001b[39;49mX_df, level\u001b[39m=\u001b[39;49mlevel)\n\u001b[1;32m   1053\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1054\u001b[0m     res_fcsts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mga\u001b[39m.\u001b[39mforecast(\n\u001b[1;32m   1055\u001b[0m         models\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels,\n\u001b[1;32m   1056\u001b[0m         h\u001b[39m=\u001b[39mh,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m   1062\u001b[0m     )\n",
      "File \u001b[0;32m~/Work/statsforecast/statsforecast/core.py:910\u001b[0m, in \u001b[0;36m_StatsForecast._parse_X_level\u001b[0;34m(self, h, X, level)\u001b[0m\n\u001b[1;32m    907\u001b[0m     expected_shape \u001b[39m=\u001b[39m (expected_shape_rows, expected_shape_cols)\n\u001b[1;32m    909\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m expected_shape:\n\u001b[0;32m--> 910\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected X to have shape \u001b[39m\u001b[39m{\u001b[39;00mexpected_shape\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         )\n\u001b[1;32m    913\u001b[0m     X \u001b[39m=\u001b[39m DataFrameProcessing(\n\u001b[1;32m    914\u001b[0m         X, sort_dataframe\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_df, validate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    915\u001b[0m     )\u001b[39m.\u001b[39mgrouped_array()\n\u001b[1;32m    916\u001b[0m \u001b[39mif\u001b[39;00m level \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected X to have shape (8, 2), but got (8, 3)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_w_ex = pd.DataFrame(\n",
    "    {\n",
    "        'ds': np.hstack([np.arange(10), np.arange(10)]),\n",
    "        'y': np.random.rand(20),\n",
    "        'x': np.arange(20, dtype=np.float32),\n",
    "    },\n",
    "    index=pd.Index([0] * 10 + [1] * 10, name='unique_id'),\n",
    ").reset_index()\n",
    "train_mask = df_w_ex['ds'] < 6\n",
    "train_df = df_w_ex[train_mask]\n",
    "test_df = df_w_ex[~train_mask]\n",
    "xreg = test_df.drop(columns='y')\n",
    "# xreg = test_df.drop(columns='y').reset_index(drop=True)\n",
    "\n",
    "\n",
    "fcst_x = StatsForecast(models=[AutoARIMA()], freq='D')\n",
    "res = fcst_x.forecast(df=df_w_ex[train_mask], \n",
    "                      X_df=xreg, \n",
    "                      h=4)\n",
    "res.head()\n",
    "# expected_res = xreg.rename(columns={'x': 'ReturnX'}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee084bc4-7303-4dd6-99df-5a23fa7cb419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 17:14:43,665 - distributed.worker - WARNING - Compute Failed\n",
      "Key:       ('reset_index-ddc28a48b08205a7f5b4548ae95ed346', 5)\n",
      "Function:  subgraph_callable-d92a3252-009f-4cda-b462-a9ab6c00\n",
      "args:      ('lambda-f58bb160c9c653b41c79d6b9b2e4c9e9', ['unique_id'], None, <function PandasLikeUtils.safe_groupby_apply.<locals>.<lambda> at 0x7fb781ddaaf0>, 'drop_by_shallow_copy-01000b5b1e403ac7558bf7b4d4867b11',    unique_id  ... _partitions\n",
      "0          0  ...           5\n",
      "\n",
      "[1 rows x 4 columns], ['_partitions'], 'shuffle-p2p-502f87b2fa7418f415c782b6af8e9773')\n",
      "kwargs:    {}\n",
      "Exception: \"ValueError('Expected X to have shape (4, 2), but got (4, 3)')\"\n",
      "\n",
      "2023-07-08 17:14:43,672 - distributed.worker - WARNING - Compute Failed\n",
      "Key:       ('reset_index-ddc28a48b08205a7f5b4548ae95ed346', 0)\n",
      "Function:  subgraph_callable-d92a3252-009f-4cda-b462-a9ab6c00\n",
      "args:      ('lambda-f58bb160c9c653b41c79d6b9b2e4c9e9', ['unique_id'], None, <function PandasLikeUtils.safe_groupby_apply.<locals>.<lambda> at 0x7fb6a8a1ef70>, 'drop_by_shallow_copy-01000b5b1e403ac7558bf7b4d4867b11',    unique_id  ... _partitions\n",
      "0          1  ...           0\n",
      "\n",
      "[1 rows x 4 columns], ['_partitions'], 'shuffle-p2p-502f87b2fa7418f415c782b6af8e9773')\n",
      "kwargs:    {}\n",
      "Exception: \"ValueError('Expected X to have shape (4, 2), but got (4, 3)')\"\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected X to have shape (4, 2), but got (4, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| hide\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#| eval: false\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Distributed exogenous regressors\u001b[39;00m\n\u001b[1;32m      5\u001b[0m fcst_x \u001b[38;5;241m=\u001b[39m StatsForecast(models\u001b[38;5;241m=\u001b[39m[ReturnX()], freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfcst_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mX_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m expected_res \u001b[38;5;241m=\u001b[39m xreg\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturnX\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# we expect strings for unique_id, and ds using exogenous\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/distributed/client.py:3226\u001b[0m, in \u001b[0;36mClient.get\u001b[0;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   3224\u001b[0m         should_rejoin \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   3225\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3226\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgather(packed, asynchronous\u001b[39m=\u001b[39;49masynchronous, direct\u001b[39m=\u001b[39;49mdirect)\n\u001b[1;32m   3227\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   3228\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m futures\u001b[39m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/distributed/client.py:2361\u001b[0m, in \u001b[0;36mClient.gather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2360\u001b[0m     local_worker \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2361\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msync(\n\u001b[1;32m   2362\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gather,\n\u001b[1;32m   2363\u001b[0m     futures,\n\u001b[1;32m   2364\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   2365\u001b[0m     direct\u001b[39m=\u001b[39;49mdirect,\n\u001b[1;32m   2366\u001b[0m     local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[1;32m   2367\u001b[0m     asynchronous\u001b[39m=\u001b[39;49masynchronous,\n\u001b[1;32m   2368\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/distributed/utils.py:351\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[39mreturn\u001b[39;00m future\n\u001b[1;32m    350\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\n\u001b[1;32m    352\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, callback_timeout\u001b[39m=\u001b[39;49mcallback_timeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    353\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/distributed/utils.py:418\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m error:\n\u001b[1;32m    417\u001b[0m     typ, exc, tb \u001b[39m=\u001b[39m error\n\u001b[0;32m--> 418\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/distributed/utils.py:391\u001b[0m, in \u001b[0;36msync.<locals>.f\u001b[0;34m()\u001b[0m\n\u001b[1;32m    389\u001b[0m         future \u001b[39m=\u001b[39m wait_for(future, callback_timeout)\n\u001b[1;32m    390\u001b[0m     future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(future)\n\u001b[0;32m--> 391\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39myield\u001b[39;00m future\n\u001b[1;32m    392\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m     error \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/tornado/gen.py:769\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    766\u001b[0m exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     value \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    770\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     exc_info \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/distributed/client.py:2224\u001b[0m, in \u001b[0;36mClient._gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   2222\u001b[0m         exc \u001b[39m=\u001b[39m CancelledError(key)\n\u001b[1;32m   2223\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2224\u001b[0m         \u001b[39mraise\u001b[39;00m exception\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m   2225\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m   2226\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mskip\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/optimization.py:990\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minkeys):\n\u001b[1;32m    989\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m args, got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minkeys), \u001b[39mlen\u001b[39m(args)))\n\u001b[0;32m--> 990\u001b[0m \u001b[39mreturn\u001b[39;00m core\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdsk, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutkey, \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minkeys, args)))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/core.py:149\u001b[0m, in \u001b[0;36mget\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m toposort(dsk):\n\u001b[1;32m    148\u001b[0m     task \u001b[39m=\u001b[39m dsk[key]\n\u001b[0;32m--> 149\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, cache)\n\u001b[1;32m    150\u001b[0m     cache[key] \u001b[39m=\u001b[39m result\n\u001b[1;32m    151\u001b[0m result \u001b[39m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m(_execute_task(a, cache) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m(_execute_task(a, cache) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/core.py:113\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Do the actual work of collecting data and executing a function\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[39mExamples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m'foo'\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m [_execute_task(a, cache) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arg]\n\u001b[1;32m    114\u001b[0m \u001b[39melif\u001b[39;00m istask(arg):\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/core.py:113\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Do the actual work of collecting data and executing a function\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[39mExamples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m'foo'\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, \u001b[39mlist\u001b[39m):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m [_execute_task(a, cache) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arg]\n\u001b[1;32m    114\u001b[0m \u001b[39melif\u001b[39;00m istask(arg):\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m(_execute_task(a, cache) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/utils.py:73\u001b[0m, in \u001b[0;36mapply\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Apply a function given its positional and keyword arguments.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[39mEquivalent to ``func(*args, **kwargs)``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m>>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/dataframe/core.py:7023\u001b[0m, in \u001b[0;36mapply_and_enforce\u001b[0;34m()\u001b[0m\n\u001b[1;32m   7021\u001b[0m func \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_func\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   7022\u001b[0m meta \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_meta\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7023\u001b[0m df \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   7024\u001b[0m \u001b[39mif\u001b[39;00m is_dataframe_like(df) \u001b[39mor\u001b[39;00m is_series_like(df) \u001b[39mor\u001b[39;00m is_index_like(df):\n\u001b[1;32m   7025\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(df):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/dask/dataframe/groupby.py:229\u001b[0m, in \u001b[0;36m_groupby_slice_apply\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m key:\n\u001b[1;32m    228\u001b[0m     g \u001b[39m=\u001b[39m g[key]\n\u001b[0;32m--> 229\u001b[0m \u001b[39mreturn\u001b[39;00m g\u001b[39m.\u001b[39mapply(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1567\u001b[0m, in \u001b[0;36mapply\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1559\u001b[0m     new_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   1560\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe operation \u001b[39m\u001b[39m{\u001b[39;00morig_func\u001b[39m}\u001b[39;00m\u001b[39m failed on a column. If any error is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1561\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mraised, this will raise an exception in a future version \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1562\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof pandas. Drop these columns to avoid this warning.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1563\u001b[0m     )\n\u001b[1;32m   1564\u001b[0m     \u001b[39mwith\u001b[39;00m rewrite_warning(\n\u001b[1;32m   1565\u001b[0m         old_msg, \u001b[39mFutureWarning\u001b[39;00m, new_msg\n\u001b[1;32m   1566\u001b[0m     ) \u001b[39mif\u001b[39;00m is_np_func \u001b[39melse\u001b[39;00m nullcontext():\n\u001b[0;32m-> 1567\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_python_apply_general(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selected_obj)\n\u001b[1;32m   1568\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1569\u001b[0m     \u001b[39m# gh-20949\u001b[39;00m\n\u001b[1;32m   1570\u001b[0m     \u001b[39m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1574\u001b[0m     \u001b[39m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m     \u001b[39m# on a string grouper column\u001b[39;00m\n\u001b[1;32m   1577\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group_selection_context():\n\u001b[1;32m   1578\u001b[0m         \u001b[39m# GH#50538\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1629\u001b[0m, in \u001b[0;36m_python_apply_general\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1592\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m   1593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_python_apply_general\u001b[39m(\n\u001b[1;32m   1594\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     is_agg: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1600\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDFrameT:\n\u001b[1;32m   1601\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1602\u001b[0m \u001b[39m    Apply function f in python space\u001b[39;00m\n\u001b[1;32m   1603\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[39m        data after applying f\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1629\u001b[0m     values, mutated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39mapply(f, data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis)\n\u001b[1;32m   1630\u001b[0m     \u001b[39mif\u001b[39;00m not_indexed_same \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1631\u001b[0m         not_indexed_same \u001b[39m=\u001b[39m mutated \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmutated\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/pandas/core/groupby/ops.py:839\u001b[0m, in \u001b[0;36mapply\u001b[0;34m()\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39m# group might be modified\u001b[39;00m\n\u001b[1;32m    838\u001b[0m group_axes \u001b[39m=\u001b[39m group\u001b[39m.\u001b[39maxes\n\u001b[0;32m--> 839\u001b[0m res \u001b[39m=\u001b[39m f(group)\n\u001b[1;32m    840\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mutated \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[1;32m    841\u001b[0m     mutated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/triad/utils/pandas_like.py:249\u001b[0m, in \u001b[0;36mPandasLikeUtils.safe_groupby_apply.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cols) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m func(df)\n\u001b[1;32m    247\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    248\u001b[0m     df\u001b[39m.\u001b[39mgroupby(cols, dropna\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, group_keys\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 249\u001b[0m     \u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m df: _wrapper(df), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    250\u001b[0m     \u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    251\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/triad/utils/pandas_like.py:242\u001b[0m, in \u001b[0;36m_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrapper\u001b[39m(df: T) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m--> 242\u001b[0m     \u001b[39mreturn\u001b[39;00m func(df\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/fugue_dask/execution_engine.py:110\u001b[0m, in \u001b[0;36m_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     on_init_once(\u001b[39m0\u001b[39m, input_df)\n\u001b[1;32m    109\u001b[0m cursor\u001b[39m.\u001b[39mset(\u001b[39mlambda\u001b[39;00m: input_df\u001b[39m.\u001b[39mpeek_array(), \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m output_df \u001b[39m=\u001b[39m map_func(cursor, input_df)\n\u001b[1;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m output_df\u001b[39m.\u001b[39mas_pandas()[output_schema\u001b[39m.\u001b[39mnames]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/fugue/execution/execution_engine.py:1386\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1381\u001b[0m assert_or_throw(\n\u001b[1;32m   1382\u001b[0m     \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m   1383\u001b[0m     FugueBug(\u001b[39m\"\u001b[39m\u001b[39meach comap partition can have one and only one row\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1384\u001b[0m )\n\u001b[1;32m   1385\u001b[0m dfs \u001b[39m=\u001b[39m DataFrames(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_dfs(data[\u001b[39m0\u001b[39m])))\n\u001b[0;32m-> 1386\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(cursor, dfs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/fugue/extensions/_builtins/processors.py:362\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39m_cursor \u001b[39m=\u001b[39m cursor  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mignore_errors) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mtransform(dfs)\n\u001b[1;32m    364\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/fugue/extensions/transformer/convert.py:449\u001b[0m, in \u001b[0;36mtransform\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapper\u001b[39m.\u001b[39mrun(  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    443\u001b[0m         [dfs] \u001b[39m+\u001b[39m cb,\n\u001b[1;32m    444\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams,\n\u001b[1;32m    445\u001b[0m         ignore_unknown\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    446\u001b[0m         output_schema\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_schema,\n\u001b[1;32m    447\u001b[0m     )\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m dfs\u001b[39m.\u001b[39mhas_key:  \u001b[39m# input does not have key\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapper\u001b[39m.\u001b[39mrun(  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    450\u001b[0m         \u001b[39mlist\u001b[39m(dfs\u001b[39m.\u001b[39mvalues()) \u001b[39m+\u001b[39m cb,\n\u001b[1;32m    451\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams,\n\u001b[1;32m    452\u001b[0m         ignore_unknown\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    453\u001b[0m         output_schema\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_schema,\n\u001b[1;32m    454\u001b[0m     )\n\u001b[1;32m    455\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# input DataFrames has key\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     p \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dfs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nixtla/lib/python3.8/site-packages/fugue/dataframe/function_wrapper.py:93\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_unknown \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(p) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     92\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m are not acceptable parameters\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m rt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrargs)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output:\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rt, _DataFrameParamBase):\n",
      "Cell \u001b[0;32mIn[6], line 158\u001b[0m, in \u001b[0;36m_forecast_series_X\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_df) \u001b[38;5;241m!=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease be sure that your exogenous variables `X_df` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhave the same length than your forecast horizon `h`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mforecast(X_df\u001b[38;5;241m=\u001b[39mX_df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m~/Work/statsforecast/statsforecast/core.py:1052\u001b[0m, in \u001b[0;36mforecast\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_prediction_intervals(prediction_intervals\u001b[39m=\u001b[39mprediction_intervals)\n\u001b[1;32m   1051\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_fit(df, sort_df)\n\u001b[0;32m-> 1052\u001b[0m X, level \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_X_level(h\u001b[39m=\u001b[39mh, X\u001b[39m=\u001b[39mX_df, level\u001b[39m=\u001b[39mlevel)\n\u001b[1;32m   1053\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1054\u001b[0m     res_fcsts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mga\u001b[39m.\u001b[39mforecast(\n\u001b[1;32m   1055\u001b[0m         models\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels,\n\u001b[1;32m   1056\u001b[0m         h\u001b[39m=\u001b[39mh,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m   1062\u001b[0m     )\n",
      "File \u001b[0;32m~/Work/statsforecast/statsforecast/core.py:910\u001b[0m, in \u001b[0;36m_parse_X_level\u001b[0;34m()\u001b[0m\n\u001b[1;32m    907\u001b[0m     expected_shape \u001b[39m=\u001b[39m (expected_shape_rows, expected_shape_cols)\n\u001b[1;32m    909\u001b[0m     \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m expected_shape:\n\u001b[0;32m--> 910\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected X to have shape \u001b[39m\u001b[39m{\u001b[39;00mexpected_shape\u001b[39m}\u001b[39;00m\u001b[39m, but got \u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         )\n\u001b[1;32m    913\u001b[0m     X \u001b[39m=\u001b[39m DataFrameProcessing(\n\u001b[1;32m    914\u001b[0m         X, sort_dataframe\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_df, validate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    915\u001b[0m     )\u001b[39m.\u001b[39mgrouped_array()\n\u001b[1;32m    916\u001b[0m \u001b[39mif\u001b[39;00m level \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected X to have shape (4, 2), but got (4, 3)"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Distributed exogenous regressors\n",
    "fcst_x = StatsForecast(models=[ReturnX()], freq='D')\n",
    "res = fcst_x.forecast(df=train_df, \n",
    "                      X_df=xreg, \n",
    "                      h=4).compute()\n",
    "expected_res = xreg.rename(columns={'x': 'ReturnX'}).compute()\n",
    "# we expect strings for unique_id, and ds using exogenous\n",
    "expected_res[['unique_id', 'ds']] = expected_res[['unique_id', 'ds']].astype(str)\n",
    "pd.testing.assert_frame_equal(res.sort_values('unique_id').reset_index(drop=True), \n",
    "                              expected_res, \n",
    "                              check_dtype=False, \n",
    "                              check_index_type=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4f3de-910d-46eb-842b-579935cfbd10",
   "metadata": {},
   "source": [
    "### Distributed Cross-Validation\n",
    "\n",
    "For extremely fast distributed temporcal cross-validation we use `cross_validation` method that operates like the original [StatsForecast.cross_validation](https://nixtla.github.io/statsforecast/core.html#statsforecast) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d0a2f-8f3b-45cd-9f25-d8db281db3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Distributed cross-validation with FugueBackend.\n",
    "sf.cross_validation(df=df, h=12, n_windows=2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95e09b-cb70-4232-8ffa-b26fc8aea557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "# Generate Synthetic Panel Data.\n",
    "df = generate_series(10).reset_index()\n",
    "df['unique_id'] = df['unique_id'].astype(str)\n",
    "df = dd.from_pandas(df, npartitions=10)\n",
    "\n",
    "# Distribute predictions.\n",
    "sf = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = sf.forecast(df=df, h=12).compute().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "fcst_stats = sf.forecast(df=df.compute(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())\n",
    "\n",
    "# Distribute cross-validation predictions.\n",
    "sf = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = sf.cross_validation(df=df, h=12).compute().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.compute(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())\n",
    "\n",
    "# fallback model\n",
    "class FailNaive:\n",
    "    def forecast(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return 'Naive'\n",
    "    \n",
    "#cross validation fallback model\n",
    "fcst = StatsForecast(models=[FailNaive()], freq='D', fallback_model=Naive())\n",
    "fcst_fugue = fcst.cross_validation(df=df, h=12).compute().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.compute(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1084e8-c722-48d5-a038-8d7e530773bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "# test ray integration\n",
    "import ray\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast.utils import generate_series\n",
    "\n",
    "# Generate Synthetic Panel Data.\n",
    "df = generate_series(10).reset_index()\n",
    "df['unique_id'] = df['unique_id'].astype(str)\n",
    "df = ray.data.from_pandas(df).repartition(2)\n",
    "\n",
    "# Distribute predictions.\n",
    "sf = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = sf.forecast(df=df, h=12).to_pandas().sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
    "fcst_stats = sf.forecast(df=df.to_pandas(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())\n",
    "\n",
    "# Distribute cross-validation predictions.\n",
    "fcst = StatsForecast(models=[Naive()], freq='D')\n",
    "fcst_fugue = fcst.cross_validation(df=df, h=12).to_pandas().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.to_pandas(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())\n",
    "\n",
    "# fallback model\n",
    "class FailNaive:\n",
    "    def forecast(self):\n",
    "        pass\n",
    "    def __repr__(self):\n",
    "        return 'Naive'\n",
    "    \n",
    "#cross validation fallback model\n",
    "sf = StatsForecast(models=[FailNaive()], freq='D', fallback_model=Naive())\n",
    "fcst_fugue = sf.cross_validation(df=df, h=12).to_pandas().sort_values(['unique_id', 'ds', 'cutoff']).reset_index(drop=True)\n",
    "fcst_stats = sf.cross_validation(df=df.to_pandas(), h=12)\n",
    "test_eq(fcst_fugue, fcst_stats.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628146a-57b0-4b4b-8377-045b08dd2367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "\n",
    "# Distributed exogenous regressors\n",
    "sf = StatsForecast(models=[ReturnX()], freq='D')\n",
    "res = sf.forecast(df=train_df, \n",
    "                  X_df=xreg, \n",
    "                  h=4).compute()\n",
    "expected_res = xreg.compute().rename(columns={'x': 'ReturnX'})\n",
    "# we expect strings for unique_id, and ds using exogenous\n",
    "expected_res[['unique_id', 'ds']] = expected_res[['unique_id', 'ds']].astype(str)\n",
    "pd.testing.assert_frame_equal(res.sort_values('unique_id').reset_index(drop=True), \n",
    "                              expected_res, \n",
    "                              check_dtype=False, \n",
    "                              check_index_type=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('nixtla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "86691f1235dfb63c9749c1b2b5c82263c6ebe5d132bc551491747a17022061dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
